<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>第 2 页 | PaperWeekly</title>
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="PaperWeekly"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">PaperWeekly</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
      <li><a href="/about/index.html">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-15T04:00:30.000Z"><a href="/2016/07/14/The-Ubuntu-Dialogue-Corpus-A-Large-Dataset-for-Research-in-Unstructured-Multi-Turn-Dialogue-Systems-PaperWeekly/">2016-07-14</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/14/The-Ubuntu-Dialogue-Corpus-A-Large-Dataset-for-Research-in-Unstructured-Multi-Turn-Dialogue-Systems-PaperWeekly/">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文分享的paper构建了一组大型非结构化的、多轮的对话系统语料，使用的原始数据来自<a href="https://irclogs.ubuntu.com/" target="_blank" rel="external">Ubuntu IRC Logs</a>，是一些关于Ubuntu的讨论组聊天数据。paper的题目是<a href="http://arxiv.org/pdf/1506.08909v3.pdf" target="_blank" rel="external">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</a>，作者是来自蒙特利尔大学的博士生<a href="http://cs.mcgill.ca/~rlowe1/" target="_blank" rel="external">Ryan Lowe</a>。</p>
<p>数据规模在100万左右，平均每组数据有8轮对话，最少包括3轮对话。之前的bot语料包括：Dialogue State Tracking Challenge(DSTC)、SwitchBoard这类结构化的数据和Twitter、Sina Weibo这种非结构化的数据，前者专注于预测用户的需求和状态，而后者数据中包括了一定数量的非“conversational in nature”，做bot的训练数据并不那么合适。本文构建的数据集是一个特定领域内的数据，ubuntu technical conversations，规模很大，对话轮数很多，质量很高，也是后续很多paper在研究bot response问题时常常采用的corpus。</p>
<p><img src="media/1.png" alt="1"></p>
<p>语料的构建非常有意义，大型的语料可以训练更加复杂的、偏向open domain的bot model，小型的语料可以解决具体的工程应用问题，如何从杂乱无章的unstructured data中提取出有用的信息，构造出一个适合训练、测试的数据集是一个很难却十分有意义的工作。</p>
<p>本文需要的数据是多轮的、两人的对话数据，但原始的数据是多人无序的对话数据，作者采用了一些小的技巧，并且忽略了一些不合适的数据，将原始数据处理成一个四元组：</p>
<p>(time,sender,recipient,utterance)</p>
<p>在构造模型的训练和测试集时，作者将上面的四元组处理成下面的三元组：</p>
<p>(context,response,flag)</p>
<p>context类似于用户输入，flag表示response是否是context相关联的，关联则为1，否则为0。</p>
<p>给定了数据集，下面就是作者提供的benchmark model，三个非常简单的model，tf-idf，rnn和lstm，目的是为了从response candidates中选择k个最适合context的response作为答案，然后计算相应的准确率。paper中给的方法是selection的方法，而不是generation，后面的很多研究都是generation，真正地从user query生成response。</p>
<p>本文提供的ubuntu dialogue corpus对于task-oriented、response generation的研究有着非常重要的意义，相比于华为给的微博数据，有更强的conversational in nature特征，更加适合对话生成的研究。本文作者的另外一篇survey文章<a href="http://arxiv.org/pdf/1512.05742.pdf" target="_blank" rel="external">A Survey of Available Corpora for Building Data-Driven Dialogue Systems</a>,系统地介绍了各大数据集。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-14T17:28:16.000Z"><a href="/2016/07/14/随笔/">2016-07-14</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/14/随笔/">随笔</a></h1>
  

    </header>
    <div class="entry">
      
        <p>这篇文章的题目有些难产，一直想不出叫一个什么名字好。想写写最近看的东西的一点思考，也想写写一些别的东西，很纠结。以前写文章都喜欢用豆瓣fm推荐的第一首歌作为题目，然后开始写，虽然写的内容可能与题目毫无关系，但却不纠结。听着豆瓣fm，写着blog，是一种很多年的习惯了，习惯是一种可怕的东西，养成了之后就会一直这么做，一点都不能变，不然就会不舒服。</p>
<p>人之所以开心地活在这个世界上是因为大家有很多有意思的事情要做，人之所以害怕离开这个世界是因为很多有意思的事情还没做就结束了。不管现在的状况是怎样，心中充满希望就会不一样。有的人说生活不重要，家庭不重要，只有事业最重要，顾及儿女情长没有什么出息，实在不敢苟同，没有了坚实的地基，空中楼阁再漂亮又有何用？生活的目的就是生活本身，而不是虚伪地活给谁看，和谁比较，与谁相争，向谁证明。</p>
<p>我想用一个比喻来形容我遇到我的爱人可能会比较恰当。从前，有一只在一个无形的笼子里飞来飞去的鸟，他看着地上的人们心中总是有一种优越感，以为自己看得到整个世界，浑然不知自己身在牢笼中。后来来了另外一只鸟，一只特别好看的鸟，帮他打开了笼子，带着他飞向了一个真正广阔的天空，带着他到处飞翔，他才恍然大悟，原来世界可以这么大，于是他们开始了属于他们的旅途。世界很大，而我们很小，我们的格局很小，我们的心胸很小，我们看到的世界很小。世界很有趣，生活也不只是油盐酱醋，也不只是眼前的苟且，还有诗和远方。她用心准备婚礼的每一个细节，请帖用了一种古代西方信件的方式，用融化的蜡块来粘合信封，并且盖上我们俩专属的印章；回礼是一个精美的多肉植物，一盆一盆地种下、包好；喜糖是精心挑选的几种糖果，用一个手工纸袋包装好，过程很麻烦，但是她很享受，你要知道，可不是只做一份、十份，是要只做150份左右，她很享受这样的过程，因为她在用她的双手实现她感兴趣的事情，乐在其中。</p>
<p>世界可以灰暗，也可以很美好，决定于你是一个怎样的人，遇见一个怎样的人。很多人的生活每天都是在钱钱钱的争吵中度过的，永远没一个够，多少钱算多呢？人的欲望又能用多少钱来满足呢？生活可以很糟糕，也可以很美好，取决于你的追求，你所追求的是一种怎样的状态。欲望简单但不乏丰富多彩的生活才是真正高质量的生活，你内心保留地纯粹和纯真越多，生活质量就会越高，相反都会生活地很累，觉得生活都是负担。生活的目的就是生活本身，享受生活就是享受生活中的每一个细节，做一顿大餐，开一个小型音乐party，到录音棚录一首歌曲，看一场演唱会，听一场相声，看一场话剧，拍一些照片，吃一些好吃的垃圾食品，带着hare到处走走，吐槽一些烂剧，开始一场说走就走的旅行。生活中如果只有一个目的，只有工作这一件事情重要的话，那么生活本身就失去了意义，你赚钱也就失去了意义，有的人会说我不努力工作，不赚更多的钱怎么生活，完全可以40或50岁之后再开始享受生活。</p>
<p>最近看bot方面的paper，简单说一下对bot的一点naive的理解。bot火是不争的事实，也是一个必然的趋势，可能做成一个true ai的bot是一件遥不可及的事情，但做出一个能够解决实际问题，提升大家效率的bot是指日可待的事情。我觉得大家对bot的期许不应该是一个什么都能解决的通用工具或者通用技术，如果媒体地热炒加上民众过高的期待会造成新一轮的人工智能寒潮，对这个领域并不是好事。大家可以认为bot是一种新的交互方式，是一种新的操作入口，就像互联网，就像操作系统一样，是一种新的模式，在这种模式背后有大量先进的人工智能技术在做支撑。用户在任何一个地方都不再需要一个特定的终端来做一些常规的事情（不是所有的事情），只需要找一个联网的bot（可能是一个手机，可能是一面镜子，可能是一个电话亭，只要能联网并识别用户身份）就可以完成了，bot执行的过程不需要透明，只需要给出一个结果反馈就可以了，大家的生活围绕着各种各样垂直的bot来展开，只需要最简单的交互就可以完成之前需要复杂操作的事情，比如办个护照，买个机票。如果一个事情很难做的话，我们通常会将其分解成多个容易的事情，逐个攻破，不用期许过高，但相信bot一定会给大家的生活带来一次革命。</p>
<p>bot是一个很大的市场，如果真的能做成一个入口式的平台，相当于重新开辟了一个新的市场，重新定义了这个世界，任何的软件和应用都需要换一种形式，来为用户提供服务。bot确实是一个很美好的梦想，也不是那么遥不可及，但也不是那么容易，那么触手可及。还是需要大量的研究人员不断地努力，攻克难题。现在很多公司都在做bot领域的技术积累和市场占坑，以方便在日后新的一轮机会到来之时，分一杯羹。当前bot的平台有几家大企业在做，但整体来说还是将bot作为一种交互方式，将命令菜单化，并不是真正的对话，有一点iffft的感觉，但确实很多企业也在用这样的平台，大家都是在占坑。bot虽热，但并不是媒体热炒的那样，还是有很多的坑在里面，只有对其进行深入地思考和理解，保持一种冷静和独立地思考，才能真正地抓到痛点和需求，而不是一味地盲目跟随。技术的积累非常重要，因为真正要瓜分市场还是要很高门槛的，不是说你做一个简单的陪聊、逗乐用的机器人就掌握了bot核心技术，真的没有这么简单。周末的时候，准备对最近读bot paper的一些思考，写一篇survey。</p>
<p>美好的生活就是做喜欢的事情，比如亲手构建一个美好的世界，一个bot化的世界，一个更加简便、纯粹的世界。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-13T18:19:26.000Z"><a href="/2016/07/13/A-Hierarchical-Latent-Variable-Encoder-Decoder-Model-for-Generating-Dialogues-PaperWeekly/">2016-07-13</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/13/A-Hierarchical-Latent-Variable-Encoder-Decoder-Model-for-Generating-Dialogues-PaperWeekly/">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文分享的paper旨在解决语言模型生成部分存在的问题，并且以bot为应用背景进行了实验。paper的题目是<a href="https://arxiv.org/pdf/1605.06069v3.pdf" target="_blank" rel="external">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</a>，作者来自蒙特利尔大学和Maluuba公司，这家公司的研究水平非常地高，arxiv上常常可以刷出高质量的paper。</p>
<p>通常来讲，自然语言对话都会包含两个层次的结构，一个是utterance，由语言的局部统计信息来表征其含义，一个是topic，由一些随机的特征来表征。本文的工作就是对这些utterance中存在的随机特征进行建模，从而提高语言模型生成人类语言时的质量。本文认为，类似于RNNLM这样的语言模型在生成人话质量不高的根本原因在于，没有处理好隐藏在utterance中的随机feature或者说noise，从而在生成next token（short term goal）和future tokens（long term goal）效果一般。</p>
<p>本文的模型Latent Variable Hierarchical Recurrent Encoder Decoder(VHRED)，在生成过程中分为两步：</p>
<p>step 1 随机采样latent variables</p>
<p>step 2 生成输出序列</p>
<p>架构示意图见下图：</p>
<p><img src="media/1.png" alt="1"></p>
<p>在生成每一个utterance时，需要用到四个部分，encoder RNN、context RNN、latent variable、decoder RNN，按顺序依次输入和输出。这里的latent variable和IR中的LSI有一点异曲同工，latent表明我们说不清他们到底具体是什么，但可能是代表一种topic或者sentiment，是一种降维的表示。</p>
<p>实验部分，选择了bot作为应用背景，得到了不错的效果。见下图：</p>
<p><img src="media/2.png" alt="2"></p>
<p>本文解决的不仅仅是bot领域对话生成的问题，而是整个seq2seq框架中decoder的问题，只要涉及到decoder生成的部分都可以采用本文的思想来解决问题。latent topic是一个非常有意思的东西，在LSI、推荐系统中都有非常重要的意义，矩阵分解之后得到两个降维之后的矩阵，从一组两个维度映射到了两组两个维度，也就是多了所谓的latent topic，说不清这些topic是什么，但的确可以将相似的东西聚到了一起。本文也是用latent topic来描述隐藏在utterance中那些说不清道不明的随机noise，得到了更好的效果。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-12T22:11:19.000Z"><a href="/2016/07/12/A-Network-based-End-to-End-Trainable-Task-oriented-Dialogue-System-PaperWeekly/">2016-07-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/12/A-Network-based-End-to-End-Trainable-Task-oriented-Dialogue-System-PaperWeekly/">A Network-based End-to-End Trainable Task-oriented Dialogue System #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>构建一个task-oriented的bot，比如订票，技术支持服务等等，是一件非常难的事情，因为对于特定任务，训练数据会非常非常有限。在学术界，bot领域现在最流行的解决方案之一是seq2seq，在一个非常庞大的open-domain数据集上进行训练，得到一些效果不错的模型，但难以应用到具体task中，因为这类模型无法做到与数据库交互以及整合其他有用的信息，从而生成实用的response。还有一种非常流行的方案是reinforcement learning，上一篇分享的paper<a href="http://rsarxiv.github.io/2016/07/11/Deep-Reinforcement-Learning-for-Dialogue-Generation-PaperWeekly/">Deep Reinforcement Learning for Dialogue Generation</a>将两者有机地结合在了一起，增强学习可以使得response生成时考虑更长远的影响。</p>
<p>本文将分享的这篇paper，针对task-oriented的bot问题，平衡了两种流行方案的优缺点，提出了一套有参考价值的、具有实际意义的seq2seq解决方案。paper的题目是<a href="http://arxiv.org/pdf/1604.04562v2.pdf" target="_blank" rel="external">A Network-based End-to-End Trainable Task-oriented Dialogue System</a>，本文于2016年5月20日发表于arxiv上，作者是来自剑桥大学Dialogue System Group的博士生<a href="http://mi.eng.cam.ac.uk/~thw28/" target="_blank" rel="external">Tsung-Hsien Wen</a>，该组专门研究chatbot相关技术，发表过大量与之相关的paper，后续会更多地关注该组的工作。</p>
<p><img src="media/1.png" alt="1"></p>
<p>上图是本文方案的架构示意图，分为五个部分。下面分别进行介绍：</p>
<p>1、Intent Network</p>
<p>这个部分可以理解为seq2seq的encoder部分，将用户的输入encode成一个vector z(t)。encoder部分分别用了lstm和cnn两种模型对该输入进行建模。这两种句子表示的方法在之前的文章中都有介绍。</p>
<p>2、Belief Trackers</p>
<p>这个部分又被称作是Dialogue State Tracking(DST)，是task-oriented bot的核心部件。本文的Belief Trackers具有以下的作用：</p>
<ul>
<li><p>支持各种形式的自然语言被映射成一个有限slot-value对集合中的元素，用于在数据库中进行query。</p>
</li>
<li><p>追踪bot的state，避免去学习那些没有信息量的数据。</p>
</li>
<li><p>使用了一种weight tying strategy，可以极大地减少训练数据的需求。</p>
</li>
<li><p>易扩展新的组件。</p>
</li>
</ul>
<p><img src="media/2.png" alt="2"></p>
<p>这个组件的输入时用户的input，输出是一个informable slot和requestable slot的概率分布，这里的informable slot是指food，price range和area（以订餐为例），用来约束数据库中的查询，requestable slot是指address，phone，postcode等一些可以被询问的值。这里会定义一个针对具体task的知识图谱，来表示这些slot之间的关系，每个slot都会定义一个tracker，tracker的模型如上图所示，包括一个CNN特征提取模块和一个Jordan型的RNN模块，CNN不仅仅对当前的input进行处理，还对上一轮的user input进行处理，综合起来作为RNN的输入。</p>
<p>这个组件的意义在于获取到预先定好的知识图谱中每个slot的分布，就是说弄清楚用户在这轮对话中的需求是哪个词或者词组。</p>
<p>3、Database Operator</p>
<p>数据库查询的输入来自于Belief Trackers的输出，即各种slot的概率分布，取最大的那个作为DB的输入，进行查询，获取到相应的值。</p>
<p>4、Policy Network</p>
<p>这个组件是像一个胶水，起到粘合其他上面三个组件的作用。输入是上面三个组件的输出，输出是一个向量。</p>
<p>5、Generation Network </p>
<p>最后一个组件是生成模型，本质上是一个语言模型，输入是Policy Network的输出，输出是生成的response，再经过一些处理之后可以返回给用户了。这里的处理主要是将response中的slot，比如s.food还原成真实的值。生成部分用简单的LSTM-LM可以做，用Attention Model也可以做，效果会更好。</p>
<p>数据的准备这部分，利用了众包进行收集，一共采用了680轮对话作为训练数据，数据库中保存了99个饭馆，3个informable slots和7个requestable slots。</p>
<p>训练分为两个阶段，第一阶段是训练belief trackers，得到模型之后，更新参数，对生成网络中的语言模型进行训练，得到full model，batch size取1。</p>
<p>bot模型自动评价这块是一个非常难的事情，本文选择了BLEU score、entity matching rate和objective task success rate，本文模型均取得了不错的结果。另外，通过人工评价对本文模型和rule-based进行了对比，结果看下图：</p>
<p><img src="media/3.png" alt="3"></p>
<p>最后paper给出了一种生成的句子向量的二维图，如下图：</p>
<p><img src="media/4.png" alt="4"></p>
<p>几乎同一类话都被聚集到了相似的位置上，验证了模型的有效性。</p>
<p>开放域的bot只是根据query生成一句response，虽然质量可以做到很高，但实用价值不大。面向具体业务的闭域bot一直难以应用seq2seq的解决方案在于，无法将大量的专业信息建模到模型中来，包括：历史信息，用户身份信息，业务信息等等，本文打开了一扇窗，就是将具体的业务信息和历史信息加到了模型中，并且通过将对话中的slot词转换为一些slot表示，就好比构建了很多的模板，降低了对训练数据的需求，避免了seq2seq在应用时存在的问题。如果再考虑上Jiwei Li的那篇<a href="http://rsarxiv.github.io/2016/07/10/A-Persona-Based-Neural-Conversation-Model-PaperWeekly/">A Persona-Based Neural Conversation Model</a>中对用户信息的建模，bot的实用价值就会更大，用data来解决真正的业务问题就会更进一步。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-11T23:47:50.000Z"><a href="/2016/07/11/Deep-Reinforcement-Learning-for-Dialogue-Generation-PaperWeekly/">2016-07-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/11/Deep-Reinforcement-Learning-for-Dialogue-Generation-PaperWeekly/">Deep Reinforcement Learning for Dialogue Generation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文将会分享一篇深度增强学习在bot中应用的文章，增强学习在很早的时候就应用于bot中来解决一些实际问题，最近几年开始流行深度增强学习，本文作者将其引入到最新的bot问题中。paper的题目是<a href="http://arxiv.org/pdf/1606.01541v3.pdf" target="_blank" rel="external">Deep Reinforcement Learning for Dialogue Generation</a>，作者是Jiwei Li，最早于2016年6月10日发在arxiv上。</p>
<p>现在学术界中bot领域流行的解决方案是seq2seq，本文针对这种方案抛出两个问题：</p>
<p>1、用MLE作为目标函数会导致容易生成类似于“呵呵呵”的reply，grammatical、safe但是没有营养，没有实际意义的话。</p>
<p>2、用MLE作为目标函数容易引起对话的死循环，如下图：</p>
<p><img src="media/1.png" alt="1"></p>
<p>解决这样的问题需要bot框架具备以下的能力：</p>
<p>1、整合开发者自定义的回报函数，来达到目标。</p>
<p>2、生成一个reply之后，可以定量地描述这个reply对后续阶段的影响。</p>
<p>所以，本文提出用seq2seq+增强学习的思路来解决这个问题。</p>
<p>说到增强学习，就不得不提增强学习的四元素：</p>
<ul>
<li>Action</li>
</ul>
<p>这里的action是指生成的reply，action空间是无限大的，因为可以reply可以是任意长度的文本序列。</p>
<ul>
<li>State</li>
</ul>
<p>这里的state是指[pi,qi]，即上一轮两个人的对话表示。</p>
<ul>
<li>Policy</li>
</ul>
<p>policy是指给定state之后各个action的概率分布。可以表示为：pRL(pi+1|pi, qi)</p>
<ul>
<li>Reward</li>
</ul>
<p>reward表示每个action获得的回报，本文自定义了三种reward。 </p>
<p>1、Ease of Answering</p>
<p>这个reward指标主要是说生成的reply一定是容易被回答的。本文用下面的公式来计算容易的程度：</p>
<p><img src="media/2.png" alt="2"></p>
<p>其实就是给定这个reply之后，生成的下一个reply是dull的概率大小。这里所谓的dull就是指一些“呵呵呵”的reply，比如“I don’t know what you are talking about”等没有什么营养的话，作者手动给出了这样的一个dull列表。</p>
<p>2、Information Flow</p>
<p>生成的reply尽量和之前的不要重复。</p>
<p><img src="media/3.png" alt="3"></p>
<p>这里的h是bot的reply表示，i和i+1表示该bot的前后两轮。这个式子表示同一个bot两轮的对话越像reward越小。</p>
<p>3、Semantic Coherence</p>
<p>这个指标是用来衡量生成reply是否grammatical和coherent。如果只有前两个指标，很有可能会得到更高的reward，但是生成的句子并不连贯或者说不成一个自然句子。</p>
<p><img src="media/4.png" alt="4"></p>
<p>这里采用互信息来确保生成的reply具有连贯性。</p>
<p>最终的reward由这三部分加权求和计算得到。</p>
<p>增强学习的几个要素介绍完之后，接下来就是如何仿真的问题，本文采用两个bot相互对话的方式进行。</p>
<p><b>step 1</b> 监督学习。将数据中的每轮对话当做target，将之前的两句对话当做source进行seq2seq训练得到模型，这一步的结果作为第二步的初值。</p>
<p><b>step 2</b> 增强学习。因为seq2seq会容易生成dull reply，如果直接用seq2seq的结果将会导致增强学习这部分产生的reply也不是非常的diversity，从而无法产生高质量的reply。所以，这里用MMI(Maximum Mutual Information，这里与之前Jiwei Li的两篇paper做法一致)来生成更加diversity的reply，然后将生成最大互信息reply的问题转换为一个增强学习问题，这里的互信息score作为reward的一部分（r3）。用第一步训练好的模型来初始化policy模型，给定输入[pi,qi]，生成一个候选列表作为action集合，集合中的每个reply都计算出其MMI score，这个score作为reward反向传播回seq2seq模型中，进行训练。整个仿真过程如下图：</p>
<p><img src="media/5.png" alt="5"></p>
<p>两个bot在对话，初始的时候给定一个input message，然后bot1根据input生成5个候选reply，依次往下进行，因为每一个input都会产生5个reply，随着turn的增加，reply会指数增长，这里在每轮对话中，通过sample来选择出5个作为本轮的reply。</p>
<p>接下来就是评价的部分，自动评价指标一共两个：</p>
<p>1、对话轮数。<br><img src="media/6.png" alt="6"></p>
<p>很明显，增强学习生成的对话轮数更多。</p>
<p>2、diversity。<br><img src="media/7.png" alt="7"><br>增强学习生成的词、词组更加丰富和多样。</p>
<p>下图给出了一个MMI seq2seq与RL方法的对比结果：</p>
<p><img src="media/8.png" alt="8"><br>RL不仅仅在回答上一个提问，而且常常能够提出一个新的问题，让对话继续下去，所以对话轮数就会增多。原因是，RL在选择最优action的时候回考虑长远的reward，而不仅仅是当前的reward。</p>
<p>本文是一篇探索性的文章，将seq2seq与RL整合在一起解决bot的问题是一个不错的思路，很有启发性，尤其是用RL可以将问题考虑地更加长远，获得更大的reward。用两个bot相互对话来产生大量的训练数据也非常有用，在实际工程应用背景下数据的缺乏是一个很严重的问题，如果有一定质量的bot可以不断地模拟真实用户来产生数据，将deep learning真正用在bot中解决实际问题就指日可待了。</p>
<p>RL解决bot问题的文章在之前出现过一些，但都是人工给出一些feature来进行增强学习，随着deepmind用seq2seq+RL的思路成功地解决video games的问题，这种seq2seq的思想与RL的结合就成为了一种趋势，朝着data driven的方向更进一步。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-11T20:03:05.000Z"><a href="/2016/07/11/Consensus-Attention-based-Neural-Networks-for-Chinese-Reading-Comprehension-PaperWeekly/">2016-07-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/11/Consensus-Attention-based-Neural-Networks-for-Chinese-Reading-Comprehension-PaperWeekly/">Consensus Attention-based Neural Networks for Chinese Reading Comprehension #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文分享的是今天刚刚刷出的一篇paper，是研究阅读理解的同学们的福音，因为要放出新的而且是中文的数据集。本文的题目是<a href="http://cn.arxiv.org/pdf/1607.02250" target="_blank" rel="external">Consensus Attention-based Neural Networks for Chinese Reading Comprehension</a>，作者均来自哈工大讯飞联合实验室。</p>
<p>对于机器阅读理解的基本内容就不作介绍了，感兴趣的同学可以参考之前写的一篇摘要<a href="http://rsarxiv.github.io/2016/06/18/%E6%95%99%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%98%85%E8%AF%BB/">教机器学习阅读</a>。本文最大的亮点在于构建了中文机器阅读语料，语料分为两个部分，一个是训练集和自测试集，一个是领域外的测试集，包括人工的提问和自动获取的提问两种。（<a href="http://hfl.iflytek.com/chinese-rc/" target="_blank" rel="external">语料地址</a>，可能过段时间会publish出来）</p>
<p>第一个部分是从人民日报获取的新闻语料，构建方法比较简单，先用POS工具对每篇新闻的词性进行标注，选择出现过两次以上的名词作为候选答案词。从候选词总随机选择一个词作为答案词，用包含答案词的句子作为问题query，剩下的部分作为document，从而构造出一个<document,query,answer>对。这种做法的好处是基于一个不太多的语料都可以构建出大量的<document,query,answer>对用来训练，这样也迎合了deep learning的需求。</document,query,answer></document,query,answer></p>
<p>第二个部分也是非常有意思的部分，就是提出了用一个训练数据领域外的数据集作为测试集，构造的方法分为两种，一种是自动的方法和第一部分相同，第二种是基于人工的提问，而且是对于机器来说难度较大的问题。之所以采用领域外的数据进行测试，是为了防止新闻数据中很多问题可以通过外部知识库来进行回答，导致问题变得简单，如果用一个儿童读物的数据作为测试集，就会将这个问题变得更加纯粹和有挑战性。</p>
<p>既然提出了新数据，baseline模型也省不了，本文提出的模型叫Consensus Attention Sum Reader，没有太多的新东西，效果也没有之前文章中Gate Attention Reader和Iterative Alternating Attention那么好，所以就不再介绍了。</p>
<p>训练数据的自动标注和生成是deep learning应用的关键，很多领域发展缓慢或者在工程中应用不好都是因为data的量不够多，且没有太多好的方法来生成或者标注。机器阅读这个领域，相对来说，dataset的自动构建还是很容易做的，操作也比较简单，抠掉一个核心词就可以。而bot，自动文摘，在实际的工程应用中都难以用流行的data driven方案来解决，因为代价太大了。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-11T18:06:42.000Z"><a href="/2016/07/11/A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models-PaperWeekly/">2016-07-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/11/A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models-PaperWeekly/">A Diversity-Promoting Objective Function for Neural Conversation Models #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇分享的文章是前一篇分享<a href="http://rsarxiv.github.io/2016/07/10/A-Persona-Based-Neural-Conversation-Model-PaperWeekly/">A Persona-Based Neural Conversation Model</a>的pre-paper，题目是<a href="http://arxiv.org/pdf/1510.03055v1.pdf" target="_blank" rel="external">A Diversity-Promoting Objective Function for Neural Conversation Models</a>，作者是Jiwei Li，最早于2015年10月11日发在arxiv上。</p>
<p>本文针对的问题是seq2seq方案在解决bot问题上容易生成一些“呵呵”的reply，比如“I don’t know”之类的非常safe，grammatical的reply，但是营养太少，没有太多实际的意义。造成这种情况的原因是目标函数不合适，在最近流行的自然语言生成任务中一般都采用MLE作为目标函数，这个目标函数可以保证生成出最自然的语言，但diversity太差，当然如果在decoding部分生成大量的N-best list的话，也是有很多不错的reply但都排名很靠后。</p>
<p>本文就是针对这样的一个问题，提出了用Maximum Mutual Information（MMI）作为目标函数来提高reply的diversity和实用性。MMI这个目标函数在Jiwei Li的多篇文章中都出现过，他很喜欢用这个来代替MLE作为目标函数来解决问题。互信息的方程如下：</p>
<p><img src="media/1.png" alt="1"></p>
<p>经过简单的推导，可得出下式作为目标函数：</p>
<p><img src="media/2.png" alt="2"></p>
<p>而，一般的seq2seq采用MLE，如下式：</p>
<p><img src="media/4.png" alt="4"></p>
<p>本文方法比传统seq2seq多了后面的一项。</p>
<p>p(T)其实是一个语言模型，为了在目标中控制reply的多样性，添加一个惩罚系数，如下式：</p>
<p><img src="media/3-1.png" alt="3"></p>
<p>这个式子记作(4)，经过简单的推导得到下式：</p>
<p><img src="media/5.png" alt="5"></p>
<p>记作(5)</p>
<p>作者根据式子(4)和(5)提出了两种MMI，分别是MMI-antiLM和MMI-bidi。</p>
<p>首先是antiLM，单看-log p(T)这一项，其实就是一个语言模型，anti表示反着的，因为有个负号。这一项不仅仅可以影响到你生成reply的diversity，同时也可以影响到你生成的reply是否是grammatical的，其实是一把双刃剑，需要做好控制，一般来说lambda小于1之后，后一项的影响相对较小了。</p>
<p>本文用一个带权重的语言模型U(T)来替换当前的p(T)，如下式：</p>
<p><img src="media/6.png" alt="6"></p>
<p>这里g(k)是权重，k是index，g(k)的特点是随着k的增加单调递减。这样做有两个目的：</p>
<p>1、decoding时对先生成的词的惩罚比后生成的词的惩罚对diversity的影响更大。</p>
<p>2、随着decoding部分的输入对后续生成影响的减弱，语言模型U(T)将会占主导地位，reply后面的部分也会非常grammatical。</p>
<p>bidi这个目标函数的思路是，先从第一项来生成N-Best List，然后用第二项对其进行排序，将diversity更好的reply放在前面。</p>
<p>在训练过程中，仍旧是采用MLE，但在测试的时候，用本文提到的MMI来做测试。</p>
<p>这个结果是由MMI-antiLM产生的：</p>
<p><img src="media/7.png" alt="7"></p>
<p>这个结果是MMI-bidi产生的：</p>
<p><img src="media/8.png" alt="8"></p>
<p>生成的reply确实seq2seq更加有营养。</p>
<p>本文解决问题的一个思路是很有借鉴意义的，正如abstractive summarization中有一篇paper用MRT来替换传统的MLE作为目标函数，将评价指标考虑进了目标函数中进行优化，起码在benchmark上得到非常好的结果。这其实是一条不错的路，就是将你当前的评价指标融入到你的优化目标中进行优化学习，自然会得到比单纯地用MLE来优化要好的多，也有很多的paper在用这样的思路解决问题。我们不仅仅满足于可以生成一个grammatical的reply，我们更需要的是有意义的、有实际使用价值的bot。另外就是具体到目标函数的建模，如果你希望目标中减小哪些因素对目标的影响，就增加一项惩罚项，这也是做优化时候的一般方案，但在解决具体问题时会非常有效。本文虽然针对的是bot reply的生成问题，其实可以推广到一般的自然语言生成问题上来，只是要涉及到MLE做生成都可以换成本文的方法来提升相应的指标。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-10T17:29:40.000Z"><a href="/2016/07/10/A-Persona-Based-Neural-Conversation-Model-PaperWeekly/">2016-07-10</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/10/A-Persona-Based-Neural-Conversation-Model-PaperWeekly/">A Persona-Based Neural Conversation Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文将分享的paper在生成对话时考虑了user identity，解决了多轮对话中response不一致的问题，如下图：</p>
<p><img src="media/1.png" alt="1"></p>
<p>同样的问题，换一种问法之后得到了不同的答案，而且答案不一致。paper的题目是<a href="https://arxiv.org/pdf/1603.06155.pdf" target="_blank" rel="external">A Persona-Based Neural Conversation Model</a>，作者是来自斯坦福的博士生Jiwei Li，他是一个非常高产的作者，很多文章质量都非常不错。文章最早在2016年6月8日在arxiv上发出。</p>
<p>本文针对的问题是开头说的多轮对话中response不一致的问题，这个问题很关键，多轮对话在工程应用中的意义更大，一致性是一个基础问题，解决不好，效果就会非常地差。本文针对这个问题，将user identity（比如背景信息、用户画像，年龄等信息）考虑到model中，构建出一个个性化的seq2seq模型，为不同的user，以及同一个user对不同的对象对话生成不同风格的response。</p>
<p><img src="media/2.png" alt="2"></p>
<p>本文一共提出两个模型，一个是Speaker Model，一个是Speaker-Addressee Model。</p>
<p>上图是Speaker Model的架构示意图，是一个典型的seq2seq模型，不同的地方在于在decoding部分增加了一个speaker embedding，类似于word embedding，只是说对用户进行建模。因为无法对用户的信息显式地进行建模，所以用了一种embedding的方法，通过训练来得到speaker向量，下面左边的图是speaker向量在二维平面上的表示，具有相似背景信息的user就会很接近，与word向量一个道理。decoding部分计算lstm各个gate用下式，vi表示speaker embedding。</p>
<p><img src="media/3.png" alt="3"></p>
<p>第二个模型是Speaker-Addressee Model，这个模型与上一个模型思想是一致的，只是考虑了一种更加细致的情况，在多人多轮对话（电视剧）中，每个人对不同的人说话style是不同的，所以应该在这类问题中需要考虑说话的对象，用V(i,j)来表示speaker i和j之间的这种关系，decoding部分计算如下式：</p>
<p><img src="media/4.png" alt="4"></p>
<p>seq2seq模型在最后的生成过程中，经常遇到一个问题，就是生成一些“呵呵”的话，就是指一些常见的、安全的但没有什么营养的话。针对这个问题，本文用beam search生成一个的N-Best List，用一些规则来重新排序得到更好的结果。</p>
<p>训练语料这块，第一个模型主要是依靠Twitter的数据，第二个模型是依靠电视剧的剧本数据。关于语料这块，它是一个非常重要的问题，甚至是决定了你能够用deep learning技术来解决某个具体问题的关键。开放域的bot数据充分，但模型还有待提高；闭域的bot数据匮乏，虽然可以解决实际问题，但开发成本太高，而且横向扩展性很差，基本上都是针对一个客户做一个系统，可复用的地方并不多，大量的人工feature需要花费大量的人力、物力、财力来做，所以这个问题也是bot在实际应用中难以使用deep learning的最大原因。有一种思路是用增强学习来造数据，但前提是你得有一个质量不错的生成模型，才能不断地自学习，这个问题便成了一个先有鸡先有蛋的问题，是一个死循环。</p>
<p>下图展示了Speaker Model在解决多轮对话一致性问题上的突出表现，User1采用了本文模型，User2是普通的seq2seq模型。</p>
<p><img src="media/5.png" alt="5"></p>
<p>将用户信息建模是一个必要的部分，从本文的结果来看确实有更好的效果。多轮对话不仅仅是考虑用户信息，还要考虑大量的上下文信息，或者说是历史信息，尤其是对于具体的企业客服来说，历史信息和用户信息都非常重要。当然如果是娱乐用的机器人，用户的情绪也是一个非常有意思的信息，情感分析已经有不错的正确率，所以考虑人的情绪也是一件非常好玩的事情。本文用了类似于word embedding的思路，对user infomation进行建模表示，同样的思路可以用于情绪、上下文信息中。我相信context的建模应该也有很多人在做，后续的paper也一定会读到，届时再分享。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-10T03:42:57.000Z"><a href="/2016/07/09/A-Dataset-for-Research-on-Short-Text-Conversation-PaperWeekly/">2016-07-09</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/09/A-Dataset-for-Research-on-Short-Text-Conversation-PaperWeekly/">A Dataset for Research on Short-Text Conversation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文是一篇2013年的bot文章，题目是<a href="http://staff.ustc.edu.cn/~cheneh/paper_pdf/2013/HaoWang.pdf" target="_blank" rel="external">A Dataset for Research on Short-Text Conversation</a>，作者来自中科大和华为诺亚方舟实验室。</p>
<p>本文最大的贡献在于release出一个大型短文本对话语料，并且提出了一种基于检索的对话生成模型。到底是否是第一个用检索的方式来解决bot问题我不得而知，可以确定的一点是很多现在活跃在市面上的“逗逼”bot都是基于这个模型做的。</p>
<p>语料的数据来自于新浪微博，大概的收集过程如下图：</p>
<p><img src="media/1.png" alt="1"></p>
<p>首先选择10个NLP领域比较活跃的用户，然后依次作为种子用户，进行爬取，直到获得3200与NLP和ML相关的用户，然后抓取每个用户的微博和下面的评论，时间跨度为2个月。这样定向爬取的好处是选择的用户和所发表的微博涉及的领域比较窄，而不至于天马行空什么都有。</p>
<p><img src="media/2.png" alt="2"></p>
<p>数据准备好了之后，就是模型部分。模型一共分为两步，第一步是选择出评论候选列表，第二步是在候选列表中进行排序。候选列表选择一共分为三个baseline模型，如下：</p>
<p>1、Post-Response Semantic Matching</p>
<p>根据微博和评论之间的语义匹配程度选择出10个候选评论。</p>
<p>2、Post-Response Similarity</p>
<p>根据微博和评论之间的相似度选择出10个候选评论。</p>
<p>3、Post-Post Similarity</p>
<p>根据微博和微博之间的相似度选择出10个候选评论，即用相似微博的评论作为候选评论。</p>
<p>给定一条微博之后，模型会通过三个baseline各选择10条评论，构成一个&lt;=30的评论候选列表，然后进行标注。标注工作是将评论分为两类，即suitable和unsuitable，即正样和负样。判断一个评论是否是suitable一共有三个准则：（1）semantic relevance，这个准则是判断微博和评论是否语义相关；（2）logic consistency，这个准则是判断微博和评论是否在逻辑上是一致的；（3）speech act alignment，这个准则是判断微博和评论在act方面是否是对齐的。</p>
<p>接下来就是通过标注数据进行排序，排序学习的目标是让正例的score比负例的score更大。</p>
<p>基于检索的bot解决方案是一种常见的方案，这种方案的重点在于知识库的质量，也就是那个database，一个query对应多个reply。如果只是简单的对话，效果会不错，而且如果知识库很有特点的话，reply经常会有一些意想不到的好玩的话，小黄鸡当年在人人网上火了好一阵子。但稍微复杂的问题，知识库的应变能力差的缺点就暴露出来了，比如query中有named entity，并且这个entity在知识库中没有出现过，这时reply就会出现牛头不对马嘴的情况，解决单轮对话都存在很大的缺陷，那么解决多轮对话就会更困难。虽然说，可以通过做query和reply、query和query之间语义层面相似度的计算，比如用一些成熟的deep learning技术来做。但根本上并没有解决这种方法的弊端，倒是非常垂直的闭域bot可以考虑用这个方案加上一些策略来解决，比如企业客服bot，因为知识库规模小，根据企业的资料和一些过往的用户对话数据可以建设一个质量不错的知识库，从而得到质量不错的bot。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-10T01:08:36.000Z"><a href="/2016/07/09/Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation-PaperWeekly/">2016-07-09</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/09/Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation-PaperWeekly/">Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>从今天开始后面的paper都与bot有关，除非arXiv刷出一些好玩的paper。本文是<a href="http://cn.arxiv.org/pdf/1607.00970" target="_blank" rel="external">Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation</a>，2016年7月4日发在arxiv上，作者是来自北京大学的博士生<a href="http://sei.pku.edu.cn/~moull12/" target="_blank" rel="external">Lili Mou</a>。</p>
<p>这里所讲的bot是指chat bot，也是当下研究领域最火的应用之一。在实际的工程应用中使用的方法可以分为两类，基于rule、template的和基于database query的，应用范围比较窄，比如垂直领域的客服机器人，解决的问题通常都是一个闭域的问题；而真正的AI是应该可以解决开域问题的，无论问什么样的问题，都会得到一个满意的答案，当然，现在的研究水平还难以达到这样的境界。最近几年随着深度学习技术的火热，nlp领域中很多任务都得到了长足的进步，现在最流行的解决方案是seq2seq，尤其是在自然语言生成任务中得到了广泛的应用。简单bot的问题可以理解为给定一个query，生成一个reply，这样的bot是single turn，研究意义大于应用意义。更多的实际问题都是一个multi turn问题，以客服bot为例，单轮对话很难解决了客户的疑问，一般都是通过多轮对话来帮助用户得到满意的答复。关于多轮bot的文章，后面会慢慢涉及到，今天分享的paper是关于单轮、短文本的对话生成。</p>
<p>生成式的bot比起基于rule、template和database query的bot具有更加灵活的特点，不仅仅拘泥于现有的rule、template和database，而是可以生成更加多样性的reply。但生成式的bot也有一个非常显著的问题，就是经常生成一些非常“呵呵”的reply，比如“我不知道”，“我也是”等等没有营养但绝对“安全”的话，导致了这种bot没有什么实用价值。产生这个问题可能有两个原因：一是在decoder部分以log概率最大为目标，而不是别的目标，所以容易生成一些没有意义的人类语言，因为训练语料中这样无意义的reply会经常出现，用deep learning从data中抓feature的时候就会出现这样的问题；二是query的信息量太少，encoder捕捉的意思有限，导致了生成“呵呵”的reply。</p>
<p>本文旨在提出一种叫做content introducing的方法来生成短文本reply，一共分为两个step，如下图：</p>
<p><img src="media/1.png" alt="1"></p>
<p><b>step 1</b> 给定query之后，预测一个keyword作为reply的topic，这个topic词性是名词，这里的keyword并不能捕捉复杂的语义和语法，而只是根据query的每个词来预估出一个PMI（Pointwise Mutual Information）最高的名词作为keyword，两个单词之间的PMI由下式计算：</p>
<p><img src="media/2.png" alt="2"></p>
<p>每个单词与query之间的PMI由下式计算：</p>
<p><img src="media/3-1.png" alt="3"></p>
<p>虽然数学上不太严谨，但后面的实验表明用这个来计算结果还是不错的。</p>
<p><b>step 2</b> 本文的模型叫做Sequence To Backward and Forward Sequences，首先进行backward step，给定一个query，用encoder表示出来得到一个context，decoder的部分首先给定keyword作为第一个词，然后进行decoding，生成的这部分相当于keyword词前面的部分；接下来进行的是forward step，也是一个典型的seq2seq，用encoder将query表示成context，然后给定backward生成的话和keyword作为decoder的前半部分，继续decoding生成后半部分。整个的流程这样简单描述下：</p>
<p>query + keyword =&gt; backward sequence</p>
<p>query + keyword + backward sequence(reverse) =&gt; forward sequence</p>
<p>reply = backward (reverse) sequence + keyword + forward sequence</p>
<p>传统的seq2seq模型都是从第一个词生成到最后一个词，无法生成指定词，而本文的模型可以生成指定词，并且该词可以出现在reply的任意位置。</p>
<p>数据集是从百度贴吧上爬下来的对话数据，规模有500k的query reply pairs，PMI统计是由100M的query reply paris。结果是与seq2seq进行比较，本文模型得到了更好的结果。下图展示了本文的example：</p>
<p><img src="media/4.png" alt="4"></p>
<p>本文用keyword来做topic的思路是一个很好的思路，会让算法生成的reply更加有营养，这个在单轮的应用背景下可以取得不错的结果。但是本文用topic的思路和处理方法太多简单，如果考虑到多轮对话的问题，我想用上下文信息来预测topic，而不是只考虑该句query的信息，而且不仅仅用一个单词来做topic，可能还会是短语，也可能是语义层面上的topic，而不仅仅是从一个候选列表中选择单词来作为topic。文章的思路很有启发性，我个人认为生成式的bot在闭域中应用是一个大趋势，传统的rule、template、database都会被替代，但真实应用场景中的bot需要将context做好处理，然后作为先验知识，来生成reply。其实难点也就在context的处理上，包括user profile，dialogue history，user current state等等各种context信息。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-09T04:35:03.000Z"><a href="/2016/07/08/生活毕竟不是一场秀/">2016-07-08</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/08/生活毕竟不是一场秀/">生活毕竟不是一场秀</a></h1>
  

    </header>
    <div class="entry">
      
        <p>生活毕竟是生活，生活的本质应该是生活本身，而不是一场秀。事业再好，终究还是为了有一个更好的生活，而家庭是生活中最重要的环节，永远都是应该是最重要的，抛去家庭来谈事业，简直滑稽。</p>
<p>为什么要伪装自己，在别人的面前表现出一个不同的自己？人与人为什么会不平等，就因为你现在处在更好的位置？一堆长者围着一个年轻人转，就因为年轻人是领导的秘书？说好的人生而平等呢？人的特征那么多，为什么只在乎地位这一个feature呢？才华、修养都是人的重要feature，为什么大家在地位面前都变得那么地低三下四呢？这不是生活的意义。</p>
<p>在酒桌之上，在领导面前，在地位和权势面前，为什么大家都变了一个模样？都在表演自己的人生，都在说一些虚伪的假话，都在迎合着一些人的需求。</p>
<p>忘了吗？每个人其实都是独立的，是一个活生生的个体，独立地活在这个世界上。难道家庭和事业比起来不重要吗？我一点都不认同，我觉得人都是平等的、独立的。</p>
<p>感谢我的妻子，我一直觉得自己就像是一只牢笼里的小鸟，在一个非常狭窄的区域内飞来飞去，我自以为我是在广阔的天空里，后来遇到了另外一只美丽的小鸟，带着一起飞，飞向了真正的天空，看到了更大的世界，明白了真正的人生。</p>
<p>我希望自己以后的孩子可以自由自在、不卑不亢地活在这个世界上，并不会因为自己的富有或贫穷而对这个世界和这个世界上的人产生偏见，我希望他爱读书，希望他可以看到更大的世界、真正的世界，而不是为了一个目的和身份苟活在这个世界上。</p>
<p>自由不只是身体上的自由，更是心灵的自由，一种真正的独立和平等，大家不论身份的高低都可以坐在一起进行平等地交流，没有半点虚伪，没有半点惶恐，只有最简单的纯真。</p>
<p>此刻，我想哭，因为有太多的无奈，但此刻，我也想笑，因为未来的生活会更加美好，因为我知道了美好的世界是可以通过自身的努力得来的，而不需要谁的恩赐和施舍。</p>
<p>生活毕竟不是一场秀，而是自己最真实、最纯粹、最华丽的人生。</p>
<p>献给那些生活在种种挣扎中的人们。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-07T00:16:17.000Z"><a href="/2016/07/06/关于创业的几点思考/">2016-07-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/06/关于创业的几点思考/">关于创业的几点思考</a></h1>
  

    </header>
    <div class="entry">
      
        <p>北京之行非常充实，白天上课，晚上和业界一线的创业者们交流，收获颇丰。自己之前对创业有一些朦胧的感觉和认识，但并不成熟，这次交流之后，有了一个更加清晰的认识，在这里做个总结。</p>
<p>1、SaaS（Software as a Service）+ B2B的模式确实很棒，这里讲的SaaS服务比较狭隘，特指用一些算法和模型来帮助中小企业解决他们解决不了的问题，并不包括工具类的SaaS。这种商业模式对创业团队的学习能力和研发能力有非常高的要求，团队短小精悍，可以快速对问题进行建模，快速开发出原型系统和产品。这种模式可以做成Private解决方案式的业务，此种业务对需求的分析和对自身的定位非常重要，一定要做的非常精和细，抓住企业用户的需求来展开业务；还有一种是做Public，提供算法API供企业用户调用，算是一种平台。SaaS+B2B最关键的是技术能力，不需要太多无关的人。SaaS是一种趋势，你公司中的很多业务其实都是不需要自己来打理，只需要找到做相应工作的SaaS公司即可，比如：会计和税务，报账，HR等等。他们做这个比你更加专业，也比你花钱雇人更加实惠。所以，这个世界是合作的，你做好、做精你的事情，其他耗时但有用的事情可以找专业的人来做，各赚各的钱。</p>
<p>2、创业最难的是人才，是如何招到靠谱的人才。创业公司太多了，而且创业都是在用时间赌未来，不一定会成功，所以对人才的吸引力不如成熟的大公司那么大。初期的招人很难，也很重要，基本上就是从熟人下手，从身边的人下手，或者找一些志同道合的人才。我个人觉得，如果你不是特别急着追一个所谓的创业机会的话，招人这事是宁缺毋滥，就和找女朋友类似，看条件合适不合适根本没用，创业是很实在的事情，合适的人比牛逼的人更加重要，因为牛的人性格一般都不会那么地随性，都是有个性的人，性格合适会让这个团队走向一个正确的方向，而不是说凑了几个大牛校或者海龟，大家听起来title都不错，但合作起来却问题百出。说道人才，我觉得不论什么出身，什么专业背景，只要是学习能力特别强，这里不是说你GPA多少，高考分多少之类的，而是给你一个新的topic，你能在多短的时间对这个topic有一个不错的理解；动手能力特别强，毕竟产品需要编程实现，模型系统也需要编程实现，纸上学来终觉浅嘛；沟通能力特别强，听得懂别人在说什么，也让别人听得懂自己在说什么，谦虚谨慎才会有利于沟通和交流，才会让团队更加高效。</p>
<p>3、创业环境，这一点颠覆了之前的认识。一直不喜欢北京，一个像吸血鬼一样的城市，一个没有生活质量的城市。但不得不说，是最好的创业城市，因为资源都在这里。深圳虽然好，但人才方面相对匮乏，软件创业环境不如北京，可能硬件够好一些吧，做机器人创业的比较适合在那里。和来自硅谷的创业咖聊过之后，感觉美国的创业环境更加规范，毕竟人家做了那么多年，体制机制都非常规范，不会像中国这样出现一些诓骗的事情，违法的成本太高，所以大家都规规矩矩在一个框架内做事情，环境比较纯粹一些。当然，硅谷也面临着招人的问题，创业公司太多，大家都想要最好的人，所以都很难。现在的创业支持和各种资源都非常充分，环境非常好，难的不是找投资，不是组建公司，因为现在有很多的孵化器，各个阶段的孵化器，他们会帮你解决好各种各样的事情，难是难在你自己到底有没有这个能力，做好这件事情。</p>
<p>4、创业方向，我个人关注的是自然语言处理技术，所以对这个方向观察和思考的多一些。首先说，bot热，大热，从arxiv上paper的数量也可以看得出，bot这个领域是最火的，从各大公司对bot研发的投入力度来看，bot是下一个big thing，facebook甚至希望将bot作为入口，代替现在的操作系统，想一想都觉得这个世界是多么地美好和神奇。bot这个大理想非常丰满，现实中nlp技术的不成熟却显得非常骨感，bot paper发的热火朝天，但在工程应用上却难以被用到，世界上最远的距离就是从research到engineering的距离。bot是大趋势，但技术确实不成熟，所以带来了大量的机会，每个有积累、有准备的人都是公平的，大家都有机会在下一个big thing上分一杯羹。让机器来理解和灵活应用人类语言是一件非常难的事情，也是实现true AI中非常关键的一步，所以我看好这个方向。当然，如果你着急开始赚钱，nlp并不适合你，因为nlp是未来。</p>
<p>5、是否该创业，这个问题如果你犹豫了，那么其实也就有了明确的答案。太多的顾虑是做不成一件事情的，不如安心地做好现在的工作，一步一步地在现在的地方踏踏实实地混下去，混到更高的title，直到退休。如果你在这个问题上不犹豫，那么恭喜你，你是天生的创业者，但这离创业成功还有十万八千里。那天问了谷哥（clickstone）为什么创业这个问题，他是这么回答我的，就是想按照自己的方式去努力做成一件事情，得到一个不错的回报，这里的回报更多的是成就感，而不只是money。可能，我们踏踏实实在一家大公司工作，或者在体制内工作，会为很多举世瞩目的事情做出贡献，但个人太微不足道了，在这样的环境中，自己做的事情太小了，能够决策的东西也太少了，就像一颗螺丝钉一样，事情成功了，也没有什么成就感。所以说，做爱做的事情，并且做成。</p>
<p>6、厚积薄发，打好基础，放平心态，不要急着去和谁比较。谷哥说他的同学有的都已经是教授级别了，但近况没那么理想吧，创业者要耐得住寂寞，不去和人比，因为每个人都不一样，拿同一标尺来衡量是很幼稚的事情。既然，选择了这条路，就是要一直走下去，而不是瞻前顾后、犹豫不决。你过去放弃的、现在放弃的种种都会在未来的某一个时间换一种形式回报给你的。</p>
<p>一点思考，欢迎交流。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-06T20:08:50.000Z"><a href="/2016/07/06/cips总结和思考/">2016-07-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/06/cips总结和思考/">cips总结和思考</a></h1>
  

    </header>
    <div class="entry">
      
        <p>前几天在北京参加了一个深度学习的“速成班”，由中文信息学会（cips）举办的。一共分为四天，上午由一名老师讲授理论和前沿技术的发展情况，下午由一名刚刚毕业的博士讲授用tensorflow来实现上午的部分内容。覆盖的范围包括神经网络与表示学习，以及应用于自然语言处理中的神经网络模型。都是国内一线的老师在讲，所以内容的质量非常高，来参加课程的同学也都非常的热情，去的晚了连个像样的座位都找不到，每次茶歇时间，老师都会被围得水泄不通。</p>
<p>深度学习非常火爆，很多人都在传言学会了深度学习就可以拿到高薪，甚至都可以to be someone，说的神乎其神，将深度学习等价于几十万、几百万的年薪。个人感觉这样的说法并不负责任，有点过度宣传的意思，与之相关的培训班也跟着水涨船高，且不论教学质量如何，确实涌现出了很多的教学、培训班来为学生们提供捷径，质量良莠不齐，学生们觉得上了你这个基础班、提高班之后就会有一份特别棒的工作和薪水。这样的事情有点像迷信神一样，传教士们将舆论造好，说的神乎其神，然后开始传教，让大家来信教，小白们本身就没有太多的判断力，只是单纯地相信传教士们，相信这个世界有捷径通往成功。其实，仔细想一想，怎么可能？不管火的是深度学习还是别的技术，让你能够找到一个不错的工作或者不错的薪水都是需要具备很强的数学基础和编程实现能力，以及超强的学习能力，从而跟得上技术的发展，而不是说跟着几个班学过之后，就能够如何如何。大家盲目地追逐捷径，却忽略了本质问题，实在是令人觉得可笑，微博上总能看到一些懂deep learning的人可以拿到多少多少薪水，会用什么工具或者开发过什么工具的人可以拿到多少多少薪水，这些言论会让大家变得浮躁。虽然说deep learning很火，大家也看的很热闹，那么到底有多少人可以从深层次的角度上或者说数学层面上理解了deep learning？</p>
<p>随波逐流不难，难的是坚持自我。tensorflow很火，所以大家都开始转tensorflow，mxnet在微博上被人转来转去，再加上“mxnet的主创人员都拿到了几百万的年薪”这一微博发出之后，有一种mxnet暗流涌动的感觉。这个问题，我是这么看的，流行确实有流行的原因，但公关的因素也不少，但是往往大家分不清真的好用还是大公司在公关，但工具毕竟是工具，选择了一个趁手的兵器就应该坚持用下去，而不是根据别人的言论而改变自己的初衷。因为工具只是工具，目的就是为了快速地实现模型，得到结果，torch、theano、tf、mxnet、caffee哪种用熟练了，用到了极致都是高手，不需要那么地盲目崇拜。</p>
<p>上面的言论是我自己的一些观点，并不针对特定的人和工具，世上没有偏见，只是角度不同、理解不同产生了偏见。对一个事物的认识和学习，都是要经历一个迷信和质疑的过程，所谓尽信书则不如无书，有自己的观点和态度不仅仅是一个严肃的媒体应该具备的特质，一个有独立思考能力的人都应该有自己的态度和认知。</p>
<p>这次的课程上午的内容都非常的棒，下午的内容个人觉得有一点鸡肋，用tf来实现一些model，像是编程实验课，但课时太短，大家没有太多动手和思考的时间，有一点点填鸭地感觉。编程从来都应该是一个实践课，编程能力都是自己一行一行写出来的，不是从谁那里听来的，从0开始学，照着docs和demo一边写一边学，遇到不懂的地方，在github上发起issue来讨论或者在其他的论坛上进行讨论，step by step地学习，或早或晚地一定会get到这个技能。我个人觉得下午的时间还不如将上午的内容进行更加深入地讲解，因为明显感觉地到上午老师讲的有一点点赶，讲的不够充分和透彻，时间快到了12点的时候，开始慌张地收尾，还真如下午继续讲呢。</p>
<p>学习这个事情，没有什么捷径，理解一个概念，理解一行代码，理解一个框架都需要你扎扎实实地去实践、去体会，不是谁能够给你的。这次所谓的“速成班”就是帮你打开一扇窗，让你在一个更大的空间里来理解这个世界，至于怎么理解和理解地怎么样都只有靠自己。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-06T18:49:03.000Z"><a href="/2016/07/06/Towards-Abstraction-from-Extraction-Multiple-Timescale-Gated-Recurrent-Unit-for-Summarization-PaperWeekly/">2016-07-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/06/Towards-Abstraction-from-Extraction-Multiple-Timescale-Gated-Recurrent-Unit-for-Summarization-PaperWeekly/">Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>前几天因为去北京参加中文信息学会组织的深度学习“速成班”，一直都没空更新博客。今天分享的paper是昨天刚刚刷出的一篇关于自动文摘的paper，题目是<a href="http://cn.arxiv.org/pdf/1607.00718v1" target="_blank" rel="external">Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization</a>。</p>
<p>用seq2seq的思路来解决文本摘要问题仍停留在short text的生成水平上，最多到paragraph level。原因也比较简单，rnn也好，gru、lstm也罢，终究都面临着一个长程依赖的问题，虽然说gru、lstm等技术用gate机制在一定程度上缓解了长程依赖和梯度消失、爆炸的问题，但终究文本过长的话，神经网络的深度就会随之变得非常深，训练起来难度就会随之增加。所以，这也是为什么document level或者说multi document level的abstractive式的摘要生成问题至今都是一个难以解决的问题。确实，short text的理解、表示在一定程度上有了很大的突破，也可以在工程上有不错的应用，比如机器翻译。但text变了之后，一篇很长的文章如何更加准确地理解和表示是一个非常难的问题，attention是一个不错的解决方案，在decoder的部分不需要考虑encoder的全部，只需确定需要注意的几个点就可以了，其实人在看一篇长文的时候也是这样一种机制，从某种角度上来讲，attention在decoder时提供了一种降维的手段，让model更能捕捉到关键的信息。</p>
<p>对于document level的abstractive摘要问题，人是怎么做的呢？比如我写了一篇paper，最后写abstract的部分，基本上是从每个section中提炼出key sentences，组成一段abstract，其实这里有一点extractive的意思，但人和extractive不同的地方在于可以轻松地将each sentence连贯地表达出来，看起来不那么僵硬，更加地顺畅，当然也不会出现指示代词找不到实体的情况。本文的思路正是借鉴了人类在解决这个问题时所采用的一般思路，数据源是arxiv paper中的introduction和abstract部分。</p>
<p><img src="media/2.png" alt="2"></p>
<p>将document分解成多个paragraph，然后从每个paragraph中extract出key sentence作为该paragraph的target summary，每个document可以构造出多个(paragraph,key sentence) pair作为seq2seq的训练数据。生成摘要的过程正好相反，将document分解成paragraph，对每个paragraph用model生成summary，将所有的summary拼接起来形成abstract，然后与paper自身的abstract作对比。</p>
<p>这里从paragraph中提取key sentence用了最简单的TF-IDF来打分排序，当然给n个句子排序有很多的方法，比如textrank。(paragraph,key sentence) pair的训练是通过一个叫Multi Timescale Gated Recurrent Unit(MTGRU)模型来做的，这个模型乍一看好新鲜，其实是N年前一个叫做MTRNN模型将RNN替换为GRU的成果，gru、lstm的变种非常的多，本文的这个模型是其中一个，之所以选择用这个模型来解决问题，是因为多个timescale可以在收敛速度上有更大的优势，并且在自然语言这种层次性的问题上有天然的优势。model的结构如下图</p>
<p><img src="media/1.png" alt="1"></p>
<p>在GRU的基础上增加一个时间项tao，用来控制gru的时间尺度，tao越大，表示model可以越好地捕捉序列数据中的slow features，不知道理解的对比对，这里的slow features是不是可以理解为更大的context window，控制着context的颗粒度。MTGRU可以看作是GRU的一般表示，当tao=1时，自动退化为GRU。</p>
<p>与传统的GRU进行了对比实验，证明了该model在speed和performance上均有更好的表现。下图展示了生成的一些结果：</p>
<p><img src="media/3.png" alt="3"></p>
<p>输入的是本文的introduction部分，输出的是每段生成的summary。</p>
<p><img src="media/4.png" alt="4"></p>
<p>这个是本文算法生成的摘要和纯extractive方法的对比，明显比extractive的方法概括地更加全面。</p>
<p>本文是一篇占坑的文章，内容并不完整，提出了MTGRU model来替换一般的GRU，但并不是full data driven，用了一些extractive的手段来辅助进行训练，在文章的future work这部分作者也提到了下一步要做成一个真正的data driven的model，每个paragraph的target summary也是data driven的，而不是用extractive的提取出来的，我在想，是否可以构造一个hierarchy model，一个维度在训练paragraph到sentence的mapping，一个维度在训练document到abstract的mapping，这个idea可以认真琢磨下，也欢迎大家讨论。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-29T17:54:35.000Z"><a href="/2016/06/29/教机器学习表示/">2016-06-29</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/29/教机器学习表示/">教机器学习表示</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>让机器来理解人类语言一直都是人工智能的梦想，从词、短语、句子、段落到文档，每个层次的文本都承载着语义，机器对各个层次文本的理解都需要先对文本进行表示，用机器看的明白的形式来表示。那么，今天就来分享一篇关于表示的综述文章。</p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>从人类理解文本的角度来看，理解一篇文档需要理解其中的每个段落，而理解段落需要理解每个段落的每句话，理解每句话就得理解每句话中的每个词。所以，表示一篇文档，需要从表示一个词、一句话开始。关于词的表示，在之前很长一段时间都流行的是one-hot模型，每一个词都用一个词表大小的向量表示，向量中该词所在的位置是1，其余都是0，而句子或者文档的表示，以前都是用bag-of-words模型，该文本用一个词表大小的向量表示，向量中的元素表示对应位置的词的个数。这两种经典的模型用了很久，有一些非常明显的缺点，比如：规模太大，太过稀疏，没有考虑词序信息，而词序又是一种非常有用的信息。</p>
<p>终于，在神经网络语言模型的帮助下，词向量（word embedding）出现了[1]，最初词向量是语言模型的一个副产物，后来随着word2vec的热潮，词向量几乎成了NLP中模型的标配，紧接着就是sentence2vec，document2vec，everything2vec，整个世界都被vector表示了。词向量与之前的one-hot模型不同，用了一些低维的、稠密的实数向量来表示，虽然说不清每个维度都表示什么，但用起来效果就是棒。</p>
<p>本文要探讨的问题是如何学习用低维的、稠密的实数向量来表示词、句子和文档。</p>
<h1 id="语料"><a href="#语料" class="headerlink" title="语料"></a>语料</h1><p>对于监督学习来说，需要找到一些分好类的语料进行训练，比如情感分析、新闻分类之类的数据。</p>
<p>对于无监督学习来说，只需要找到大量的文本进行训练即可。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>关于词、句子表示的模型实在是太多了，本文只选择PaperWeekly中读过的paper里的模型。</p>
<p>首先介绍的是词表示模型。</p>
<p><img src="media/14672414619993.png" alt=""></p>
<p>1、CBOW模型[2]</p>
<p>1、2都是word2vec的模型，整体的思路差不多，都是用context来预测word（skip-gram是用word来预测context，把context和word的概念对等交换一下都一样的。），典型的语言模型。之前Bengio[1]的文章提出用词向量来表示一个词，通过前面的n个词来预测下一个词，重点关注的是词的生成情况，并没有对词向量本身进行探讨。而word2vec的作者Tomas Mikolov重点研究的是词向量，并不在意语言模型的部分，所以这里在各种先前研究成果的基础上进行了大量地“偷工减料”，以达到快速求解词向量的目的。</p>
<p>如左图，该模型将每个词赋以一个n维向量初值，将context中的每个词向量求和来表示context，拿来预测目标词，不断地训练得到最终的词向量。</p>
<p>2、SKIP-GRAM模型[2]</p>
<p>如右图，该模型与CBOW类似，只是用word来预测context。</p>
<p>3、GloVe模型[3]</p>
<p>该模型的思路是将全局词-词共现矩阵进行分解，训练得到词向量。整体上的思路和推荐系统当年横扫Netflix百万美元比赛的LFM模型类似，也和信息检索中LSI的思路类似。不同的地方是，本文采用的词-词共现矩阵比起词-文档矩阵更加稠密，模型中对低频词和高频词的影响做了一定地弱化处理。模型的目标函数如下：<br><img src="media/14672420569666.png" alt=""></p>
<p>这里的f(x)是一个权重函数，具有以下的特点：</p>
<p>(a) f(0) = 0</p>
<p>(b) f(x)是增函数，这样低频词不会被over weight。</p>
<p>(c) 当x很大时，f(x)相对小一些，这样高频词也不会被over weight。</p>
<p>根据以上的特点，作者构造了下面的函数：</p>
<p><img src="media/14672421669446.png" alt=""></p>
<p>其次介绍的是句子表示模型。</p>
<p>1、PV-DM模型[4]</p>
<p><img src="media/14672421910018.png" alt=""></p>
<p>句子模型1和2是word2vec作者的进一步工作，乍一看模型和CBOW很像。不同的地方在于，输入中多了一个paragraph vector，可以看做是一个word vector，作用是用来记忆当前上下文所缺失的信息，或者说表征了该段落的主题。这里，所有的词向量在所有段落中都是共用的，而paragraph vector只在当前paragraph中做训练时才相同。后面的过程与word2vec无异。</p>
<p>2、PV-DBOW模型[4]</p>
<p><img src="media/14672422824272.png" alt=""></p>
<p>这个模型看着雨SKIP-GRAM很像。这两个模型都是无监督学习模型，在准备数据时需要给每个paragraph定义一个id，以区分不同的paragraph。</p>
<p>3、Skip Thought Vectors模型[5]</p>
<p><img src="media/14672423416074.png" alt=""></p>
<p>本模型是一个无监督句子表示模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。模型采用了当下流行的seq2seq框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。</p>
<p>4、Sequence AutoEncoder LSTM模型[9]</p>
<p><img src="media/14672434462899.png" alt=""></p>
<p>该模型利用自编码器来进行sentence表示，从图中可以看得出是一种端到端的学习，只不过这里的input和target是同一句话。模型中用LSTM来对文本进行建模，整个过程和之前分享的seq2seq并无区别，属于比较简单的模型。</p>
<p>5、Hierarchical Attention AutoEncoder模型[8]</p>
<p><img src="media/14672435940204.png" alt=""></p>
<p>这个模型在模型4的基础上，用了分层和注意力的思想，更加复杂。在encoder部分，对每一个word进行表示，同时也对每一个sentence进行表示，用每句话最后一个词的state作为该句子的state，并且将每句话的表示综合起来作为整个输入的输出，也就是context。在decoder部分，生成词的时候会使用attention机制，在确定和input中哪句话的关系更加密切。</p>
<p>6、CNN模型[6]</p>
<p><img src="media/14672428840879.png" alt=""></p>
<p>这个模型是CNN在NLP中得到应用的一个比较早的模型，也是一个有监督模型。用一个k维向量表示一个词之后，很容易将一句话表示成一个矩阵，矩阵中的每一行都是一个词向量。那么既然得到了矩阵表示，很容易套用CNN在二维图像中的处理方法，一层层地堆叠起来，得到sentence的表示。只是说卷积窗口的选择，有两种思路，一种是窗口大小就是k，每一次卷积其实就相当于从n-gram中提取feature；另外一种思路是窗口大小小于k，和CNN处理图像类似的思路，从几个词的不同部分来提取feature。本模型采用的是第一种方式。</p>
<p>7、RCNN模型[7]</p>
<p><img src="media/14672425700988.png" alt=""></p>
<p>该模型是一个RNN和CNN的组合模型，CNN的卷积层用一个双向RNN模型来做，既用双向RNN弥补了CNN模型卷积窗口大小固定的问题，又利用了CNN提取feature的强大能力。这个模型也非常好地体现了deep learning模型的灵活性和组合性，不同类型的single模型通过一定的方式进行组合，可以衍生出多种多样的模型，很难从理论上讲哪个模型会更好，因为single模型都各有各的优点和缺点。该模型是一个有监督模型。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>本文只对比一下各个sentence表示模型的结果。</p>
<ul>
<li>Stanford Sentiment Treebank Dataset<br><img src="media/fig3.png" alt="fig3"></li>
</ul>
<p>在这个数据集上，居然是无监督的Paragraph Vector模型效果最好，虽然也好不到哪里去，但至少不比有监督学习的模型差。</p>
<ul>
<li><p>多种分类数据集<br><img src="media/fig2-1.png" alt="fig2"><br>这里的评价指标是分类正确率，从结果上看CNN作为一个有监督学习的模型好于另外两个无监督的模型，仅仅在SUBJ这个数据集上稍落后于skip thought vectors模型。</p>
</li>
<li><p>对比CNN和RCNN[7]<br><img src="media/fig1.png" alt="fig1"><br>这个对比结果来自RCNN模型的paper，很显然RCNN在作者选的几个数据集上都由于CNN，可以大胆推测一下，RCNN应该在第二类数据集上也能优于无监督模型。</p>
</li>
</ul>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>词表示、句子表示等各种层次上的表示有着各种各样的模型，几乎在arxiv上每天都可以刷出一篇文章是做这方面的，都说自己的模型是state-of-the-art。对于这个领域，我有下面几点思考：</p>
<p>1、关于对比结果的事情。数据集的不同、数据规模的不同、模型的不同、模型超参数的不同都会导致不同的结果和结论，在对比的过程中，我们往往是用尽浑身解数调出最好的参数给出一个最好结果，而对于对比的模型，往往就没有这么上心了，复现别人的模型或者用别人开源的代码来对比的时候，可能很难调出该作者模型的最优参数，所以每篇paper的结果也总是有一些出入，从上面的结果中也能看得出，同一个数据集，在不同paper作者复现下结果也是不同的。而且，对比的结果与所选的数据集也有很大的关系，有的模型在某一些数据集上表现非常好，然而在另外一些上就不那么尽如人意了，这也是一个对比的技巧吧，但真正在工程应用上，还是需要在自己的数据上做对比实验，而不是用所谓的“理论”来剖析哪种模型更牛逼。因为毕竟deep learning是data driven的模型，效果的好坏与数据有着直接的关系。</p>
<p>2、读了一些paper之后，看过很多别人的思路和model，有一个感觉就是思维一定要更宽一些。单模型效果一般的时候，可以考虑试一下组合模型，比如RCNN，可以考虑下端到端模型，比如Autoencoder和skip thought vector，可以考虑下分层模型，比如Hierarchical Autoencoder。无监督模型效果一般的时候，可以考虑用无监督作初值，代入到有监督模型中进行训练，一般来说会比纯粹的有监督或者无监督模型效果更好一些。模型看多了，就会有一种感觉，做什么事情的时候，可选的东西就会特别多，思路就会特别多，所谓精神病人思路广吧。</p>
<p>3、词向量的用法，得到一个好的词表示之后，可以用来做什么呢？首先可以继续表示句子，段落，然后做分类也好，计算相似度也罢；其次，可能会有一些比较好玩的东西，比如我用word2vec给自己之前写的应用rsarxiv（一个arxiv paper的推荐系统）做了一个paper knowledge graph，把author，subject，keywords连成了一张大图，在推荐paper的同时可以推荐其他的一些metadata，带来了更好的用户体验。同样的道理，有的人用这个来做app推荐，也有不错的效果。所以，我在想，一个模型其实不仅仅是一个用于特定事情的模型，如果你有很强的抽象问题的能力，你可以看得穿问题的本质，就可以很容易地将特定领域的模型应用于其他领域，有可能还会形成突破。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a></p>
<p>[2] <a href="http://cn.arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a></p>
<p>[3] <a href="http://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="external">GloVe: Global Vectors for Word Representation</a></p>
<p>[4] <a href="http://cn.arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="external">Distributed Representations of Sentences and Documents</a></p>
<p>[5] <a href="http://cn.arxiv.org/pdf/1506.06726v1.pdf" target="_blank" rel="external">Skip-Thought Vectors</a></p>
<p>[6] <a href="http://cn.arxiv.org/pdf/1408.5882.pdf" target="_blank" rel="external">Convolutional Neural Networks for Sentence Classification</a></p>
<p>[7] <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="external">Recurrent Convolutional Neural Networks for Text Classification</a></p>
<p>[8] <a href="http://arxiv.org/pdf/1506.01057.pdf" target="_blank" rel="external">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a></p>
<p>[9] <a href="http://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="external">Semi-supervised Sequence Learning</a></p>
<p>如果大家觉得有写的不够清楚的地方或者错误的地方，欢迎留言交流。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<p>知乎专栏<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">paperweekly</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-28T23:40:40.000Z"><a href="/2016/06/28/Deep-Reinforcement-Learning-with-a-Natural-Language-Action-Space-PaperWeekly/">2016-06-28</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/28/Deep-Reinforcement-Learning-with-a-Natural-Language-Action-Space-PaperWeekly/">Deep Reinforcement Learning with a Natural Language Action Space #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文继续分享一篇深度增强学习在NLP中应用的paper，题目是<a href="http://arxiv.org/pdf/1511.04636v5.pdf" target="_blank" rel="external">Deep Reinforcement Learning with a Natural Language Action Space</a>，作者是来自微软的<a href="http://washington.academia.edu/JiHe" target="_blank" rel="external">Ji He</a>博士，文章最早于2015年11月发在arxiv上，2016年6月8号update。</p>
<p>通过前两篇文章的介绍，基本对DQN在NLP中应用有了一个清晰的认识，与DQN之前应用不同的地方在于两个方面：</p>
<p>1、actions的量级很大。</p>
<p>2、transition tuple的具体形式随着模型来变化。</p>
<p>本文也是以text games为研究背景，将输入从state变为(state,action)对，提出了Deep Reinforcement Relevant Network(DRRN)模型。</p>
<img src="/2016/06/28/Deep-Reinforcement-Learning-with-a-Natural-Language-Action-Space-PaperWeekly/fig1.png" width="600" height="600">
<p>上图中，前两个是baseline模型，第三个是本文模型，理解起来都比较简单。</p>
<p>(a) Max-action DQN</p>
<p>该模型适用于每一个transition中actions的最大数量是已知的情况，将每个transition中state和actions拼接成一个向量作为输入，通过一个Deep Network得到每个action的Q值。</p>
<p>(b) Per-action DQN</p>
<p>该模型将每一对(state,action)拼接成一个向量作为输入，通过network得到每个action的Q值。</p>
<p>(c) DRRN</p>
<p>本文模型分别将每对(state,action)中的state和action单独构建network，分别学习出不同的表示，然后用一种逐元素操作方法得到Q值，比如对两个向量作内积。这里，state往往是一个比较长的文本，可能是几句话，而action一般来说是一个动词短语，通过不同的网络结构进行学习，得到相同维度的表示，然后做内积，内积就是相似度的一种表征，也就是本文模型中的relevant。</p>
<p>其实，对比着看不同DRL paper，只需要仔细对比看算法流程图，就知道哪些地方不同了，本文的如下图：</p>
<img src="/2016/06/28/Deep-Reinforcement-Learning-with-a-Natural-Language-Action-Space-PaperWeekly/fig2.png" width="600" height="600">
<p>本文算法中还有一个不同的地方在于，在策略选择上的trade-off，一般的方法均采用ε-greedy策略，本文用了一种softmax selection的方法来做exploration（对应着ε）策略，根据下面计算出的概率来进行选择：</p>
<img src="/2016/06/28/Deep-Reinforcement-Learning-with-a-Natural-Language-Action-Space-PaperWeekly/fig3.png" width="200" height="200">
<p>本文模型最大的优点在于可以处理比较复杂的action，不像<a href="http://rsarxiv.github.io/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/">Language Understanding for Text-based Games using Deep Reinforcement Learning</a>文章中只能处理一个action word加一个object word组成的command。</p>
<p>本文考虑问题的角度不同，不是传统RL给定一个state，然后通过一个最优的Q来确定一个最优的action，而是将state和action放在一个层面上来做计算，虽然最后也是通过最优的Q来选择action，但通过用action和state的相关性来计算Q，使得其具有更广的应用前景。</p>
<p>这是DQN在NLP中应用系列的最后一篇文章，文章数量比较少，所以不写综述了。整体的感觉是，应用还不太多，也没有看到特别惊艳的表现。不过，可以无穷无尽地构造训练用的样本是一个非常大的优点。三篇文章有两篇是研究text games的，只有一篇是做text generation的，并且DQN的痕迹很重，都是依着同一个框架进行修改和适应，并没有很多特别的地方。很期待，后面的研究可以将Deep Reinforcement Learning在NLP的各个任务中进行应用，像seq2seq+attention模型那样横扫整个NLP任务。</p>
<p>如果大家觉得有写的不够清楚的地方或者错误的地方，欢迎留言交流。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<p>知乎专栏<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">paperweekly</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-28T17:19:32.000Z"><a href="/2016/06/28/Generating-Text-with-Deep-Reinforcement-Learning-PaperWeekly/">2016-06-28</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/28/Generating-Text-with-Deep-Reinforcement-Learning-PaperWeekly/">Generating Text with Deep Reinforcement Learning #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>上一篇介绍了DQN在文字游戏中的应用，本文将分享一篇DQN在文本生成中的应用，将一个领域的知识迁移到其他领域应用的时候，都需要做概念上的等效替换，比如context可以替换为state，被预测的word可以替换为action。本文分享的题目是<a href="http://arxiv.org/pdf/1510.09202v1.pdf" target="_blank" rel="external">Generating Text with Deep Reinforcement Learning</a>，作者是来自National Research Council of Canada的<a href="http://www.site.uottawa.ca/~hguo028/mainpage.htm" target="_blank" rel="external">Hongyu Guo</a>研究员，文章最早于2015年10月30日submit在arxiv上。</p>
<p>语言模型往往用来生成文本，在很多例子中都有应用，比如：自动文摘、bot、机器翻译、QA等等。本文想要做的事情是用DQN来生成文本，起到一个语言模型的作用，并且这是第一次尝试用DQN来生成文本。仔细想想，DQN在解决video games时遇到的情况和现在不同，state可能还好，可生成的action数量远远大于游戏中action的数量，所以，如何解决action的问题对于DQN在NLP中的应用前景至关重要。目前来看，读此类的paper，需要关注的有两个部分：</p>
<p>1、action规模的问题如何解决？</p>
<p>2、DQN中每条数据集包含的元素与NLP问题中各个元素的对应关系。</p>
<p>本文解决第一个问题采用的方法是，用传统的语言模型decoder来为DQN的action生成candidated actions，虽然这个actions集合是动态，但对于每一条数据来说，action的数量只有很少了，与video games差不太多了。</p>
<p>NLP不像video games直接可以用游戏画面作为输入，用多层CNN提取feature进行action选择，因为text不仅仅是一个序列，而且是变长度的，所以一般来说也都是RNN来处理。本文模型如下图：</p>
<img src="/2016/06/28/Generating-Text-with-Deep-Reinforcement-Learning-PaperWeekly/fig1.png" width="600" height="500">
<p>首先，回答关心的第二个问题，DQN中各元素的对应关系（transition tuple）。</p>
<p>DQN中：(s(t),a(t),r(t),s(t+1))<br>本文：([EnSen(t),DeSen(t)],y(t),r(t),[EnSen(t),DeSen(t+1)])</p>
<p>整个模型是一个迭代decoding的过程，通过LSTM decoder生成最初的DeSen(t)之后，开始不断地迭代。在decoder的每一个time step，DQN会从DeLSTM使用的词表中选择一个可以获得最大reward的action作为该time step新生成的词，用这个新词来代替之前的旧词，生成新的状态DeSen(t+1)，依次迭代下去，每一次迭代都只生成一个新词来代替旧词，直到最后一个新词被生成。这里的reward r(t)是计算target sentence和DeSen(t+1)的相似度得来的。</p>
<p>本文在这个模型的基础上，尝试了在decoder部分用双向的LSTM来表示，并且用一个光滑的BLEU来做reward。这一点与<a href="http://rsarxiv.github.io/2016/05/17/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/">Neural Headline Generation with Minimum Risk Training</a>这篇文章从某一个角度来说有一点点类似，本文是用最终的评价指标BLEU作为reward，而那篇文章是用ROUGE指标作为优化函数，最终取得了非常令人满意的结果。</p>
<p>整个算法流程大体上都是遵循DQN的框架，只是细节有一些不同，如下图：</p>
<img src="/2016/06/28/Generating-Text-with-Deep-Reinforcement-Learning-PaperWeekly/fig2.png" width="600" height="600">
<p>实验部分，用了10000个sentences作为训练集，source和target是一样的，是一个autoencoder问题，对比了只用LSTM decoder和本文模型的结果，如下表：</p>
<img src="/2016/06/28/Generating-Text-with-Deep-Reinforcement-Learning-PaperWeekly/fig3.png" width="300" height="300">
<p>验证了本文方法的有效性。</p>
<p>本文较前一篇文字游戏的文章更难的一点是处理大量actions的方法，上一篇其实仍旧是个游戏，actions的数量在一个非常有限的范围内，本篇是做语言模型的，actions和词汇表一样大，这是DQN在NLP中应用最头疼的问题。本文用了DeLSTM在每个time step中使用的Top N个词作为候选actions，很好地解决了这个问题，那么到底有没有更好的方法来减少actions呢？有没有不需要用这么复杂的模型来做处理呢？想到一个idea，用char-level来做语言模型，用到的vocabulary size远远小于word-level，对于DQN来说，actions集合非常固定，并且非常小，只是增大了state的表示。</p>
<p>如果大家觉得有写的不够清楚的地方或者错误的地方，欢迎留言交流。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<p>知乎专栏<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">paperweekly</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-27T15:30:10.000Z"><a href="/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/">2016-06-27</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/">Language Understanding for Text-based Games using Deep Reinforcement Learning #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>继上上周的机器阅读理解和上周的自动文摘分享之后，本周开始分享几篇Deep Reinforcement Learning在NLP中应用的paper。在网上看到过这样的言论，一些大牛认为深度增强学习是人工智能研究的未来，是真正的AI，还给出了一个这样的公式：DL+RL=AI。其实，增强学习一直都是机器学习中常用的一种无监督学习方法，随着deepmind公司的一些好玩的成果，比如：用程序来玩简单的video games，AlphaGo战胜李世乭等等，深度增强学习随之兴起。今天将分享的paper是<a href="http://arxiv.org/pdf/1506.08941v2.pdf" target="_blank" rel="external">Language Understanding for Text-based Games using Deep Reinforcement Learning</a>，作者是来自麻省理工学院的博士生<a href="http://people.csail.mit.edu/karthikn/" target="_blank" rel="external">Karthik Narasimhan</a>和<a href="http://tejask.com/" target="_blank" rel="external">Tejas Kulkarni</a>，文章最早于2015年6月30日刊在arxiv上。</p>
<p>在介绍深度增强学习在NLP中的应用之前，需要简单介绍下增强学习和深度增强学习。</p>
<img src="/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/fig1.png" width="600" height="500">
<p>上图来自于deepmind David Silver的slide。</p>
<p>图中的大脑可以理解为一个agent，并且这个agent有一个action集合，地球可以理解为environment，并且environment每个time step都有一个状态state。增强学习想要做的一件事情是agent在某一个time step中接收来自environment的state，执行一个action，然后从environment中得到一个reward，根据state和reward继续选择一个action，目标是使得reward最大。整个过程是一个不断交互和反馈的控制过程，通过引入值函数Q(s,a)来计算当前state和action对应的未来所有reward的期望，这里涉及到一些细节，推荐看<a href="https://zhuanlan.zhihu.com/intelligentunit" target="_blank" rel="external">智能单元知乎专栏</a>的系列博客《DQN从入门到放弃》，包括Bellman Equation、多种增强学习模型、ε-greedy policy等等内容。</p>
<p>深度增强学习，这里专指DQN（Deep Q-Network），是将深度学习模型引入到了传统的Q-Network中，用深度神经网络来近似Q函数，构造输入和输出数据，进行端到端的训练来学习这个问题。deepmind那两篇有名的paper，<a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" target="_blank" rel="external">Playing Atari with Deep Reinforcement Learning</a>和<a href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf" target="_blank" rel="external">Human-level control through deep reinforcement learning</a>，第一篇最早提出了DQN的方法，另一篇提升了第一篇的模型，并且发表在了nature上，deepmind做的事情都是用计算机程序来玩video games，并且在多个游戏中取得了比人类玩家更好的成绩。由于都需要处理游戏画面，所以encoder部分都是采用多层CNN提取feature。</p>
<p>简单介绍了一些预备的内容之后，下面来介绍本文的内容。本文是基于一个文字游戏来展开的，通过text state，通常是一段比较长的介绍性文字，来给出一个合适的action进入下一个state。如下图：</p>
<img src="/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/fig2.png" width="400" height="400">
<p>上图中从state1 <b>The old bridge</b>通过选择<b>Go east</b>这个动作进入了state 2 <b>Ruined gatehouse</b>。</p>
<p>如果对DQN的相关东西有所了解的话，本文的模型就显得非常简单了。</p>
<img src="/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/fig3.png" width="400" height="400">
<p>模型分为两个部分，第一部分是representation generator，将state中的text用lstm处理，每个word对应一个time step，然后将所有time step的向量做一次平均池化处理，得到该状态的表示。video games因为输入的是游戏图像，所以用cnn来处理，而text games都是文字，用rnn来处理也最合适不过的了。另外一点是，本文也考虑过用lstm的last hidden state作为state的表示，但收敛速度没有平均池化快。</p>
<p>第二部分是action scorer，就是一个多层的神经网络，用state的表示作为输入，得到的输出是集合中每个action的score，而不是每一对action-state的score，这样的计算效率更高一些。</p>
<p>由此用神经网络模型近似得到了Q函数。本文特点强调了一点，为了保证计算效果，只选取游戏命令是一个action（比如：go）和一个object（比如：east）的组合，当然这一类命令也占据了游戏命令的绝大多数。从模型图中可以看出，计算action和object是同样的结构，最终该命令的Q值由action和object的Q值由两者的平均值来代替。</p>
<p>关于训练方法，和DQN也区别不大。最关键的一点是构造训练数据集，每一条数据格式是(s(t-1),a(t-1),r(t-1),s(t))，每次进行游戏的时候，都会有一个当前的state、action和reward，执行完action之后会得到一个新的state，将这四个元素保存为一条数据，积累大量的类似的数据，保存到一个数据池中，每次训练的时候从池中sample出，这个池是一个FIFO的队列。具体可看下面的流程图：</p>
<img src="/2016/06/27/Language-Understanding-for-Text-based-Games-using-Deep-Reinforcement-Learning-PaperWeekly/fig4.png" width="600" height="600">
<p>这里简单地说一下ε-greedy是怎么一回事。ε通常是一个非常小的值，这种策略一般被称为exploration，探索性的策略，面对未知的情况，随机选择一个action进行执行，探索一下会有什么情况发生，这种策略容易更新Q值；greedy策略，通常称为exploitation，利用性的策略，选择令Q值最大的action，不利于更新出更好的Q值，但可以得到相对更好的测试结果。</p>
<p>训练的目标函数是target与近似函数的差的平方，用一般的优化方法就可以训练了。其中，近似函数是我们需要学习的模型，target是通过Bellman方程来进行计算的（这里面涉及到的概念可以参考前面提到的智能单元的博客来学习）。</p>
<p>读完本文，有几点思考，分享一下：</p>
<p>1、增强学习看着也挺像监督学习的，到底有什么优势和区别吗？</p>
<p>我想它最大的优势是不是在于那个动态的、大型的数据池，可以源源不断地提供样本，可以说是无穷无尽的样本，这一点比一般的监督学习更加厉害，因为毕竟监督学习需要给定一个自带标签的数据集。通过不断的学习，来得到一个不错的模型。</p>
<p>2、DQN和经典的增强学习相比优势在哪里？</p>
<p>其实这个问题从某个角度上来看可以等同于Deep Learning与经典的机器学习相比，优势在哪里？一方面是神经网络模型强大地非线性函数近似，另一方面就是不需要人工feature，所有的feature都是从data中学习来的，典型的data-driven。</p>
<p>3、DQN在NLP中的应用，相比于DQN几乎没有什么新的东西，只是做了一些概念的替换。比如：在生成问题上，都是计算一个P(word|context)，这里将DQN中的state理解为context，将word理解为action。不过非常不同的一点是，nlp中的action space会非常大，因为对于生成问题来说，词汇表会变得非常大。这个问题该如何解决呢？大家可以期待明天的分享<a href="http://arxiv.org/pdf/1510.09202v1.pdf" target="_blank" rel="external">Generating Text with Deep Reinforcement Learning</a>。</p>
<p>如果大家觉得有写的不够清楚的地方或者错误的地方，欢迎留言交流。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<p>知乎专栏<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">paperweekly</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-25T17:59:44.000Z"><a href="/2016/06/25/教机器学习摘要/">2016-06-25</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/25/教机器学习摘要/">教机器学习摘要</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>文本摘要是自然语言处理中比较难的一个任务，别说是用机器来做文摘了，就连人类做文摘的时候都需要具备很强的语言阅读理解能力和归纳总结能力。新闻的摘要要求编辑能够从新闻事件中提取出最关键的信息点，重新组织语言来写摘要；paper的摘要需要作者从全文中提取出最核心的工作，然后用更加精炼的语言写成摘要；综述性的paper需要作者通读N篇相关topic的paper之后，用最概括的语言将每篇文章的贡献、创新点写出来，并且对比每篇文章的方法各有什么优缺点。自动文摘本质上做的一件事情是信息过滤，从某种意义上来说，和推荐系统的功能有一点像，都是为了让大家更快地找到感兴趣的东西，只是用了不同的手段而已。</p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>文本摘要问题按照文档数量可以分为单文档摘要和多文档摘要问题，按照实现方式可以分为提取式（extractive）和摘要式（abstractive）。摘要问题的特点是输出的文本要比输入的文本少很多很多，但却蕴藏着非常多的有效信息在内。有一点点感觉像是主成分分析（PCA），作用也与推荐系统有一点像，都是为了解决信息过载的问题。现在绝大多数应用的系统都是extractive的，这个方法比较简单但存在很多的问题，简单是因为只需要从原文中找出相对来说重要的句子来组成输出即可，系统只需要用模型来选择出信息量大的句子然后按照自然序组合起来就是摘要了。但是摘要的连贯性、一致性很难保证，比如遇到了句子中包含了代词，简单的连起来根本无法获知代词指的是什么，从而导致效果不佳。研究中随着deep learning技术在nlp中的深入，尤其是seq2seq+attention模型的“横行”，大家将abstractive式的摘要研究提高了一个level，并且提出了copy mechanism等机制来解决seq2seq模型中的OOV问题。</p>
<p>本文探讨的是用abstractive的方式来解决sentence-level的文本摘要问题，问题的定义比较简单，输入是一个长度为M的文本序列，输出是一个长度为N的文本序列，这里M&gt;&gt;N，并且输出文本的意思和输入文本的意思基本一致，输入可能是一句话，也可能是多句话，而输出都是一句话，也可能是多句话。</p>
<h1 id="语料"><a href="#语料" class="headerlink" title="语料"></a>语料</h1><p>这里的语料分为两种，一种是用来训练深度学习模型的大型语料，一种是用来参加评测的小型语料。</p>
<p>1、<a href="http://duc.nist.gov/" target="_blank" rel="external">DUC</a></p>
<p>这个网站提供了文本摘要的比赛，2001-2007年在这个网站，2008年开始换到这个网站<a href="http://www.nist.gov/tac/" target="_blank" rel="external">TAC</a>。很官方的比赛，各大文本摘要系统都会在这里较量一番，一决高下。这里提供的数据集都是小型数据集，用来评测模型的。</p>
<p>2、<a href="https://catalog.ldc.upenn.edu/LDC2003T05" target="_blank" rel="external">Gigaword</a></p>
<p>该语料非常大，大概有950w篇新闻文章，数据集用headline来做summary，即输出文本，用first sentence来做input，即输入文本，属于单句摘要的数据集。</p>
<p>3、CNN/Daily Mail</p>
<p>该语料就是我们在机器阅读理解中用到的语料，该数据集属于多句摘要。</p>
<p>4、Large Scale Chinese Short Text Summarization Dataset（<a href="http://icrc.hitsz.edu.cn/Article/show/139.html" target="_blank" rel="external">LCSTS</a>）[6]</p>
<p>这是一个中文短文本摘要数据集，数据采集自新浪微博，给研究中文摘要的童鞋们带来了福利。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>本文所说的模型都是abstractive式的seq2seq模型。nlp中最早使用seq2seq+attention模型来解决问题的是machine translation领域，现如今该方法已经横扫了诸多领域的排行榜。</p>
<p>seq2seq的模型一般都是如下的结构[1]：</p>
<img src="/2016/04/24/自动文摘（五）/model.png" width="600" height="800">
<p>encoder部分用单层或者多层rnn/lstm/gru将输入进行编码，decoder部分是一个语言模型，用来生成摘要。这种生成式的问题都可以归结为求解一个条件概率问题p(word|context)，在context条件下，将词表中每一个词的概率值都算出来，用概率最大的那个词作为生成的词，依次生成摘要中的所有词。这里的关键在于如何表示context，每种模型最大的不同点都在于context的不同，这里的context可能只是encoder的表示，也可能是attention和encoder的表示。decoder部分通常采用beam search算法来做生成。</p>
<p>1、Complex Attention Model[1]</p>
<img src="/2016/04/24/自动文摘（五）/complex.png" width="400" height="650">
<p>模型中的attention weights是用encoder中每个词最后一层hidden layer的表示与当前decoder最新一个词最后一层hidden layer的表示做点乘，然后归一化来表示的。</p>
<p>2、Simple Attention Model[1]</p>
<img src="/2016/04/24/自动文摘（五）/simple.png" width="400" height="650">
<p>模型将encoder部分在每个词最后一层hidden layer的表示分为两块，一小块用来计算attention weights的，另一大块用来作为encoder的表示。这个模型将最后一层hidden layer细分了不同的作用。</p>
<p>3、Attention-Based Summarization(ABS)[2]</p>
<p>这个模型用了三种不同的encoder，包括：Bag-of-Words Encoder、Convolutional Encoder和Attention-Based Encoder。Rush是HarvardNLP组的，这个组的特点是非常喜欢用CNN来做nlp的任务。这个模型中，让我们看到了不同的encoder，从非常简单的词袋模型到CNN，再到attention-based模型，而不是千篇一律的rnn、lstm和gru。而decoder部分用了一个非常简单的NNLM，就是Bengio[10]于2003年提出来的前馈神经网络语言模型，这一模型是后续神经网络语言模型研究的基石，也是后续对于word embedding的研究奠定了基础。可以说，这个模型用了最简单的encoder和decoder来做seq2seq，是一次非常不错的尝试。</p>
<p>4、ABS+[2]</p>
<p>Rush提出了一个纯数据驱动的模型ABS之后，又提出了一个abstractive与extractive融合的模型，在ABS模型的基础上增加了feature function，修改了score function，得到了这个效果更佳的ABS+模型。</p>
<p>5、Recurrent Attentive Summarizer(RAS)[3]</p>
<p>这个模型是Rush的学生提出来的，输入中每个词最终的embedding是各词的embedding与各词位置的embedding之和，经过一层卷积处理得到aggregate vector：</p>
<img src="/2016/04/30/自动文摘（六）/formula21.png" width="300" height="400">
<p>根据aggregate vector计算context（encoder的输出）：</p>
<img src="/2016/04/30/自动文摘（六）/formula22.png" width="300" height="400">
<p>其中权重由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula23.png" width="300" height="400">
<p>decoder部分用RNNLM来做生成，RNNLM是在Bengio提出的NNLM基础上提出的改进模型，也是一个主流的语言模型。</p>
<p>6、big-words-lvt2k-1sent模型[4]</p>
<p>这个模型引入了large vocabulary trick(LVT)技术到文本摘要问题上。本方法中，每个mini batch中decoder的词汇表受制于encoder的词汇表，decoder词汇表中的词由一定数量的高频词构成。这个模型的思路重点解决的是由于decoder词汇表过大而造成softmax层的计算瓶颈。本模型非常适合解决文本摘要问题，因为摘要中的很多词都是来自于原文之中。</p>
<p>7、words-lvt2k-2sent-hieratt模型[4]</p>
<img src="/2016/05/07/自动文摘（七）/Pointer.png" width="600" height="800">
<p>文本摘要中经常遇到这样的问题，一些关键词出现很少但却很重要，由于模型基于word embedding，对低频词的处理并不友好，所以本文提出了一种decoder/pointer机制来解决这个问题。模型中decoder带有一个开关，如果开关状态是打开generator，则生成一个单词；如果是关闭，decoder则生成一个原文单词位置的指针，然后拷贝到摘要中。pointer机制在解决低频词时鲁棒性比较强，因为使用了encoder中低频词的隐藏层表示作为输入，是一个上下文相关的表示，而仅仅是一个词向量。这个pointer机制和后面有一篇中的copy机制思路非常类似。</p>
<p>8、feats-lvt2k-2sent-ptr模型[4]</p>
<img src="/2016/05/07/自动文摘（七）/Attention.png" width="600" height="800">
<p>数据集中的原文一般都会很长，原文中的关键词和关键句子对于形成摘要都很重要，这个模型使用两个双向RNN来捕捉这两个层次的重要性，一个是word-level，一个是sentence-level，并且该模型在两个层次上都使用attention，权重如下：</p>
<img src="/2016/05/07/自动文摘（七）/formula1.png" width="300" height="300">
<p>9、COPYNET[8]</p>
<img src="/2016/05/18/自动文摘（十三）/fig2.png" width="600" height="600">
<p>encoder采用了一个双向RNN模型，输出一个隐藏层表示的矩阵M作为decoder的输入。decoder部分与传统的Seq2Seq不同之处在于以下三部分：</p>
<ul>
<li><b>预测</b>：在生成词时存在两种模式，一种是生成模式，一种是拷贝模式，生成模型是一个结合两种模式的概率模型。</li>
<li><b>状态更新</b>：用t-1时刻的预测出的词来更新t时刻的状态，COPYNET不仅仅词向量，而且使用M矩阵中特定位置的hidden state。</li>
<li><b>读取M</b>：COPYNET也会选择性地读取M矩阵，来获取混合了内容和位置的信息。</li>
</ul>
<p>这个模型与第7个模型思想非常的类似，因为很好地处理了OOV的问题，所以结果都非常好。</p>
<p>10、MRT+NHG[7]</p>
<p>这个模型的特别之处在于用了Minimum Risk Training训练数据，而不是传统的MLE（最大似然估计），将评价指标包含在优化目标内，更加直接地对评价指标做优化，得到了不错的结果。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>评价指标是否科学可行对于一个研究领域的研究水平有着直接的影响，目前在文本摘要任务中最常用的评价方法是ROUGE(Recall-Oriented Understudy for Gisting Evaluation)。ROUGE受到了机器翻译自动评价方法BLEU的启发，不同之处在于，采用召回率来作为指标。基本思想是将模型生成的摘要与参考摘要的n元组贡献统计量作为评判依据。</p>
<p>在英文数据集DUC-2004上进行评测，结果如下：</p>
<img src="/2016/06/25/教机器学习摘要/fig1.png" width="500" height="500">
<p>在中文数据集LCSTS上进行评测，结果如下：</p>
<img src="/2016/06/25/教机器学习摘要/fig2.png" width="500" height="500">
<p>不管是中文数据集还是英文数据集上，最好的结果都是来自于模型10[7],并且该模型只是采用最普通的seq2seq+attention模型，都没有用到效果更好的copy机制或者pointer机制。</p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>自动文摘是我关注的第一个nlp领域，早期了很多相关的paper，从方方面面都有所了解，也有一些比较浅薄的想法，现在总结一下。</p>
<p>1、为什么MRT那篇文章的结果会比其他各种各样的模型都要好呢？因为他直接将ROUGE指标包含在了待优化的目标中，而不是与其他模型一样，采用传统的MLE来做，传统的目标评价的是你的生成质量如何，但与我们最终评价的指标ROUGE并无直接关系。所以说，换了一种优化目标，直接定位于评价指标上做优化，效果一定会很好。这点不仅仅在自动文摘中出现过，我记得在bot相关的paper中还有机器阅读理解相关的paper中都有出现，只是具体的评价指标不同而已。这一点很有启发性，如果在文章[7]中采用copy机制来解决OOV问题，会不会有更加惊人的效果呢？我们拭目以待。</p>
<p>2、OOV(out of vocabulary)的问题。因为文本摘要说到底，都是一个语言生成的问题，只要是涉及到生成的问题，必然会遇到OOV问题，因为不可能将所有词都放到词表中来计算概率，可行的方法是用选择topn个高频词来组成词表。文章[4]和[8]都采用了相似的思路，从input中拷贝原文到output中，而不仅仅是生成，这里需要设置一个gate来决定这个词是copy来还是generate出来。显然，增加了copy机制的模型会在很大程度上解决了OOV的问题，就会显著地提升评价结果。这种思路不仅仅在文摘问题上适用，在一切生成问题上都适用，比如bot。</p>
<p>3、关于评价指标的问题。一个评价指标是否科学直接影响了这个领域的发展水平，人工评价我们就不提了，只说自动评价。ROUGE指标在2003年就被Lin提出了[9]，13年过去了，仍然没有一个更加合适的评价体系来代替它。ROUGE评价太过死板，只能评价出output和target之间的一些表面信息，并不涉及到语义层面上的东西，是否可以提出一种更加高层次的评价体系，从语义这个层面来评价摘要的效果。其实技术上问题不大，因为计算两个文本序列之间的相似度有无数种解决方案，有监督、无监督、半监督等等等等。很期待有一种新的体系来评价摘要效果，相信新的评价体系一定会推动自动文摘领域的发展。</p>
<p>4、关于数据集的问题。LCSTS数据集的构建给中文文本摘要的研究奠定了基础，将会很大程度地推动自动文摘在中文领域的发展。现在的互联网最不缺少的就是数据，大量的非结构化数据。但如何构建一个高质量的语料是一个难题，如何尽量避免用过多的人工手段来保证质量，如何用自动的方法来提升语料的质量都是难题。所以，如果能够提出一种全新的思路来构建自动文摘语料的话，将会非常有意义。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1512.01712" target="_blank" rel="external">Generating News Headlines with Recurrent Neural Networks</a></p>
<p>[2] <a href="http://cn.arxiv.org/pdf/1509.00685.pdf" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a></p>
<p>[3] <a href="http://harvardnlp.github.io/papers/naacl16_summary.pdf" target="_blank" rel="external">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</a></p>
<p>[4] <a href="http://cn.arxiv.org/pdf/1602.06023" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></p>
<p>[5] <a href="http://cn.arxiv.org/pdf/1604.00125v1.pdf" target="_blank" rel="external">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></p>
<p>[6] <a href="http://cn.arxiv.org/pdf/1506.05865" target="_blank" rel="external">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<p>[7] <a href="http://cn.arxiv.org/pdf/1604.01904.pdf" target="_blank" rel="external">Neural Headline Generation with Minimum Risk Training</a></p>
<p>[8] <a href="http://cn.arxiv.org/pdf/1603.06393v2.pdf" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning Training</a></p>
<p>[9] <a href="http://research.microsoft.com/en-us/people/cyl/naacl2003.pdf" target="_blank" rel="external">Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</a></p>
<p>[10] <a href="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<p>知乎专栏<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">paperweekly</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-23T18:19:43.000Z"><a href="/2016/06/23/如果我也做一家公司/">2016-06-23</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/23/如果我也做一家公司/">如果我也做一家公司</a></h1>
  

    </header>
    <div class="entry">
      
        <img src="/2016/06/23/如果我也做一家公司/1.jpg" width="800" height="800">
<p>内心一直有一种去创业的情怀，一直想追逐一个属于自己的事业，想开一家很棒的公司。如果我也做一家公司的话，可能是下面的样子：</p>
<p>1、人数一定不会很多，少而精，因为我有密集恐惧症，受不了很多人在我面前。其实，人少并不代表战斗力差，很多东西都不需要自己公司去做，很多业务都可以外包给质量更高的专业公司来做，我们只需要做好最核心的东西。人少有太多的好处，交流和沟通的效率会非常高，不会被每天大量的、无聊的、低效率的会议占据了时间。</p>
<p>2、技术一定要新，且不断地尝试新的东西，不断地推出好玩的产品，这个需要我们具备非常好地学习能力和动手能力。可以跟上最新的研究成果，比如从最新的paper中汲取新方法，可以用最新、最牛的技术高效地解决难题。不管是硕士还是博士地学习，我觉得最重要的都不是知识上的积累，而是思考问题和学习解决问题的能力地锻炼，知识上的积累只是一个附带的成果，最直接、最重要的成果应该是你通过学习具备了解决各种各样未知问题的能力。</p>
<p>3、不是利益驱动，而是快乐或者别的与利益无关的东西在驱动。我们做什么事情采用什么样的方法，从根本上决定于你的目标是什么，你要达到一个什么样的程度。你是为了赚钱，还是为了满足自己的一些情怀，还是为了得到一些肯定和声望？不同的目标或者说不同的驱动模式，会直接影响你的心态以及做事情的方式。可能国内的环境容易让大家背离初心，变得更加实用，更加地浮躁，更加地急躁，更加地利益化，衡量一切成功或失败的唯一标准变成了是否赚钱，而不是说你做出了什么有意义或者伟大的事情。最终，企业将只是一个为了养家糊口的企业了，那么和你找一份工作有什么本质上的区别呢？如果只是为了养家糊口。</p>
<p>4、我关注nlp，也看好nlp在将来的应用会比现在更加地智能，可以更好地服务于大众。因为自然语言是与每一个人类息息相关的东西，也是人类在表达情感以及各种各样情绪的最根本的东西，人类每天都需要阅读、写作文字，大量的与文字相关的东西有朝一日都可以用nlp技术来辅助、帮忙甚至替代。这一点都不危言耸听。</p>
<p>5、工作时间一定是弹性的，因为人少所以容易管理，其实也不需要管理，基本上处于一种高度自觉的、无组织的状态，但这并不意味着偷懒或者低效。因为人少，既然能一起做这样的事情，一定具有相似的三观和基本条件，所以不需要什么强制性的制度来约束大家。最自由的状态反而是最高效的状态，企业嘛，效率是一切行动的基础和生命力，很多大厂都模仿美国的公司搞一些软性的公司待遇，比如吃不完的零食、带宠物、弹性工作等等都是为了让大家更高效地工作，然后在不知不觉中更加充分地剥削其剩余价值。而且我不反对远程办公，相反我很喜欢远程办公，一切以高效地做好事情为最终目标。</p>
<p>说了这么多，我想可能最合适的企业模式是当下也很流行的SaaS+2B，类似的企业有很多也有很多种，比如国内的face++做人脸识别，比如国外的Maluuba做bot和机器阅读。尤其是bot的兴起，大量的企业开始做bot API。这种企业有一个特点，就是赚其他需要该技术的企业的钱，也就是2B(To Business)，提供服务的形式一般都是给API接口，都是将后台的算法封装好，通过web协议供企业用户来调用，按次收费。</p>
<p>这种类型的企业需要有非常有竞争力的技术和持续学习的能力，不需要前期的资本涌入来烧钱，靠自身就可以实现盈利，企业模式的框架比较简单清晰，核心的东西在于算法。整个模式正是非常适合我上述想法的，那么说这么多，我自己现在具备什么以及还差什么？</p>
<p>1、实现一个SaaS+2B门户网站以及算法接口问题不大，php、python开发一个网站非常容易，实现具体的算法也实践了很多，封装成接口也没有问题。毕竟之前开发的rsarxiv网站、微信公众号、ios app都用到了这些技术，有了一定的积累。（rsarxiv是一个arxiv paper的推荐系统）</p>
<p>2、跟上最新的paper，从中汲取新方法和新思路。这一点正在做，每天都在做，每天的PaperWeekly就是在做这样的事情，在不断地积累，在不断学习，同时也在用torch实现着，前几天刚刚开源了一个完整的seq2seq+attention的代码。当然，这一块是核心的东西，因为算法是你的生命力，是你立足于市场的最根本的东西，所以还需要更深地积累，毕竟厚积薄发嘛。</p>
<p>3、团队。我理想中的团队不要很多的人，因为我习惯了一个人，也不喜欢和志不同道不合的人说太多的话。所以，找团队不能急，就像你找女朋友一样，都是缘分，缘分到了自然会有合适的人来加入，不可勉强，也不可速求。希望慢慢地可以遇到三观相符的有缘人。</p>
<p>4、一些必备的能力还有所欠缺。比如推销自己的能力，这一点非常重要，也是我一直忽视的能力。如何在现有的条件下最大地将自己推销出去是一件很难的事情。在网上看过一个帖子，说印度人在美国的公司比中国人更容易得到升迁的机会，一个很重要的原因在于他们更善于推销自己，而且美国就是一个重视这种事情的国家。这一点，我非常佩服自己的导师，他对待任何事情都非常认真，比如昨天准备一个评奖的ppt，愣是几天几夜不休息，一直在认认真真地准备着，这一点非常打动我，让我感到肃然起敬。也让我深深地从他身上学到了一点，怎样在一个20分钟的ppt里尽可能多地推销自己，为自己争取到更多的机会。再比如，推销产品的能力，也就是销售，换句话说就是怎么去忽悠企业来用你的服务，这个能力需要好好地锻炼，也许我的爱人可以帮助我。</p>
<p>创业对于我来说一直都是一个情怀，也是一个梦想。有的梦想在不正确时间将很那被实现，但我会选择将其珍藏起来，有朝一日来努力实现它。今年27岁，这个梦从初中看盗版的比尔盖茨传开始就播下了种子，我想现在该拿出来，开始好好琢磨琢磨了。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-20T03:56:21.000Z"><a href="/2016/06/19/My-first-open-source-code-in-deep-learning/">2016-06-19</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/19/My-first-open-source-code-in-deep-learning/">My first open source code in deep learning</a></h1>
  

    </header>
    <div class="entry">
      
        <p>前面的博客中有提到过要开源最近写的code，<a href="https://github.com/rsarxiv/seq2seq-attention" target="_blank" rel="external">seq2seq-attention</a>，今天正式开源了，欢迎各路大神来fork和star。这是我从5月中旬开始决定用torch框架来写deep learning code以来写的第一个完整的program，在写的过程中走过不少弯路，尤其是在选择demo进行学习的过程中，被HarvardNLP组的<a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">seq2seq-attn</a>难以阅读的代码搞得非常崩溃，差一点就放弃了Torch。后来从<a href="https://github.com/oxford-cs-ml-2015/practical6" target="_blank" rel="external">oxford的课程代码</a>和<a href="https://github.com/karpathy/char-rnn" target="_blank" rel="external">char-rnn</a>中慢慢找到了Torch写code的感觉。本文简单介绍下踩过的坑和一些感受。</p>
<p>1、最开始想要用的框架是theano，因为之前一直用python写代码，而且theano的开源demo很多，并且有二次封装的框架keras，非常简单易用。但后来在写的时候总是报一些难以定位的错误，就放弃了。接下来，摆在我面前的剩下了两个选择，一个是tensorflow，一个是torch。这里为什么没有其他的上层框架呢？（比如keras等二次框架）因为，我觉得灵活性是我对框架的第一要求，大家都知道框架封装越友好，越高级，灵活性就会越差，在实现自己的model时大多数会捉襟见肘，难以胜任；但如果你选择了一个非框架的方案，也就是说用c/c++来写，虽然运行效率很高，但实现效率就会太低了，时间成本是最贵的成本，绝对不要浪费时间在一些很成熟的东西上面。</p>
<p>接着说，二选一怎么做选择。tensorflow其实是刚刚发布不久的，而且是python系的，但太新了，存在很多不稳定的地方或者不成熟的地方，而torch已经在facebook、deepmind等公司被使用了很久，尤其是facebook肩负起了维护和开发torch的重任，迷信了权威，选择了更加成熟的torch，尽管用torch先得过lua这关，我也认了。后来，在微博上每天都出现大量tensorflow的分享，有那么一丝动摇，但最终仍然坚持了torch。这个过程和我做PaperWeekly都给我带来了一样的感受和力量，那就是<code>坚持就会让你变得不一样</code>，我特别信这句话。</p>
<p>2、torch做数据预处理是一个坑，一个很大的坑，用python几行搞定的事情，在torch这里需要大量的代码，十分恶心。所以，就换了一种思路，用python做预处理，将处理好的结果扔到hdf5中，用torch读入hdf5中的数据。其实这里，用hdf5也行，用普通的文本文件也行，毕竟只是一个交换数据的媒介，但hdf5更加高效一些。</p>
<p>3、demo的问题，忘了是哪个大神说过torch的缺点，之一就是找不到一个特别规范的demo来引导新手，于是我在google上搜到了所有与之相关的torch代码来看，从最简单的开始，后来发现oxford的课程代码和char-rnn很好懂，就靠这个入了门。其实这两个代码做的是同一件事情，都是char-level的语言模型，只不过oxford更加简单，功能没有那么全面，而char-rnn是github上一个知名的code，与karpathy博士的blog配合一起十分有名，殊不知他的很多code都是从oxford的课程中来的，但好像oxford有一些代码是从一个叫<a href="https://github.com/wojciechz/learning_to_execute" target="_blank" rel="external">learning_to_execute</a>的code中来的，大家纷纷继承了优秀的功能代码，在此基础上写自己model这一部分。因为lua是函数式编程语言，函数是第一公民，各种各样的闭包令人难以阅读，这一点让学demo和继承demo的代码变得困难了。</p>
<p>4、nngraph，代码中所有的model都是用nngraph来写的，刚刚开始用nngraph的时候，总是会出现莫名其妙的错误，告诉你定义的节点并没有被使用。后来慢慢地找到了感觉，这个过程其实就是model的forward过程，特别需要注意的是每个节点的定义是什么，自己一定要标识好，在train的过程中调用时，输入一定要给对，顺序到一定不能错。但如果你踩过了那个坑之后，就会发现用torch写deep learning的model就会很简单了，把整个forward过程梳理清楚，用nngraph写出来，基本上具有你自己特色的model就ok了。</p>
<p>5、forward、backward函数，这两个函数构成了torch框架的整个计算过程，训练无非就是正向预测、反向梯度传播更新参数，循环往复罢了。使用这两个函数可以自动完成计算，非常地方便。在train的过程中，就是不断地将你的data丢入到model中forward然后backward，直到获得满意的误差。</p>
<p>6、GPU的使用，这一点也是非常地简单和方便，只需要把整个过程中的torch变量变为cutorch或者cltorch即可。</p>
<p>7、不建议开始就用高级的库，比如rnn或者dp，而是从最基本的rnn、lstm、gru这些开始写起，对概念、对整个计算流程都会有更深入地理解，同样是解决了问题，从更低层次的东西做起会了解到更多的细节，在灵活性上也有更好的优势。</p>
<p>这次开源的代码是端到端+注意力模型，用了当下流行的bot来做demo，包括数据预处理，训练，测试，部署到本地服务器等一些功能。欢迎大家多fork，一起改进它，也是一个促进我进步的机会。如果您想交流，可以make一个issue或者直接发邮件给我，mcgrady150318@gmail.com。</p>
<p><b>推荐</b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">
<p>知乎专栏也是<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">PaperWeekly</a>，欢迎大家关注。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-18T17:20:27.000Z"><a href="/2016/06/18/教机器学习阅读/">2016-06-18</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/18/教机器学习阅读/">教机器学习阅读</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>机器学会阅读将是人工智能在处理和理解人类语言进程中一个里程碑式的事件，是一个真正AI必须达到的标准。最近一家叫做<a href="http://www.maluuba.com/" target="_blank" rel="external">Maluuba</a>的科技公司，号称开发了目前最领先的机器阅读理解系统EpiReader[10]，成为了业界的领跑者，也被媒体盛赞。本文是一篇机器阅读理解的综述文章，系统地总结和对比一下最近阅读过的相关paper。</p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>首先定义下机器阅读理解问题的数学模型。</p>
<p>问题可以表述为一个三元组(d,q,a)，这里d是指原文document，q是指问题query或者question（不同的paper可能称呼不同，但指的是同一回事），a是answer，即问题的答案。这个答案是来自一个固定大小的词汇表A中的一个词。我们要解决的问题就变成了：给定一个document-query对(d,q)，从A中找到最合适的答案a。</p>
<p>经常听到这么一句话，没有分类解决不了的问题。虽然有一点夸张，但这个问题是一个典型的多分类问题，预测候选列表中每个word或者entity的条件概率，最大的那个就是正确答案。其实很多问题也都是这样，尤其是在生成问题上，给定context，来predict每一个word的概率。</p>
<p>这里不同的paper在词汇表A中有些不同，有的paper将A定义为document和query中的所有词，而有的将A定义为所有的entity，而有的将会附带一个包括正确答案在内的10个候选答案，并且每个答案的词性都一致。</p>
<h1 id="语料"><a href="#语料" class="headerlink" title="语料"></a>语料</h1><p>语料对于NLP的研究有着十分重要的基础作用，尤其是大规模的语料为研究相关任务带来了革命性的变化。前些年的语料都非常小，比如MCTest。从2015年开始，出现了两大主流的大型数据集。</p>
<p>1、CNN/Daily Mail[9]</p>
<p>数据集构建基本的思路是受启发于自动文摘任务，从两个大型的新闻网站CNN和Daily Mail中获取数据源，用abstractive的方法生成每篇新闻的summary，用新闻原文作为document，将summary中去掉一个entity作为query，被去掉的entity作为answer，从而得到阅读理解的数据三元组(document,query,answer)。这里存在一个问题，就是有的query并不需要联系到document，通过query中的上下文就可以predict出answer是什么，也就失去了阅读理解的意义。因此，本文提出了用一些标识替换entity和重新排列的方法将数据打乱，防止上面现象的出现。处理之后的效果见下图：</p>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig1.png" width="600" height="600">
<p>2、Children’s Book Test(CBT)[3]</p>
<p>CBT的数据均来自Project Gutenberg，使用了其中的与孩子们相关的故事，这是为了保证故事叙述结构的清晰，从而使得上下文的作用更加突出。每篇文章只选用21句话，前20句作为document，将第21句中去掉一个词之后作为query，被去掉的词作为answer，并且给定10个候选答案，每个候选答案是从原文中随机选取的，并且这10个答案的词性是相同的，要是名词都是名词，要是命名实体都是实体，要是动词都是动词。例子看下图：</p>
<img src="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/fig1.png" width="600" height="600">
<p>左图为电子书的原文，右图为构建成数据集之后的几个元素，document、query、answer和candidate。数据集根据词性一共分为四类，第一类是Named Entity，第二类是Nouns，第三类是Verbs，第四类是Preposition。其实阅读理解问题的难度在于前两种词性，后面的两种用语言模型通过query的上下文就可以预测出来，不需要借助于document的信息。这个数据集并没有像CNN那样做替换和重排的处理，反而是鼓励大家用更少的信息来做阅读理解。</p>
<p>说完最流行的两个数据集，接下来介绍一下昨天刚刚在arxiv上submit的一个数据集。</p>
<p>3、Stanford Question Answering Dataset(SQuAD)[1]</p>
<p>该数据集的构建分为三个步骤：</p>
<ul>
<li><p>在Wikipedia中选取质量排名在10000以内的article，（这里用了 Project Nayuki’s Wikipedia’s internal PageRanks来做rank），从每篇文章中提取出paragraph，经过一系列处理之后得到了23215个paragraph，涉及了很宽泛的话题。</p>
</li>
<li><p>然后雇佣了crowdworkers给每个paragraph提问和回答，而且鼓励workers用自己的话来提问。（这一点和CNN中用abstractive的思路很像，只不过是用了人工来做。）</p>
</li>
<li><p>第三步是让crowdworkers来用原文中的text（word或者是span）来回答这个问题，如果无法用原文回答的话，直接提交问题。</p>
</li>
</ul>
<p>这个数据集的答案类型非常丰富，看下表：</p>
<img src="/2016/06/18/教机器学习阅读/fig1.png" width="400" height="400">
<p>4、LAnguage Modeling Broadened to Account for Discourse Aspects(LAMBADA)[11]</p>
<p>该数据集的paper于2016年6月20日于arxiv上，特补充在此。</p>
<p>这个数据集的数据源来自Book Corpus，一共包括10022个passage，平均每个passage包括4.6句话+1句target，每个passage大约75个单词。在选择数据的过程用了很多的人力来做，按照以下的过程：</p>
<ul>
<li><p>第一个人阅读全文之后来猜target word，如果猜对了。</p>
</li>
<li><p>第二个人继续阅读全文来猜target word，如果猜对了。</p>
</li>
<li><p>更多人不让阅读全文，只能读target sentence来猜target word，直到猜对或者达到猜的人数上限，比如说10.如果没有人猜的对，就将该数据归入LAMBADA中。</p>
</li>
</ul>
<p>这种方法很费时费力，但从质量上得到了保证，所获得的数据集都可以保证通过阅读全文之后一定会得到正确结果，不阅读全文一定找不到结果，避免了语言模型通过分析target sentence直接生成target word这种情况，给研究者提供了一个质量更高的数据集。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>本文所说的模型是指neural模型，人工features的模型就不介绍了。</p>
<p>1、Deep LSTM Reader / Attentive Reader / Impatient Reader[9]</p>
<p>这个模型是配套CNN/Daily Mail数据集的模型，只是作为后面研究的baseline，所以效果不会太好。</p>
<ul>
<li>Deep LSTM Reader</li>
</ul>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig2.png" width="400" height="400">
<p>看上图，其实非常简单，就是用一个两层LSTM来encode query|||document或者document|||query，然后用得到的表示做分类。</p>
<ul>
<li>Attentive Reader</li>
</ul>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig3.png" width="400" height="400">
<p>这个模型将document和query分开表示，其中query部分就是用了一个双向LSTM来encode，然后将两个方向上的last hidden state拼接作为query的表示，document这部分也是用一个双向的LSTM来encode，每个token的表示是用两个方向上的hidden state拼接而成，document的表示则是用document中所有token的加权平均来表示，这里的权重就是attention，权重越大表示回答query时对应的token的越重要。然后用document和query的表示做分类。</p>
<ul>
<li>Impatient Reader</li>
</ul>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig4.png" width="400" height="400">
<p>这个模型在Attentive Reader模型的基础上更细了一步，即每个query token都与document tokens有关联，而不是像之前的模型将整个query考虑为整体。感觉这个过程就好像是你读query中的每个token都需要找到document中对应相关的token。这个模型更加复杂一些，但效果不见得不好，从我们做阅读理解的实际体验来说，你不可能读问题中的每一个词之后，就去读一遍原文，这样效率太低了，而且原文很长的话，记忆的效果就不会很好了。</p>
<p>2、Attention Sum Reader[6]</p>
<img src="/2016/06/14/Text-Understanding-with-the-Attention-Sum-Reader-Network-PaperWeekly/fig1.png" width="600" height="600">
<p><b>step 1</b> 通过一层Embedding层将document和query中的word分别映射成向量。</p>
<p><b>step 2</b> 用一个单层双向GRU来encode document，得到context representation，每个time step的拼接来表示该词。</p>
<p><b>step 3</b> 用一个单层双向GRU来encode query，用两个方向的last state拼接来表示query。</p>
<p><b>step 4</b> 每个word vector与query vector作点积后归一化的结果作为attention weights，就query与document中的每个词之前的相关性度量。</p>
<p><b>step 5</b> 最后做一次相同词概率的合并，得到每个词的概率，最大概率的那个词即为answer。</p>
<p>3、Memory Networks[3][5]</p>
<img src="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/fig2.png" width="600" height="600">
<p>模型将document中的每一个word保存为一个memory m(i)，每个memory本质上就是一个向量，这一点与embedding是一回事，只是换了一个名词。另外每个word还与一个输出向量c(i)相关联。可以理解为每个word表示为两组不同的embedding A和C。同样的道理，query中的每个单词可以用一个向量来表示，即对应着另一个embedding B。</p>
<p>在Input memory表示层，用query向量与document中每个单词的m(i)作内积，再用softmax归一化得到一组权重，这组权重就是attention，即query与document中每个word的相关度，与昨天的AS Reader模型有些类似。</p>
<p>接下来，将权重与document中的另一组embedding c(i)作加权平均得到Output memory的表示。</p>
<p>最后，利用query的表示和output memory的表示去预测answer。</p>
<p>然后，介绍下右图的多层模型。根据单层模型的结构，非常容易构造出多层模型。每一层的query表示等于上一层query表示与上一层输出memory表示的和。每一层中的A和C embedding有两种模式，第一种是邻接，即A(k+1) = C(k)，依次递推；第二种是类似于RNN中共享权重的模式，A(1) = A(2) = … = A(k),C(1) = C(2) = … = C(k)。其他的过程均和单层模型无异。</p>
<p>本文模型的特点是易于构造更多层的模型，从而取得更好的效果。后面Gate Attention Reader模型正式借助了这个思想。</p>
<p>4、Dynamic Entity Representation[7]</p>
<img src="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/fig1.png" width="400" height="400">
<p>计算出entity的动态表示之后，通过attention mechanism计算得到query与每个entity之间的权重，然后计算每个entity在document和query条件下的概率，找到最终的answer。</p>
<p>query向量的计算与动态entity计算过程类似，这里需要填空的地方记作placeholder，也是包括四个部分，其中两个是表示placeholder上下文的last hidden state，另外两个是表示placeholder的hidden state。</p>
<p>模型的整个计算过程就是这样。如果遇到一个entity在document中出现多次的情况，该entity就会会有不同的表示，本文采用CNN中常用的max-pooling从各个表示中的每个维度获取最大的那一个组成该entity最终的表示，这个表示包括了该entity在document中各种context下的信息，具有最全面的信息，即原文中所说的accumulate information。如下图：</p>
<img src="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/fig2.png" width="400" height="400">
<p>5、Gate Attention Reader[8]</p>
<img src="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig1.png" width="600" height="600">
<p><b>step 1</b> document和query通过一个Lookup层，使得每个词都表示成一个低维向量。</p>
<p><b>step 2</b> 将document中的词向量通过一个双向GRU，将两个方向的state做拼接获得该词的新表示。同时也将query通过一个双向GRU，用两个方向上的last hidden state作为query的表示。</p>
<p><b>step 3</b> 将document中每个词的新表示与query的新表示逐元素相乘得到下一个GRU层的输入。</p>
<p><b>step 4</b> 重复step 2和3，直到通过设定的K层，在第K层时，document的每个词向量与query向量做内积，得到一个最终的向量。</p>
<p><b>step 5</b> 将该向量输入到softmax层中，做概率归一化。</p>
<p><b>step 6</b> 因为document中有重复出现的词，聚合之后得到最终的分类结果，即确定应该填哪个词。</p>
<p>6、Iterative Alternating Attention[2]</p>
<img src="/2016/06/18/教机器学习阅读/fig2.png" width="600" height="600">
<p><b>step 1</b> 将document和query通过一个Lookup层，使得每个词都表示成一个低维向量。</p>
<p><b>step 2</b> 将document和query中的词向量通过一个双向GRU，将两个方向的state拼接获得该词的新表示。</p>
<p><b>step 3</b> 这一步文章中称为Iterative Alternating Attention，1）根据前一个inference状态s(t-1)来计算query的attention，得到query glimpse q(t)，对应图中的1;2)根据前一个状态s(t-1)和当前的query glimpse q(t)来计算document的attention，得到document glimpse d(t)，对应图中的2；3)用GRU来将前一个状态s(t-1)和当前的query glimpse q(t)和当前的document glimpse d(t)做处理得到当前的状态s(t)。</p>
<p><b>step 4</b> 重复step 3 直到t达到给定的T为止。</p>
<p><b>step 5</b> 用最后得到的每个词向量进行归一化，并且聚合相同的词概率，得到预测结果。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>所有模型都是在两大主流数据集上进行对比[2][8]，结果如下：</p>
<img src="/2016/06/18/教机器学习阅读/fig3.png" width="600" height="600">
<p>Maluuba公司的模型在CBT数据集上是最好的，在CNN/Daily Mail数据集上并没有最测试，而Gate Attention Reader占据了CNN数据集上的头把交椅。</p>
<p>model ensemble可以将single model的效果提升很多，是一种非常有效的技术。从第一个模型Attentive Reader到最后一个模型Iterative Alternating Attention时间跨度大概是半年左右的时间，阅读理解的正确率提升了近20个百分点。</p>
<p>CBT数据集上包含了人工测试的结果，最高的准确率为81.6%，而目前计算机可以达到的最高正确率是72%，离人类仍有不小的差距，需要更多更牛的model涌现出来。</p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>读了一周的Machine Reading Comprehension paper，有几点思考：</p>
<p>1、大规模语料的构建是nlp领域研究进步的重要基础和保证，深度学习模型尤其是端到端+注意力模型利用大规模的语料进行训练和学习，极大地提升了计算机阅读理解的效果。而且每出现一个新的数据集，都会弥补之前数据集存在的问题，对模型提出了更高的要求，从而提高了该领域的研究水平。同时，也给很多需要毕业的童鞋提供了一个新的刷榜平台。</p>
<p>2、模型的提出应该更多地联系人类是如何解决这个问题的，比如attention、copy mechanism等等优秀的模型。attention借助了人类观察一个事物的时候，往往第一眼会先注意那些重要的部分，而不是全部这个行为方式。copy mechanism而是受启发于当人类不了解一个事物或者该事物只是一个named entity，而又需要表达它的时候，需要“死记硬背”，需要从context中copy过来。模型Dynamic Entity Representation用一种变化的眼光和态度来审视每一个entity，不同的context会给同样的entity带来不同的意义，因此用一种动态的表示方法来捕捉原文中entity最准确的意思，才能更好地理解原文，找出正确答案。实际体会中，我们做阅读理解的时候，最简单的方法是从问题中找到关键词，然后从原文中找到同样的词所在的句子，然后仔细理解这个句子最终得到答案，这种难度的阅读理解可能是四、六级的水平，再往高一个level的题目，就需要你联系上下文，联系关键词相关联的词或者句子来理解原文，而不是简单地只找到一个句子就可以答对题目。</p>
<p>3、[4]和[6]中用简单的模型反而会得到更好的结果，[9]中用了非常复杂的注意力机制反而并没有太好的结果。[2][8]都证明了用多层结构比单层结构更优秀。那么，是不是多层简单的模型会有更好的效果？这是一个需要动手实践来研究的问题。</p>
<p>4、当前的这些模型都是纯粹的data-driven，并没有考虑人工features进来。我一直坚信，如果做一个准确率非常高的系统的话，neural models+features是必须的，针对具体的问题，做具体的分析；但是如果是对于学术界研究model的话，提出更牛的neural models比纯粹的刷榜更有意义。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1606.05250v1" target="_blank" rel="external">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></p>
<p>[2] <a href="http://arxiv.org/pdf/1606.02245v3.pdf" target="_blank" rel="external">Iterative Alternating Neural Attention for Machine Reading</a></p>
<p>[3] <a href="https://arxiv.org/pdf/1511.02301.pdf" target="_blank" rel="external">THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS</a></p>
<p>[4] <a href="http://arxiv.org/pdf/1606.02858v1.pdf" target="_blank" rel="external">A Thorough Examination of the CNN Daily Mail Reading Comprehension Task</a></p>
<p>[5] <a href="http://arxiv.org/pdf/1503.08895v5.pdf" target="_blank" rel="external">End-To-End Memory Networks</a></p>
<p>[6] <a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="external">Text Understanding with the Attention Sum Reader Network</a></p>
<p>[7] <a href="http://www.cl.ecei.tohoku.ac.jp/publications/2016/kobayashi-dynamic-entity-naacl2016.pdf" target="_blank" rel="external">Dynamic Entity Representation with Max-pooling Improves Machine Reading</a></p>
<p>[8] <a href="http://cn.arxiv.org/pdf/1606.01549v1" target="_blank" rel="external">Gated-Attention Readers for Text Comprehension</a></p>
<p>[9] <a href="http://rsarxiv.github.io/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">Teaching Machines to Read and Comprehend</a></p>
<p>[10] <a href="http://cn.arxiv.org/pdf/1606.02270.pdf" target="_blank" rel="external">Natural Language Comprehension with the EpiReader</a></p>
<p>[11] <a href="http://cn.arxiv.org/pdf/1606.06031" target="_blank" rel="external">The LAMBADA dataset:Word prediction requiring a broad discourse context</a></p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-17T18:13:56.000Z"><a href="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/">2016-06-17</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/">Dynamic Entity Representation with Max-pooling Improves Machine Reading #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文是机器阅读理解系列的第六篇文章，paper的题目是<a href="http://www.cl.ecei.tohoku.ac.jp/publications/2016/kobayashi-dynamic-entity-naacl2016.pdf" target="_blank" rel="external">Dynamic Entity Representation with Max-pooling Improves Machine Reading</a>，作者是来自日本东北大学的老师<a href="http://www.sc.isc.tohoku.ac.jp/~koba/" target="_blank" rel="external">Sosuke Kobayashi</a>，文章发表在<a href="http://naacl.org/naacl-hlt-2016/" target="_blank" rel="external">NAACL HLT 2016</a>。本文的代码开源在<a href="https://github.com/soskek/der-network" target="_blank" rel="external">Github</a>。</p>
<p>本文模型之前的模型都是用一个静态的向量来表示一个entity，与上下文没有关系。而本文最大的贡献在于提出了一种动态表示entity（Dynamic Entity Representation）的模型，根据不同的上下文对同样的entity有不同的表示。</p>
<p>模型还是采用双向LSTM来构建，这时动态entity表示由四部分构成，包括两个方向上的hidden state，以及表示该entity所在句子的last hidden state，也就是该entity所在的上下文表示。如下图所示：</p>
<img src="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/fig1.png" width="400" height="400">
<p>计算出entity的动态表示之后，通过attention mechanism计算得到query与每个entity之间的权重，然后计算每个entity在document和query条件下的概率，找到最终的answer。</p>
<p>query向量的计算与动态entity计算过程类似，这里需要填空的地方记作placeholder，也是包括四个部分，其中两个是表示placeholder上下文的last hidden state，另外两个是表示placeholder的hidden state。</p>
<p>模型的整个计算过程就是这样。如果遇到一个entity在document中出现多次的情况，该entity就会会有不同的表示，本文采用CNN中常用的max-pooling从各个表示中的每个维度获取最大的那一个组成该entity最终的表示，这个表示包括了该entity在document中各种context下的信息，具有最全面的信息，即原文中所说的accumulate information。如下图：</p>
<img src="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/fig2.png" width="400" height="400">
<p>本文的实验在CNN数据上对模型进行了对比，效果比之前的Attentive Reader好很多，验证了本文的有效性。（当然结果没法和GA Reader比）</p>
<p>最后，作者给出了一个example，来说明用max-pooling的作用，见下图：</p>
<img src="/2016/06/17/Dynamic-Entity-Representation-with-Max-pooling-Improves-Machine-Reading-PaperWeekly/fig3.png" width="400" height="400">
<p>由于用了max-pooling模型比起不用它的话，可以关注到第二句和第三句话，因为本文模型可以捕捉到entity0（Downey）和entity2（Iron Man）是关联的（Robert Downey Jr.是Iron Man的扮演者），然后就会注意到entity2出现过的几个句子，而不仅仅是query中entity0出现过的几个句子，这一点帮助了模型找到了最终的正确答案entity26（在第二句中）。</p>
<p>本文模型的一个好玩之处在于用了一种变化的眼光和态度来审视每一个entity，不同的context会给同样的entity带来不同的意义，因此用一种动态的表示方法来捕捉原文中entity最准确的意思，才能更好地理解原文，找出正确答案。实际体会中，我们做阅读理解的时候，最简单的方法是从问题中找到关键词，然后从原文中找到同样的词所在的句子，然后仔细理解这个句子最终得到答案，这种难度的阅读理解可能是四、六级的水平，再往高一个level的题目，就需要你联系上下文，联系关键词相关联的词或者句子来理解原文，而不是简单地只找到一个句子就可以答对题目。本文的动态表示正是有意在更加复杂的阅读理解题目上做文章，是一个非常好的探索。</p>
<p>另外，如何衡量阅读理解语料中题目的难度？是否可以按难度分类进行对比测试？如果说现在最好的系统可以做到75%左右的正确率，是否可以给出一些更加有难度的题目来做？比如英语考试中真正的阅读理解或者完形填空。不同的模型具有不同的特点，可以考虑用不同难度的题目来验证模型的适用性。</p>
<p>本文是这个系列文章在本周的最后一篇单文，周末的时间会整理出本周分享的模型的思路、研究动机和实验结果等各个方面来写一篇综述文章，对机器阅读理解这个点进行一个较系统地总结，敬请期待！（后续还会继续关注这个方向，读更多的paper来分享）</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-16T14:38:23.000Z"><a href="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/">2016-06-16</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/">THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文是机器阅读理解系列的第五篇文章，将会分享的题目是<a href="https://arxiv.org/pdf/1511.02301.pdf" target="_blank" rel="external">THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS</a>，作者是来自剑桥大学的博士生<a href="http://www.cl.cam.ac.uk/~fh295/" target="_blank" rel="external">Felix Hill</a>，本文的工作是在Facebook AI Research完成的，文章最早于2016年4月1日submit在arxiv上，后来发表在ICLR 2016会议上。</p>
<p>本文的贡献主要是两点：一是构建了一个新的语料，Children’s Book Test（CBT），丰富了机器阅读理解任务的数据源；二是将Facebook提出的Memory Network框架应用到了机器阅读理解任务中，并取得了不错的结果。</p>
<p>首先，来介绍CBT。CBT的数据均来自<a href="https://www.gutenberg.org/" target="_blank" rel="external">Project Gutenberg</a>，使用了其中的与孩子们相关的故事，这是为了保证故事叙述结构的清晰，从而使得上下文的作用更加突出。每篇文章只选用21句话，前20句作为document，将第21句中去掉一个词之后作为query，被去掉的词作为answer，并且给定10个候选答案，每个候选答案是从原文中随机选取的，并且这10个答案的词性是相同的，要是名词都是名词，要是命名实体都是实体，要是动词都是动词。例子看下图：</p>
<img src="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/fig1.png" width="600" height="600">
<p>左图为电子书的原文，右图为构建成数据集之后的几个元素，document、query、answer和candidate。数据集根据词性一共分为四类，第一类是Named Entity，第二类是Nouns，第三类是Verbs，第四类是Preposition。这里，用LSTM RNNLM从query本身出发就可以非常准确地预测出Verbs和Preposition，不需要借助过多的document context，但是对于前两类却束手无策。因此本文提出了用Memory Network来解决这个问题。</p>
<p>Memory Network是Facebook提出的框架，在nlp的很多任务中都表现出色，比如语言模型。</p>
<img src="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/fig2.png" width="600" height="600">
<p>原文中没有模型结构图，该图来自于文章<a href="http://arxiv.org/pdf/1503.08895v5.pdf" target="_blank" rel="external">End-To-End Memory Networks</a>。左图是一个单层的Memory Network，右图是一个多层的Network。</p>
<p>首先，介绍下单层模型。</p>
<p>模型将document中的每一个word保存为一个memory m(i)，每个memory本质上就是一个向量，这一点与embedding是一回事，只是换了一个名词。另外每个word还与一个输出向量c(i)相关联。可以理解为每个word表示为两组不同的embedding A和C。同样的道理，query中的每个单词可以用一个向量来表示，即对应着另一个embedding B。</p>
<p>在Input memory表示层，用query向量与document中每个单词的m(i)作内积，再用softmax归一化得到一组权重，这组权重就是attention，即query与document中每个word的相关度，与昨天的AS Reader模型有些类似。</p>
<p>接下来，将权重与document中的另一组embedding c(i)作加权平均得到Output memory的表示。</p>
<p>最后，利用query的表示和output memory的表示去预测answer。</p>
<p>然后，介绍下右图的多层模型。根据单层模型的结构，非常容易构造出多层模型。每一层的query表示等于上一层query表示与上一层输出memory表示的和。每一层中的A和C embedding有两种模式，第一种是邻接，即A(k+1) = C(k)，依次递推；第二种是类似于RNN中共享权重的模式，A(1) = A(2) = … = A(k),C(1) = C(2) = … = C(k)。其他的过程均和单层模型无异。</p>
<p>本文模型的特点是易于构造更多层的模型，从而取得更好的效果。我们之前分享过的一篇文章<a href="http://rsarxiv.github.io/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">Gated-Attention Readers for Text Comprehension</a>就是借鉴了其中的思想，从而获得了目前来说最棒的结果。前面的文章给我的启示是，简单的模型反而会得到更好的结果，而本文给我的一个感觉是，如果你用了更多的layer也可能会获得不错的结果。如果你用了很多层非常简单的模型会不会得到更好的结果呢？这是一个需要思考和认真实践的问题。</p>
<p>明天将会继续介绍一篇机器阅读理解的单文，周末将会写一篇类似综述的文章，系统地对比下各种模型和结果。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-15T04:31:36.000Z"><a href="/2016/06/14/Text-Understanding-with-the-Attention-Sum-Reader-Network-PaperWeekly/">2016-06-14</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/14/Text-Understanding-with-the-Attention-Sum-Reader-Network-PaperWeekly/">Text Understanding with the Attention Sum Reader Network #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本文是机器阅读系列的第四篇文章，本文的模型常出现在最新的机器阅读paper中related works部分，也是很多更好的模型的基础模型，所以很有必要来看下这篇paper，看得远往往不是因为长得高，而是因为站得高。本文的题目是<a href="https://arxiv.org/pdf/1603.01547.pdf" target="_blank" rel="external">Text Understanding with the Attention Sum Reader Network</a>，作者是来自IBM Watson的研究员Rudolf Kadlec，paper最早于2016年3月4日submit在arxiv上。</p>
<p>本文的模型被称作Attention Sum Reader，具体见下图：</p>
<img src="/2016/06/14/Text-Understanding-with-the-Attention-Sum-Reader-Network-PaperWeekly/fig1.png" width="600" height="600">
<p><b>step 1</b> 通过一层Embedding层将document和query中的word分别映射成向量。</p>
<p><b>step 2</b> 用一个单层双向GRU来encode document，得到context representation，每个time step的拼接来表示该词。</p>
<p><b>step 3</b> 用一个单层双向GRU来encode query，用两个方向的last state拼接来表示query。</p>
<p><b>step 4</b> 每个word vector与query vector作点积后归一化的结果作为attention weights，就query与document中的每个词之前的相关性度量。</p>
<p><b>step 5</b> 最后做一次相同词概率的合并，得到每个词的概率，最大概率的那个词即为answer。</p>
<p>模型在CNN/Daily Mail和CBT的Nouns、Named Entity数据集上进行了测试，在当时的情况下都取得了领先的结果。并且得到了一些有趣的结论，比如：在CNN/Daily Mail数据集上，随着document的长度增加，测试的准确率会下降，而在CBT数据集上得到了相反的结论。从中可以看得出，两个数据集有着不同的特征，构造方法也不尽相同，因此同一个模型会有着不同的趋势。</p>
<p>本文的模型相比于Attentive Reader和Impatient Reader更加简单，没有那么多繁琐的attention求解过程，只是用了点乘来作为weights，却得到了比Attentive Reader更好的结果，从这里我们看得出，并不是模型越复杂，计算过程越繁琐就效果一定越好，更多的时候可能是简单的东西会有更好的效果。</p>
<p>另外，在这几篇paper中的related works中，都会提到用Memory Networks来解决这个问题。接下来的文章将会分享Memory Networks在机器阅读理解中的应用，大家敬请关注。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-14T15:36:02.000Z"><a href="/2016/06/14/A-Thorough-Examination-of-the-CNN-Daily-Mail-Reading-Comprehension-Task-PaperWeekly/">2016-06-14</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/14/A-Thorough-Examination-of-the-CNN-Daily-Mail-Reading-Comprehension-Task-PaperWeekly/">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇是reading comprehension系列的第三篇，文章于2016年6月9号submit在arxiv上，比之前介绍的<a href="http://rsarxiv.github.io/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">Gated-Attention Readers for Text Comprehension</a>更晚地出现，但尴尬的是本文的模型结果不如GA Reader。6月7号submit的一篇<a href="http://arxiv.org/pdf/1606.02245v3.pdf" target="_blank" rel="external">Iterative Alternating Neural Attention for Machine Reading</a>，用了和GA非常类似的方法，得到了稍微差一点的结果。确实最近在arxiv上常常可以刷出reading comprehension的paper，可以看得出这个领域当前多么地火热。同时火热的还有dialogue generation任务，今天凌晨的wwdc2016大会中，苹果宣布打造更加智能的siri，几大科技巨头纷纷表示要将聊天机器人作为智能的未来，由此可见与其相关的研究将会越来越热。本文的作者是来自斯坦福大学的博士生<a href="http://cs.stanford.edu/people/danqi/" target="_blank" rel="external">Danqi Chen</a>，本科毕业于清华的姚班。</p>
<p>虽然本文并没有比GA模型有更好的效果，但作为了解整个Reading Comprehension研究的发展以及模型的思路还是很有意义的。本文最大的贡献在于提出了一种基于人工特征的分类器模型和一个改进版的端到端模型（这里是基于<a href="http://rsarxiv.github.io/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">Teaching Machines to Read and Comprehend</a>的Attentive Reader模型）。</p>
<p>第一个模型，是典型的人工特征模型，通过提取了八个特征构建特征空间，通过使得正确答案entity比其他entity获得更高的得分来训练得到模型参数。包含的特征有：该entity是否出现在原文中，该entity是否出现在问题中，出现过几次，第一次出现的位置等等八个特征。</p>
<p>第二个模型，基本思路与<a href="http://rsarxiv.github.io/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">Attentive Reader</a>接近。看下图：</p>
<img src="/2016/06/14/A-Thorough-Examination-of-the-CNN-Daily-Mail-Reading-Comprehension-Task-PaperWeekly/fig1.png" width="600" height="600">
<p>这里只介绍不同的地方：</p>
<p>1、在计算query和document的注意力权重时，没有采用非线性的tanh，而是采用了bilinear。</p>
<p>2、得到注意力权重之后，计算context的输出，然后直接用输出进行分类预测，而Attentive Reader是用输出与query又做了一次非线性处理之后才预测的。</p>
<p>3、词汇表中只包括entity，而不是所有的单词。</p>
<p>模型上的改进只有第一点算是吧，后两点只是做了一些简单的优化。</p>
<p>虽然模型简单了，但效果却比Attentive Reader好很多，提升了约5%的效果，我们不管其模型有没有什么亮点，这些简化处理反而得到非常好的效果，这一点很引人深思。</p>
<p>结果这部分，作者分析了八个特征分别对模型结果的影响，其中影响最大的是n-gram match（entity和placeholder是否有相似的上下文），其次是entity出现的频率，具体见下表：</p>
<img src="/2016/06/14/A-Thorough-Examination-of-the-CNN-Daily-Mail-Reading-Comprehension-Task-PaperWeekly/fig2.png" width="400" height="400">
<p>端到端模型比Attentive Reader效果好很多，但和最近的GA来比还是差了很多。看过本文之后，只有一个疑问，简化后的模型为什么比稍微复杂一点的模型好那么多呢？</p>
<p>最后作者总结了下Reading Comprehension任务中常用的数据集：</p>
<p>1、CNN/Daily Mail</p>
<p>2、MCTest</p>
<p>3、Children Book Test（CBT）</p>
<p>4、bAbI</p>
<p>本周末计划将本周看过的几篇reading comprehension写成一篇综述，好好做一次系统地对比和总结。敬请期待。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-13T17:57:22.000Z"><a href="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">2016-06-13</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">Teaching Machines to Read and Comprehend #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>昨天的文章text comprehension系列的第一篇，是最近刚刚submit的文章，今天分享一篇去年的文章，也是一篇非常经典的文章。我记得Yoshua Bengio在Quora Session一个问题中推荐这篇文章。本文的题目是<a href="http://arxiv.org/pdf/1506.03340.pdf" target="_blank" rel="external">Teaching Machines to Read and Comprehend</a>，作者是来自Google DeepMind的科学家<a href="http://www.karlmoritz.com/" target="_blank" rel="external">Karl Moritz Hermann</a>，是Oxford的博士后，两家机构的合作好多，很多文章都是一起写的。</p>
<p>本文的贡献主要有两点：一是提出了一种构建用于监督学习的阅读理解大型语料的方法，并开源在<a href="https://github.com/deepmind/rc-data" target="_blank" rel="external">Github</a>上，并且给出了两个大型语料，CNN和Daily Mail；二是提出了三种用于解决阅读理解任务的神经网络模型。</p>
<p>首先，聊一聊语料的构建方法。基本的思路是受启发于自动文摘任务，从两个大型的新闻网站中获取数据源，用abstractive的方法生成每篇新闻的summary，用新闻原文作为document，将summary中去掉一个entity作为query，被去掉的entity作为answer，从而得到阅读理解的数据三元组(document,query,answer)。这里存在一个问题，就是有的query并不需要联系到document，通过query中的上下文就可以predict出answer是什么，这也就失去了阅读理解的意义。因此，本文提出了用entity替换和重新排列的方法将数据打乱，防止上面现象的出现。这两个语料在成为了一个基本的数据集，后续的很多研究都是在数据集上进行训练、测试和对比。处理前和后的效果见下图：</p>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig1.png" width="600" height="600">
<p>接下来，介绍下本文的三个模型：</p>
<p>用神经网络来处理阅读理解的问题实质上是一个多分类的问题，通过构造一些上下文的表示，来预测词表中每个单词的概率，概率最大的那个就是所谓的答案。说起这一点，不禁想起了一个有趣的说法，任何nlp任务都可以用分类的思路来解决。</p>
<p>1、Deep LSTM Reader</p>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig2.png" width="400" height="400">
<p>看上图，其实非常简单，就是用一个两层LSTM来encode query|||document或者document|||query，然后用得到的表示做分类。</p>
<p>2、Attentive Reader</p>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig3.png" width="400" height="400">
<p>这个模型将document和query分开表示，其中query部分就是用了一个双向LSTM来encode，然后将两个方向上的last hidden state拼接作为query的表示，document这部分也是用一个双向的LSTM来encode，每个token的表示是用两个方向上的hidden state拼接而成，document的表示则是用document中所有token的加权平均来表示，这里的权重就是attention，权重越大表示回答query时对应的token的越重要。然后用document和query的表示做分类。</p>
<p>3、Impatient Reader</p>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig4.png" width="400" height="400">
<p>这个模型在Attentive Reader模型的基础上更细了一步，即每个query token都与document tokens有关联，而不是像之前的模型将整个query考虑为整体。感觉这个过程就好像是你读query中的每个token都需要找到document中对应相关的token。这个模型更加复杂一些，但效果不见得不好，从我们做阅读理解的实际体验来说，你不可能读问题中的每一个词之后，就去读一遍原文，这样效率太低了，而且原文很长的话，记忆的效果就不会很好了。</p>
<p>实验部分，作者选了几个baseline作为对比，其中有两个比较有意思，一是用document中出现最多的entity作为answer，二是用document中出现最多且在query中没有出现过的entity作为answer。这个和我们在实际答题遇到不会做的选择题时的应对策略有一点点异曲同工，所谓的不会就选最长的，或者最短的，这里选择的是出现最频繁的。</p>
<p>最终的结果，在CNN语料中，第三种模型Impatient Reader最优，Attentive Reader效果比Impatient Reader差不太多。在Daily Mail语料中，Attentive Reader最优，效果比Impatient Reader好了多一些，见下表：</p>
<img src="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/fig5.png" width="400" height="400">
<p>开始在看语料构建方法的时候，我在想应该是用extractive的方法从原文中提取一句话作为query，但看到paper中用的是abstractive的方法。仔细想了一下，可能是因为extractive的方法经常会提取出一些带有指示代词的句子作为摘要，没有上下文，指示代词就会非常难以被理解，从而给后面的阅读理解任务带来了困难，而用abstractive的方法做的话就会得到质量更高的query。本文的最大贡献我认为是构建了该任务的大型语料，并且配套了三个神经网络模型作为baseline，以方便后面的研究者进行相关的研究，从很大地程度上推进了这个领域的发展。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-12T17:59:52.000Z"><a href="/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/">2016-06-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/">当我们在谈论deep learning的时候，我们在谈论什么？</a></h1>
  

    </header>
    <div class="entry">
      
        <p>标题起的有一点装x了，昨天看到微博上刘知远老师对关于deep learning哪年火的问题的讨论，突然有一些自己的感触就写了下来。</p>
<p>从接触nlp到现在大概过去了4个月的时间，最初的动机是要用word2vec工具包来给自己写的app(rsarxiv)添加一个paper knowledge graph的功能。当时用word2vec的感受时，参数太多了，不知道这些参数到底是什么意思，所以就想着看看paper，看看源代码来试着理解下每个参数到底起什么作用，以方便我使用它。就是从这个时候开始算是接触了nlp。</p>
<p>因为觉得自动文摘是一个非常炫酷的功能，同时也是想给自己的app添加一个根据查询结果自动生成文献综述的功能，所以开始看一些自动文摘方面的paper，正好微博上找到了一个paper list，里面列出了近两年abstractive summarization相关的paper，加上自己买的一些书中介绍了很多传统的extractive的方法，经过了一个月时间的学习，自己从这些资料中学到了很多关于自动文摘的东西，为了记录下所学到的东西和理解到的东西，就写了一个系列博客——<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>。在看abstractive思路的时候，接触到了当前研究的一个热门方法，seq2seq+attention，这个组合几乎席卷了nlp的所有任务，一次又一次地刷新着排行榜。seq2seq的思路其实并不复杂，没有太多晦涩难懂的数学公式，看着模型图和下面的解释基本就能看懂思路是怎样的，attention也没有多么难，只是在seq2seq的接触上将输出与输入之间的关系考虑更加全面了，而不是简单地认为输出的每一个部分都与整个输入有关，而应该是将注意力放在相关的输入上。</p>
<p>因为看自动文摘的paper比较过瘾，就想着可以不可以做一个公众号来督促自己每天读一篇paper，写一篇博客。这样既可以养成一个读paper的好习惯，又可以提高自己的写作水平，同时也能够分享给可能感兴趣的童鞋。于是乎开始了PaperWeekly这个side project，前一天晚上睡觉前读paper，理解了其中模型的思路，第二天早上早起，开始写作，希望用尽量短的话介绍清楚该篇paper的模型思路和贡献。关于公众号如何做的问题，和另外一个账号的作者讨论过。他认为发布频率不应该太高，一天一篇太高了，而且要写长文，写干活，写高质量的东西，这样才能提供给用户最好的服务。我觉得他的话没有一点瑕疵，但我做PaperWeekly的初衷还是以自己为主，希望自己每天可以读一篇，写一篇，用最概括的话、图来讲清楚paper的贡献，当然不排除有的用户不喜欢这种方式，那么我只能说您不适合我这个公众号，我也不会因此去改变。于是，这个side project坚持了一个多月，现在有文章22篇+自动文章8篇一共30篇。当然读过的paper不止这些，30篇是写下来的。<code>今后的形式可能是这样，工作日时间较短，所以写单篇，周末的话，时间充裕，写综述，所以每周末都会将下周的5篇文章选好，尽量是相似topic的，这样方便写综述。</code>我觉得这是一个好习惯，长此以往坚持下来，一定会有很大的收获，这一点我坚信。</p>
<p>看了很多paper，也明白了很多的model，剩下的部分应该就是动手实践了。在选择框架的路上走过一些弯路，最开始用纯python写过一些简单的nnlm这样的模型，后来觉得用框架效率更高一些，于是尝试了keras，一个基于theano和tensorflow的框架，使用方法和torch差不多。如果只是解决一些常用的model结构的话，用keras非常地easy，代码量非常小，非常容易上手，比如rnn，cnn等等。但如果你想实现一个稍微复杂的model，对灵活性要求比较高的model，keras就有点捉襟见肘了，毕竟是一个框架上的框架，灵活性肯定好不了。后来就决定试一下theano，毕竟是deep learning发源地之一出的框架，github上开源了很多的程序都是用theano写的，而且自己也比较擅长python，于是就开始了theano的学习之路，其中最吸引我的是自动求导的功能，但最终导致我放弃theano的一个重要的原因是每次报错都让我特别头疼，因为根本没法找到错误的地方。挣扎了几天，通过重新调研，我选择了Torch，Torch是用lua封装的，意味着我得先学习lua，然后就开始torch，习惯了python的简单，用lua时感觉特别恶心，说不出原因的恶心，后来强忍着开始看一些demo，当初给自己的一个目标是用torch写出seq2seq+attention，然后做一些好玩的事情，于是找了HarvardNLP开源的代码<a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">seq2seq-attn</a>，这个组非常年轻，但非常nice，paper的code都会开源供大家学习。第一次看这个代码就觉得，写的好他么凌乱啊，根本没法读，现在来读的话也觉得很恶心，感觉写代码的人没有太多的规范，想到哪里写到哪里，于是就放弃了这个demo。后来跟着oxford的<a href="https://github.com/oxford-cs-ml-2015/practical6" target="_blank" rel="external">课程代码</a>开始一步一步地学习torch，这个project是实现一个char-level语言模型，里面的功能虽然不够完善，但从最基础的东西开始写起，并没有用类似于rnn，dp这样的框架来做，非常适合入门学习和打基础。跟着这个project，也看着<a href="https://github.com/karpathy/char-rnn" target="_blank" rel="external">char-rnn</a>，其实char-rnn也是跟着oxford的这个程序学着写的，很多的套路和源码都一样。经过了一段时间的挣扎，从0开始写起，实现了一个seq2seq+attention的torch源码，我个人认为比seq2seq-attn那个代码更加清晰和简洁，过段时间整理下会放在Github上。torch有很多的缺点，比如需要学习lua，处理文本不方便，demo不给力。所以，在文本处理这一块，我用了python+hdf5的方式来解决，因为用python处理文本太方便了，然后将处理好的结果放入hdf5中，让torch去调用。demo这块是一个很大的关，只要自己可以动手写出一个完整的project之后，后面的事情就好办了。最近一段时间，tensorflow在网上非常火，有很多的文章和微博大号都在热推他，我觉得框架各有各好，坚持用好一个就会很了不起了，不必盲目跟风。</p>
<p>以上的部分是我从接触nlp开始，到现在的一个学习过程，下面的部分谈谈我的一些感触。</p>
<p>deep learning从哪年火起来，并不重要，重要的是它还能火多久？会不会遭遇另一个寒冬？媒体的热捧是跟风、炒作还是客观？现在如果你不聊两句deep learning都不好意思出门。我认为对一个知识的理解大概有三个level：是什么？怎么样？为什么？</p>
<p>第一个level，是什么的问题。RNN、CNN、RCNN、CRNN、FNN、DNN各种各样的NN充斥在各大媒体上，每天都有大量的文章来介绍各种NN做了什么牛逼的事情，哪些牛逼的机构提出了一种新的NN，将会给人类带来前所未有的方便等等等等。大家通过看一些博客，看一些新闻媒体都会了解到这些NN的概念和作用，以及类似于端到端、注意力模型的这样的概念。</p>
<p>第二个level，是怎么样的问题。知道概念并不难，做出来才是真好汉！如何提出一个自己的模型，然后用熟悉的框架编程实现它，跑分排名发paper。这个level应该是比较难的level，可能也是一些学生处于的level。之前记得王威廉老师发过一个讨论帖：</p>
<blockquote>
<p><strong>跟大家探讨几个开放式问题：大家认为到底计算机科学(Science)与工程(Engineering)的边界在哪里？以深度学习来说，学术界和工业界的着重点应该有什么不同与相同之处？如果本科毕业生也能玩转Theano/TensorFlow/Torch等平台的话，那么深度学习的博士优势到底何在？</strong></p>
</blockquote>
<p>如果大家可以玩转torch之类的框架，并且实现自己提出的模型，然后跑分刷榜发paper，那么做深度学习研究的博士优势又会在哪里呢？</p>
<p>另外一种流传于网络的说法是，deep learning就是比谁更会调参数，这里不得不祭出这张图：</p>
<img src="http://ww2.sinaimg.cn/mw690/ba115fdfjw1f4nwmxwkn4j20ak05x74m.jpg" width="400" height="400">
<p>非常地讽刺，我想应该不是这么简单。</p>
<p>还有一点是model这部分，可能是文章看的不够深，看到的paper都是从model这个层次来创新的，基本搞清楚整个数据流，输入和输出然后就会比较清楚了，而文章中一般也会用一张图把model的数据流讲解地非常清楚。在基本的model上添加gate，用hierarchical来做，套用seq2seq加attention，用copy mechanism等等方式来提出新model，获得更好的结果，亦或者是将人工feature添加到模型中。model是一个非常灵活的东西，记得本科时参加数学建模竞赛就是在做这样的一件事情，根据问题来提出自己模型，往往都是从已有模型中进行一些改进。</p>
<p>第三个level，是为什么的问题。我个人认为Phd应该能够回答出自己所研究领域的各种各样的为什么，回答出不同level的人提出的为什么，而不仅仅是会用一个框架，提出几个model，然后刷几篇顶会就博士毕业了。更重要的是对问题的思考和理解，尤其是深度地理解。我觉得一个Phd应该具备的能力是提出一个问题，分析一个问题，解决一个问题的能力，而不仅仅是简单重复别人的东西，或者是简单改进下别人的model。调参数是一个基本工作，属于工程型的范畴，为了达到一个更好的效果，需要具备这个基本能力，但并不等同于说deep learning就是调参数，这种说法太过荒唐可笑。框架也只是一个工具，至于你用torch，用tensorflow或者是自己手写都只是一种工具而已，也是一个基本能力，但并不是说deep learning就是用框架。这种说法同样很可笑。一个升级版的model也可以发paper，但是否你的model真的改变了研究现状，带来了革命，而或者只是在原来model的基础上添加了一些小的想法，刷了一下排行榜，这里引用下不久前ACL主席Christopher D. Manning在文章中写过一句话：</p>
<blockquote>
<p><strong>However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.</strong></p>
</blockquote>
<p>只是刷分并没有意义，研究的意义应该是对问题本身的认识。虽然deep learning看着热闹，会议非常多，隔一段时间就会出现一个会议论文集，但仔细想想有仅仅有几篇paper是在做更大更重要的事情，大量的paper还是处于model创新这个level。当然，毕业对论文的要求是一个很大的压力，所以这样的现象也并不奇怪。数量的增加必定会带来质量的下降，浮躁的气息必定会让大家都变得急功近利。</p>
<p>以上是一个初学者对deep learning的一个非常浅薄的认识，很多观点只是这一阶段的观点，可能随着学习的深入会有不同的想法。看paper也有一段时间了，心中一直有些疑问，比如：</p>
<p>1、gate函数的提出是基于怎样的一种情况，为什么要用gate而不是别的？lstm，gru等单元都采用了gate函数，通过这个函数成功地解决了rnn的长程依赖问题。</p>
<p>2、miniBatch中batch的大小为什么会影响结果的优劣，包括其他的超参数，可不可以给出一些偏理论的分析，而不只是说大家都这么用，这就是经验这样的说法。</p>
<p>3、hierarchical是不是都会比non-hierarchical更好呢？分层之后的什么导致了更好的结果？</p>
<p>4、optimization是一个数学味道非常浓的学科，那么很多的model都采用sgd，或者是adam，或者是rmsprop，区别在哪里？哪种算法适合哪种model。</p>
<p>5、deep learning到底多deep算deep，越deep越好？还是说到了一定的deep就ok了，神经网络是用来近似非线性函数的，那么可不可以计算出多深的网络可以以最低的误差来拟合函数？</p>
<p>等等等等，心中有太多的疑惑，虽然现在可以提自己的model，也可以用框架来编程实现，但仍然回答不出上面的问题，那些为什么的问题。</p>
<p>路漫漫其修远兮，吾将上下而求索。</p>
<p>最后是广告时间，如果您对PaperWeekly做的事情感兴趣，可以关注下面的公众号，或者<a href="http://rsarxiv.github.io/atom.xml">订阅</a>本博客。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-12T16:33:10.000Z"><a href="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">2016-06-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">Gated-Attention Readers for Text Comprehension #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>完形填空一直是各大英语考试的常见题型，读一篇短文，填20个空。那么如果是机器来做完形填空，该如何来定义问题，提出模型呢？本周开始将会介绍一系列文本理解的模型。本文分享的题目是<a href="http://cn.arxiv.org/pdf/1606.01549v1" target="_blank" rel="external">Gated-Attention Readers for Text Comprehension</a>，最早于6月5日submit于arxiv上，作者是CMU的Graduate Research Assistant <a href="https://www.cs.cmu.edu/directory/bdhingra" target="_blank" rel="external">Bhuwan Dhingra</a>。</p>
<p>首先，介绍一下对完形填空问题的定义。问题可以表述为一个三元组(d,q,a)，这里d是指原文document，q是指完形填空的问题query（这里需要注意一点的是，与我们英语考试中的完形填空不同，更像是只用一个单词来回答的阅读理解），a是问题的答案。这个答案是来自一个固定大小的词汇表A中的一个词。即：给定一个文档-问题对(d,q)，从A中找到最合适的答案a。</p>
<p>本文精彩的部分有两个，一个是related work写的非常漂亮，另一个是提出了一种新的注意力模型GA（Gate-Attention） Reader，并且取得了领先的结果。</p>
<p>后续的文本理解系列的文章将会从related work中产生，包括以下几篇：</p>
<p>[1] <a href="http://arxiv.org/pdf/1506.03340v3.pdf" target="_blank" rel="external">Teaching machines to read and comprehend</a></p>
<p>[2] <a href="https://arxiv.org/pdf/1406.2710v1.pdf" target="_blank" rel="external">A multiplicative model for learning distributed text-based attribute representations</a></p>
<p>[3] <a href="http://www.cl.ecei.tohoku.ac.jp/publications/2016/kobayashi-dynamic-entity-naacl2016.pdf" target="_blank" rel="external">Dynamic entity representations with max-pooling improves machine reading</a></p>
<p>[4] <a href="http://arxiv.org/pdf/1603.01547v1.pdf" target="_blank" rel="external">Text understanding with the attention sum reader network</a></p>
<p>[5] <a href="http://arxiv.org/pdf/1511.02301v4.pdf" target="_blank" rel="external">The goldilocks principle: Reading children’s books with explicit memory representations</a></p>
<p>[6] <a href="http://arxiv.org/pdf/1606.02858v1.pdf" target="_blank" rel="external">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</a></p>
<p>下面来介绍本文的模型，结合下图来看：</p>
<img src="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig1.png" width="600" height="600">
<p><b>step 1</b> document和query通过一个Lookup层，使得每个词都表示成一个低维向量。</p>
<p><b>step 2</b> 将document中的词向量通过一个双向GRU，将两个方向的state做拼接获得该词的新表示。同时也将query通过一个双向GRU，用两个方向上的last hidden state作为query的表示。</p>
<p><b>step 3</b> 将document中每个词的新表示与query的新表示逐元素相乘得到下一个GRU层的输入。</p>
<p><b>step 4</b> 重复step 2和3，直到通过设定的K层，在第K层时，document的每个词向量与query向量做内积，得到一个最终的向量。</p>
<p><b>step 5</b> 将该向量输入到softmax层中，做概率归一化。</p>
<p><b>step 6</b> 因为document中有重复出现的词，聚合之后得到最终的分类结果，即确定应该填哪个词。</p>
<p>模型的计算流程还是很好理解的，下面给出一些可视化的attention结果。</p>
<img src="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig2.png" width="400" height="400">
<p>图中高亮的部分是针对问题时的最后一层注意力所关注的地方。</p>
<p>注意力模型是一个非常热门的研究领域，很多专家都看好其在今后各大nlp任务中的应用前景，不同版本、不同结构、不同层次的注意力模型丰富了模型，也提升了效果。注意力的本质就是说你关注的输出与你的输入中的哪个元素关系更加紧密，即输出的部分应该更加注意哪个输入细节，在做完形填空、阅读理解的时候，我们也会有这样的感受，就是题目的答案往往就在某一句话或某几句话当中，并不需要回答每个问题都从全文中找一遍答案，而是定位到关键句上。这里的定位就是注意力，剩下的问题就是研究如何更加准确地定义、建模注意力，是用普通的前馈神经网络，还是用GRU，还是用分层模型都需要针对具体问题的特点。</p>
<p>后续的几篇文章将会继续介绍文本理解，敬请关注。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-11T23:48:38.000Z"><a href="/2016/06/11/PaperWeekly文章分类导航/">2016-06-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/11/PaperWeekly文章分类导航/">PaperWeekly文章分类导航</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h1><p>[1] <a href="http://rsarxiv.github.io/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">A Neural Probabilistic Language Model</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">Character-Aware Neural Language Models</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">Gated Word-Character Recurrent Language Model</a></p>
<h1 id="Word-Embeddings"><a href="#Word-Embeddings" class="headerlink" title="Word Embeddings"></a>Word Embeddings</h1><p>[1] <a href="http://rsarxiv.github.io/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/">Efficient Estimation of Word Representations in Vector Space</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/">GloVe: Global Vectors for Word Representation</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">How to Generate a Good Word Embedding</a></p>
<p>[4] <a href="http://rsarxiv.github.io/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">A Joint Model for Word Embedding and Word Morphology</a></p>
<h1 id="Sentence-Embeddings"><a href="#Sentence-Embeddings" class="headerlink" title="Sentence Embeddings"></a>Sentence Embeddings</h1><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">Learning Distributed Representations of Sentences from Unlabelled Data</a></p>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Distributed Representations of Sentences and Documents</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vectors</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a></p>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">Convolutional Neural Networks for Sentence Classification</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">Recurrent Convolutional Neural Networks for Text Classification</a></p>
<h2 id="Semi-Supervised-Learning"><a href="#Semi-Supervised-Learning" class="headerlink" title="Semi-Supervised Learning"></a>Semi-Supervised Learning</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">Semi-supervised Sequence Learning</a></p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">Neural Machine Translation by Jointly Learning to Align and Translate</a></p>
<h2 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">Neural Network-Based Abstract Generation for Opinions and Arguments</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/18/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89/">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/05/17/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/">Neural Headline Generation with Minimum Risk Training</a></p>
<p>[4] <a href="http://rsarxiv.github.io/2016/05/11/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%B9%9D%EF%BC%89/">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<p>[5] <a href="http://rsarxiv.github.io/2016/05/10/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AB%EF%BC%89/">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></p>
<p>[6] <a href="http://rsarxiv.github.io/2016/05/07/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%B8%83%EF%BC%89/">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></p>
<p>[7] <a href="http://rsarxiv.github.io/2016/04/30/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AD%EF%BC%89/">A Neural Attention Model for Abstractive Sentence Summarization</a></p>
<p>[8] <a href="http://rsarxiv.github.io/2016/04/30/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AD%EF%BC%89/">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</a></p>
<p>[9] <a href="http://rsarxiv.github.io/2016/04/24/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%BA%94%EF%BC%89/">Generating News Headlines with Recurrent Neural Networks</a></p>
<h2 id="Text-Entailment"><a href="#Text-Entailment" class="headerlink" title="Text Entailment"></a>Text Entailment</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</a></p>
<h2 id="Dialogue-Generation"><a href="#Dialogue-Generation" class="headerlink" title="Dialogue Generation"></a>Dialogue Generation</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">A Neural Conversational Model</a></p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-10T19:34:27.000Z"><a href="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">2016-06-10</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">Neural Network-Based Abstract Generation for Opinions and Arguments #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇将会分享的是一篇工程性比较强的paper，如果您想做一个实实在在的意见摘要系统（比如：淘宝商品评论摘要、电影评论摘要）的话，可以仔细研读下本文的解决方案。本文的题目是<a href="http://arxiv.org/pdf/1606.02785v1.pdf" target="_blank" rel="external">Neural Network-Based Abstract Generation for Opinions and Arguments</a>，于6月9日submit于arxiv上。作者是来自美国东北大学的<a href="http://www.ccs.neu.edu/home/luwang/" target="_blank" rel="external">Lu Wang</a>助教。</p>
<p>关于<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>，之前写过一系列的文章，包含了自动文摘的方方面面以及近期的一些相关paper的详细描述。本文的自动文摘问题是一个多评论摘要问题，用的是abstractive方法，而非简单的extractive方法，就是说从多个评论中总结出观点。</p>
<p>本文模型的主题框架仍是seq2seq+attention，最主要的不同之处是输入包括多个文本序列，而是之前介绍的单文本序列。这里，seq2seq+attention的思路不再赘述，主要讲一下不同的地方。</p>
<p>为了套用seq2seq，本文将多文本拼接成单文本，中间用特殊的标记SEG隔开。但是如果只是简单的套用seq2seq的话，会存在以下两个问题：</p>
<p>1、seq2seq对序列的顺序非常敏感，多个文本排列的顺序对结果的影响比较大。</p>
<p>2、多篇评论包括的词会比较多，会导致在计算attention的时候花费更大的时间代价。</p>
<p>本文用了子采样(sub-sampling)的方法来解决上面的问题，首先给原始输入中的每个评论定义importance score，然后归一化，最后从原始输入中进行多项分布采样，获得K个候选sample作为seq2seq的输入数据，进行训练。本文针对importance score建立了一个回归模型，使用了一些人工feature作为输入进行回归打分。这些feature如下表所示：</p>
<img src="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/fig1.png" width="300" height="300">
<p>包括了词的数量，命名实体的数量，tf-idf平均数和最大数等8个feature作为输入。通过学习这个回归模型，来计算给定评论的分数。</p>
<p>最后给大家展示一个结果图：</p>
<img src="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/fig2.png" width="600" height="600">
<p>本文在模型上创新的点并不突出，最不同以往的地方便是用了人工feature来给每个评论打分，给原始输入中的评论进行排序，然后多项分布采样，子采样的过程是一个降维的过程，保留了原始数据中最重要的部分，去掉了冗余的信息。可以说本文是将人工features添加到abstractive来提升纯粹的seq2seq模型性能，针对了多文档摘要问题的特点，给出了一个实用性较强的思路。如果从模型角度来说，新的东西没有太多，而且可改进的地方有很多，比如打分模型，可以用sentence representation的思路来做，完全可以避免用人工feature这种比较low的思路，做成一个data-driven的打分模型；再比如，不用打分，而是采用CNN从多个评论中提取出最有用的feature作为输入。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-09T21:34:03.000Z"><a href="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">2016-06-09</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">A Joint Model for Word Embedding and Word Morphology #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>大家端午节快乐！本文将分享一篇关于词向量模型最新研究的文章，文章于6月8号提交到arxiv上，题目是<a href="http://cn.arxiv.org/pdf/1606.02601v1.pdf" target="_blank" rel="external">A Joint Model for Word Embedding and Word Morphology</a>，作者是来自剑桥大学的博士生<a href="https://www.cl.cam.ac.uk/~kc391/" target="_blank" rel="external">Kris Cao</a>。</p>
<p>本文最大的贡献在于第一次将词形联合词向量一同进行训练，从某种程度上解决了未登录词（OOV）的词向量表示问题，同时也得到了一个效果不错的词形分析器。</p>
<p>介绍本文模型之前先简单介绍下本文中采用的词向量训练方法，skip-gram with negative sampling（SGNS）。这个方法是word2vec中的一种方法，大概的思路是可参见下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig3.png" width="600" height="600">
<p>通过用dog这个词来预测其上下文，比如cute、fluffy、barked、loudly，为了更快地收敛，增加负样本，即图中的bicycle和Episcopal这两个与dog无关的词。skip-gram的思路就是通过word来预测上下文context，而negative sampling则是根据当前词构造出一些与之无关的词，作为负样本加速收敛。</p>
<p>接下来介绍本文的模型Char2Vec，将字符作为最小的单元进行研究，因为对于字符这个层次来说，并不会出现OOV词的情况。具体看下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig1.png" width="600" height="600">
<p>在每个单词的首和尾分别添加符号^和$作为标记，将词看作是一个字符序列。在这个序列上用一个正向LSTM和一个反向LSTM得到两组hidden state，每个位置上的字符都对应着两个hidden state，将其拼接起来，然后用一个单层前馈神经网络进行处理，得到该位置上的hidden state，记为h(i)。有了每个字符的表示，接下来用attention机制来构造出词的表示，即学习一个权重系数，来表明这个词的语义与哪个h(i)关系更大，一般来说词干所在的h(i)权重会大一些，词前缀或者后缀并不能表示语义，所以权重会小很多。见下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig2.png" width="600" height="600">
<p>图中的单词malice、hatred、greed会与序列中的词干spit、spite，前缀^、后缀ful关系就会紧密一些，而与其他错误的字符串关系不大。</p>
<p>通过attention model我们得到了词向量f(w)。剩下的过程就是用skip-gram with negative sampling来训练词向量了。先前的工作都是用lstm处理字符序列来表示整个单词向量，本文并没有这样做，而是将直接使用attention model来获取每个h(i)中的信息，包括一个正向的lstm和反向的lstm，正向的lstm包含了词干和词前缀，反向的lstm包含了词后缀。当我们处理未知的词时，可以将这个词分解为已知的部分和未知的部分，这个模型就可以通过已知的部分来预测整个词的词向量，因此解决了OOV的问题。</p>
<p>实验中测试了该模型的词形分析的能力，尤其是在单词词形很丰富（包括词干、前缀、后缀）的情况下，效果优于一些成熟的分词器。看下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig4.png" width="300" height="300">
<p>在词向量效果测试中，本文模型在semantic测试中表现很差，但在syntactic测试中表现非常好。看下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig5.png" width="300" height="300">
<p>看得出来本文模型的优势非常明显，优势在于解决了大量处于长尾尾端的合成词的词向量表示问题，通过用未知词的已知部分（词干）来预测该词的词向量，从而解决了word2vec等一系列前人工作中未解决的问题，在英语语境中效果可能没那么好，如果换作是德语或者土耳其语这种词形非常丰富的语言会有更好的效果。在整个任务评测中，可以更好地解决syntactic相似问题，因为引入了词形这个feature可以很好地解决syntactic任务；而在semantic任务中却表现非常差，原因是char-level的词向量模型在捕捉语义上效果本身就不如word-level的模型。可以说，本文在传统词向量模型的基础上考虑加入feature来提升性能，是一种非常积极的尝试，虽然并没有在方方面面上都得到改善，但毕竟是一个探索性的、且非常有意义的工作，值得学习。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-08T16:00:50.000Z"><a href="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">2016-06-08</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">Gated Word-Character Recurrent Language Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇将会分享一篇最新的paper，2016年6月6日submit到arxiv上，paper的题目是<a href="http://cn.arxiv.org/pdf/1606.01700v1" target="_blank" rel="external">Gated Word-Character Recurrent Language Model</a>，作者是来自纽约大学的硕士生<a href="http://cds.nyu.edu/people/yasumasa-miyamoto/" target="_blank" rel="external">Yasumasa Miyamoto</a>。</p>
<p>语言模型或者说一切自然语言生成的问题都面临着一个严峻的挑战就是未登录词（OOV），一般的语言模型处理方法都是将前N个高频词当做词表，后面的低频词都用unk来代替，而且所有的低频词都用同一个词向量来表示。本文的最大贡献在于提出了一种混合char-level和word-level的语言模型，通过一种gate机制来选择是用char-level来表示一个词向量，还是直接用word-level来表示一个词向量。char-level模型的优势在于解决低频词的表达，很多之前分享过的模型都是用char来作为基本单元。</p>
<p>本文的模型并不复杂，思路也非常清晰，如下图：</p>
<img src="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig1.png" width="600" height="600">
<p>模型分为两个部分：</p>
<p>1、词向量。模型中的词向量由两部分综合而成。第一部分是传统的词向量，每一个词都用一个低维实向量来表示，第二部分是将每个词认为是一个char-level的序列，用一个双向LSTM来表示这个词。两部分词向量由一个门函数来决定使用哪个，如下式：</p>
<img src="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig2.png" width="300" height="300">
<p>门函数我们见过太多了，尤其是在LSTM和GRU中，各种各样的门函数来控制信息的流动，本文模型中采用了一种非常简单的机制来决定采用哪种词向量，高频词的话，一定是采用传统的word-level方式，直接从lookup table中读取；低频词的话，用char-level的方式获得一个更好的表示。这里需要注意的一点是，门函数的值，也就是说每个单词用哪种词向量是与上下文无关的，只要是同一个单词，就会采用相同的选择方式。</p>
<p>2、语言模型。这个部分就非常简单了，就是一个典型的RNNLM，这里的隐藏单元采用LSTM。</p>
<p>实验部分选了三个baseline，（1）仅仅用word-level，（2）仅仅用char-level，（3）将两种词向量拼接。在三个数据集上进行了测试，本文模型比起baseline具有明显的优势。</p>
<p>最后简单讨论了门函数值与词出现的频率之间的关系，如下图：</p>
<img src="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig3.png" width="600" height="600">
<p>本文采用了一中混合模型，然后用gate mechanism从多个模型中进行选择。这种思路有一种似曾相识的感觉，好比是参加kaggle比赛，通常一个分类器并不能得到最好的结果，混合使用多个分类器往往会得到更好的结果。本文的感觉有一点类似，用了char-level的优势来弥补word-level的劣势，从而取得更好的效果。也是一种很好的启发。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-07T14:11:34.000Z"><a href="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">2016-06-07</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">Semi-supervised Sequence Learning #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>之前分享过几篇有监督的sentence表示方法，比如<a href="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">Recurrent Convolutional Neural Networks for Text Classification</a>、<a href="http://rsarxiv.github.io/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">Convolutional Neural Networks for Sentence Classification</a>，也分享过很多几篇无监督的sentence表示方法，比如<a href="http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Distributed Representations of Sentences and Documents</a>、<a href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vectors</a>。本篇将分享是一篇半监督的sentence表示方法，该方法比Paragraph Vectors更容易做微调，与Skip-Thought相比，目标函数并没有它那么困难，因为Skip-Thought是用来预测相邻句子的。本文的题目是<a href="http://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="external">Semi-supervised Sequence Learning</a>，作者是来自Google的<a href="http://homepages.inf.ed.ac.uk/s0681274/About_Me.html" target="_blank" rel="external">Andrew M. Dai</a>博士。</p>
<p>纯粹的有监督学习是通过神经网络来表示一个句子，然后通过分类任务数据集去学习网络参数；而纯粹的无监督学习是通过上文预测下文来学习句子表示，利用得到的表示进行分类任务。本文的方法将无监督学习之后的表示作为有监督训练模型的初始值，所以称为半监督。本文的有监督模型采用LSTM，无监督模型共两种，一种是自编码器，一种是循环神经网络语言模型。</p>
<p>第一种模型称为Sequence AutoEncoder LSTM(SA-LSTM)，模型架构图如下：</p>
<img src="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/fig1.png" width="400" height="400">
<p>这幅图大家看着都眼熟，和<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>中的seq2seq架构图很相似，只不过target和input一样，即用input来预测input自己。将自编码器学习到的表示作为LSTM的初始值，进行有监督训练。一般来说用LSTM中的最后一个hidden state作为输出，但本文也尝试用了每个hidden state权重递增的线性组合作为输出。这两种思路都是将无监督和有监督分开训练，本文也提供了一种联合训练的思路作为对比，称为joint learning。</p>
<p>第二种模型称为Language Model LSTM(LM-LSTM)，将上图中的encoder部分去掉就是LM模型。语言模型介绍过很多了，比如<a href="http://rsarxiv.github.io/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">A Neural Probabilistic Language Model</a>和<a href="http://rsarxiv.github.io/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">Character-Aware Neural Language Models</a>，详细的可以看之前的分享，这里不再赘述了。</p>
<p>模型部分就是这些，后面作者在情感分析、文本分类、目标分类等多组任务中进行了对比实验，均取得了不错的结果。</p>
<p>本文的创新点在于结合了无监督和有监督学习两种思路的优点来解决一个传统问题，虽然说无监督是一种趋势所在，但有监督针对具体的问题会有更好的效果。这种融合各类模型优点的模型会更受欢迎，也是一种不错的思路。</p>
<p>这里进行几点说明：</p>
<p>1、为什么不对实验结果进行详细的介绍？</p>
<p>因为我个人更加关注的是解决问题的思路，也就是模型部分；另一方面，paper中的实验结果只能在某些程度上说明问题，对比结果中的数据可能是作者精心挑选的最好数据，并不一定可以复现，所以我不会太纠结于到底哪个模型比哪个模型高几个百分点。而文中的模型思路会带给我更多的启发，所以更加有意义一些。</p>
<p>2、为什么内容总是这么短？</p>
<p>因为我对PaperWeekly的定位是每天一篇或者几天一篇的paper短文介绍和理解，并不是详细地剖析它，我希望内容尽可能短，大家可以用5-10分钟来明白一篇文章的贡献和创新点在哪里，更多的是为了带给大家更多的思考或者说是启发。另外一个方面，短的文章我写起来也会很快，基本上都是前一晚睡觉前来读，六点半早起来写，不影响一天的正常工作生活，却一天一天地在积累着。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-06T14:09:52.000Z"><a href="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">2016-06-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">A Hierarchical Neural Autoencoder for Paragraphs and Documents #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将会分享一篇用自动编码器(AutoEncoder)来做文档表示的文章，本文的结果会给自然语言生成、自动文摘等任务提供更多的帮助。本文作者是来自斯坦福大学的博士生<a href="http://web.stanford.edu/~jiweil/" target="_blank" rel="external">Jiwei Li</a>，简单看了下其简历，本科居然是北大生物系的，是一个跨界选手。本文的题目是<a href="http://arxiv.org/pdf/1506.01057.pdf" target="_blank" rel="external">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a>，于2015年6月放在arxiv上。</p>
<p>自动编码器之前接触的并不多，所以读了下Yoshua Bengio的deep learning一书补了一下知识。其实挺简单的，就是通过构造一个网络来学习x-&gt;x，最简单的原型就是h=f(x)，x=g(h)。如果输入和输出的x都是完全一样的话，那么就没什么意义了。一般来说，后一个x会与前一个x有一些“误差”或者说“噪声”。而且自动编码器关注的是中间层h，即对输入的表示。如果h的维度小于x的维度，学习这个表示其实就是一个降维的过程。自动编码器有很多种类型，这里就不一一赘述了。</p>
<p>本文的贡献在于用分层LSTM模型来做自动编码器。模型分为三个，为递进关系。</p>
<p>1、标准的LSTM，没有分层。模型结构看起来和最简单的seq2seq没有区别，只是说这里输入和输出一样。看下图：</p>
<img src="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig1.png" width="600" height="600">
<p>2、分层LSTM。这里分层的思想是用句子中的所有单词意思来表示这个句子，用文档中的所有句子意思来表示这个文档，一层接一层。看下图：</p>
<img src="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig2.png" width="600" height="600">
<p>在word这一层，用一个标准的LSTM作为encoder，每一句中的最后一个word的hidden state作为该句的state，在sentence这一层，文档中所有的句子构成一个序列，用一个标准的LSTM作为encoder，得到整个文档的表示。decoder部分同样是一个分层结构，初始state就是刚刚生成的文档表示向量，然后先decoder出sentence这一层的表示，然后再进入该sentence对其内部的word进行decoder。</p>
<p>3、分层LSTM+Attention，这里的Attention机制和之前分享的是一样的，并且只在sentence这一层用了attention，参看下图：</p>
<img src="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig3.png" width="600" height="600">
<p>在decoder部分中生成句子表示时，会重点注意输入中与该句子相关的句子，也就是输入中与之相同的句子。这里注意力的权重与<a href="http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">Neural Machine Translation by Jointly Learning to Align and Translate </a>中的计算方法一样。</p>
<p>在实验中验证了本文模型的有效性，并且经过对比验证了第三种模型的效果最好，其次是第二种，最差的第一种，也与预期的相符。</p>
<p>昨天分享的也是一个分层模型，相比于单层的模型效果更好一些，这是否可以引起一些思考？本文也提到后面可以将本文的这种思想应用到自动文摘、对话系统、问答系统上。虽然seq2seq+attention已经在这几大领域中取得了不错的成绩，但如果改成分层模型呢，是不是可以取得更好的成绩？是不是可以将本文的input和output换作自动文摘中的input和target，然后用同样的方法来解决呢？我想应该是可以的。</p>
<p>另外，因为我个人比较关注自动文摘技术，自动文摘中abstractive类的方法一般都会涉及到Paraphrase（转述，换句话说），本文的自动编码器模型正好很适合做Paraphrase，输入一句话或者一段话，得到一个带有“误差”的语句通顺的版本。一种最简单的思路，用传统的方法提取出文中最重要的几句话（extractive式的方法），用Paraphrase处理一下得到文本摘要。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-05T17:57:48.000Z"><a href="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">2016-06-05</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>昨天介绍了一篇工程性比较强的paper，关于对话生成（bot）任务的，今天继续分享一篇bot方面的paper，6月2日刚刚submit在arxiv上。昨天的文章用了一种最最简单的端到端模型来生成对话，取得了不错的结果，而本文用了一种更加复杂的模型来解决这个问题，取得了更好的结果。文章的题目是<a href="http://cn.arxiv.org/pdf/1606.00776v1" target="_blank" rel="external">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation</a>，作者是来自蒙特利尔大学的博士生<a href="https://mila.umontreal.ca/en/person/iulian-vlad-serban/" target="_blank" rel="external">Iulian Vlad Serban</a>。</p>
<p>本文最大的贡献在于提出了一种多尺度循环神经网络（Multiresolution RNN,MrRNN），这里的多尺度是指描述文本序列的方式有多种尺度，不仅仅是传统的用一个又一个word来表示序列，这种表示称为自然语言表示，还包括了一种所谓的high-level信息来表示文本序列，这种表示称为粗糙序列表示。本文的模型受启发于分层循环端到端模型（Hierarchical Recurrent Encoder-Decoder，HERD），该模型应用于搜索领域，将用户的search session划分为两个层次的序列，一个是query的序列，一个是每个query中词的序列。</p>
<p>本文模型中一个非常重要的部分是数据的预处理，将训练数据中的所谓high-level信息提取出来构造第二种序列来表示整个文本，这里用了两种思路。</p>
<p>1、提取文本中的名词。用词性标注工具提取出文本中的名词，去掉停用词和重复的词，并且保持原始的词序，还添加了句子的时态。通过这个过程构造了一种表示原始文本的序列。</p>
<p>2、提取文本中的动词和命名实体。用词性标注工具提取文本中的动词，并标记为activity，然后通过一些其他工具从所有训练数据中构造了一个命名实体的词典，帮助提取原句中的命名实体。因为数据集是ubuntu对话数据集，会涉及到大量的linux命令，所以还构造了一个linux命令词典，以标记原句中的命令。同样地也添加了句子的时态。通过这个处理过程，构造了另外一种表示原始文本的序列。</p>
<p>两种处理方法将原句用一种关键词的形式表示出来，尤其是第二种方法针对Ubuntu数据集的特点，包含了非常多的特征进来。这样的表示本文称为coarse sequence representation，包含了high-level的信息，比起单纯的word by word sequence具有更加丰富的意义。</p>
<p>接下来，看一下本文模型的架构图：</p>
<img src="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/fig1.png" width="600" height="600">
<p>模型中包括了两个层次，或者说是两种尺度，一种用了很多的词来表示一个句子，另外一种用了经过处理的包含了更加重要的信息的词来表示一个句子。下层生成的预测值将会作为上层docoder在做预测时的context的一部分，这部分context包含了重要的、high-level的信息，再加上上层自己encoder的输出也作为context，可以说这个模型的context包含了非常丰富的内容。理解上面的图，只要仔细看好箭头的指向，也就明白了各个部分的输入输出是哪些。每个time step的数据流过程如下：</p>
<p>下层：coarse encoder -&gt; coarse context -&gt; coarse decdoer -&gt; coarse predciton encoder</p>
<p>上层：natural language encoder -&gt; <b>(natural language context + coarse prediction encoder)</b> -&gt; natural language decoder -&gt; natural language prediction</p>
<p>不管是用自动评价指标还是人工评价，结果都表明了本文的模型效果比baseline要高出很多个百分点，远远好于其他模型。下面展示一个结果，是ubuntu数据集上的测试效果：</p>
<img src="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/fig2.png" width="600" height="600">
<p>可以看的出本文模型生成的结果效果比其他模型好很多。</p>
<p>本文模型并不是一个纯粹的数据驱动的模型，在初始的阶段需要做一些非常重要的数据预处理，正是这个预处理得到的序列表示给本文的好结果带来了保证。我想，这种处理问题的思路可以推广到解决其他问题上，虽然本文模型很难直接应用到其他问题上，但我相信经过一些不大的变化之后，可以很好地解决其他问题，比如我一直关注的自动文摘问题，还有机器翻译、自动问答等等各种涉及到自然语言生成问题的任务上。这篇文章的结果也支持了我之前的一个观点，就是在解决问题上不可能存在银弹，不同的问题虽然可以经过一些假设变成相同的数学问题，但真正在应用中，不同的问题就是具有不同的特点，如果只是想用一种简单粗暴的data driven模型来解决问题的话，相信效果会不如结合着一些该问题feature的模型。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-04T17:57:31.000Z"><a href="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">2016-06-04</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">A Neural Conversational Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>前面介绍过几篇seq2seq在机器翻译、文本蕴藏、自动文摘领域的应用，模型上每篇稍有不同，但基本的思想是接近的。本文继续分享一篇seq2seq在对话生成任务上的应用，是一篇工业界的论文，因此并没有什么理论创新。之所以选这一篇，是因为对话生成是一个非常热门的研究领域和应用领域，也可能是一个非常热门的创业领域，另外一个原因是为了充实seq2seq在各个领域中的应用这一主题。论文题目是<a href="http://cn.arxiv.org/pdf/1506.05869.pdf" target="_blank" rel="external">A Neural Conversational Model</a>，作者是来自Google Brain，毕业于UC Berkeley的<a href="http://www1.icsi.berkeley.edu/~vinyals/" target="_blank" rel="external">Oriol Vinyals</a>博士，论文最早于2015年7月放在arxiv上。</p>
<p>模型部分不用多说，是最简单的seq2seq，架构图如下：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig1.png" width="400" height="400">
<p>本篇主要想分享的东西是结果以及一些思考。文中采用了两个数据集，IT Helpdesk Troubleshooting dataset和OpenSubtitles dataset，前者是一个关于IT类的FAQ数据集，后者是一个电影剧本的数据集。</p>
<p>我们可以看一下训练后的模型生成的对话结果，这里只关注第二个数据集的结果：</p>
<p>常识类问题：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig2.png" width="400" height="400">
<p>哲学类问题：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig3.png" width="400" height="400">
<p>道德类问题：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig4.png" width="400" height="400">
<p>为了对比，作者添加了一组<a href="www.cleverbot.com">cleverbot</a>(cleverbot是一个在线聊天机器人)的对比结果，如下：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig5.png" width="400" height="400">
<p>从对比结果中可以看得出，本文模型生成的结果比网上流行的在线聊天机器人要看起来更加“智能”一些，之前在知乎上回答过一个问题 <a href="https://www.zhihu.com/question/46558198/answer/102722213?group_id=720215813641998336" target="_blank" rel="external">三代聊天机器人在技术上的区别在哪里？</a>，我想cleverbot更接近于第二代，采用了对话检索，即对话是从一个很庞大的数据库中匹配检索来的，而本文的模型属于第三代，更加智能，给定输入生成输出，并不需要借助于人工特征。</p>
<p>但bot这个领域确实还面临一些问题，就像文中作者所说，如何客观地评价生成的效果非常重要，尤其是对于一些没有标准答案的问题来说，根本无法衡量哪个结果更加好。其实不仅仅bot，在自动文摘、机器翻译等各种nlp任务中，评价都是一个很难的问题，自动评价只是从某种意义上解决了各个模型相互比较的一种需求，但在实际应用当中用户的评价更加重要，虽然有时并不是那么客观。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-03T14:06:55.000Z"><a href="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">2016-06-03</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>前面几篇文章分享的都是seq2seq和attention model在机器翻译领域中的应用，在自动文摘系列文章中也分享了六七篇在<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>领域中的应用。本文将分享的这篇文章研究了seq2seq+attention在<a href="https://en.wikipedia.org/wiki/Textual_entailment" target="_blank" rel="external">textual entailment</a>领域的应用。本文题目是<a href="http://arxiv.org/pdf/1509.06664.pdf" target="_blank" rel="external">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</a>，作者是来自英国伦敦大学学院的<a href="http://rockt.github.io/" target="_blank" rel="external">Tim Rocktaschel</a>博士（后面两个作者来自Google Deepmind），文章于2015年9月放在arxiv上，被ICLR 2016录用。</p>
<p>首先，介绍一下文本蕴藏（textual entailment）是一个怎样的任务，简单点说就是用来判断两个文本序列之间的是否存在推断关系，是一个分类问题（具体可参见Wikipedia）。两个文本序列分别称为premise和hypothesis。</p>
<p>本文最大的贡献在于：</p>
<p>1、将end2end的思想应用到了文本蕴藏领域，取得了不错的效果。</p>
<p>2、提出了一种seq2seq模型、两种attention模型和一种trick模型。</p>
<p>本篇关注的重点在于四种模型的构建，来看模型架构图：</p>
<img src="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig1.png" width="600" height="600">
<p>图中A是我们最熟悉的简单seq2seq模型，本文称为conditional encoding模型；B是本文提出的Attention模型，context与之前分享的都不一样；C是我们之前介绍过的attention模型，本文称为word-by-word attention模型。</p>
<p>1、首先介绍A模型。将premise和hypothesis认为是source和target，即用encoder来处理premise，用decoder来处理hypothesis，只不过这里的decoder并不是一个语言模型，而只是一个和encoder一样的LSTM，decoder的输入是encoder的最后一个hidden state，对应图中的c5和h5。最后decoder的输出是一个联合表示premise和hypothesis的向量，用于最终的分类。</p>
<p>2、介绍B模型。该任务和机器翻译不同，并不一定需要做所谓的soft alignment，而是只需要表示好两个句子之间的关系即可，因此这个模型的想法是将hypothesis的句子表示与premise建立注意力机制，而不是将hypothesis的每个单词都与premise做alignment。从上图中标记B的地方也可以看出，attention仅仅依赖于hypothesis的last hidden state。结果可以参看下图：</p>
<img src="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig2.png" width="600" height="600">
<p>从图中可以看出hypothesis与premise中哪些词相关性更强。</p>
<p>3、介绍C模型。这个模型与我们之前一直分享的attention模型一致，模型对hypothesis和premise每个单词做了alignment，所以这里称为word-by-word attention，从模型架构图中也可以看出，hypothesis中的每个词都与premise中对应的词进行了alignment。这里并不是生成单词，而是建立起两个文本序列之间的关系。结果可以参看下图：</p>
<img src="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig3.png" width="600" height="600">
<p>图中表示的是alignment矩阵，更暗的地方表示这两个词更加相关。</p>
<p>4、最后一种模型称为two-way模型，其实是一个trick，借鉴了BiRNN的思想，使用两个相同参数的LSTM，第一个LSTM从一个方向上对基于hypothesis的premise进行表示，而第二个LSTM从相反的方向上对基于premise的hypothesis进行表示，最终得到两个句子表示，拼接起来作为分类的输入。（过程与BiRNN类似，从两个方向上对hypothesis-premise进行了表示，可与前面的模型组合使用，从结果上来看并没有什么明显的作用）</p>
<p>最后的实验结果表明，采用模型C，即word-by-word attention模型效果最好，其次是B模型，最差的是A模型。结果与预期基本符合，但加了two-way的效果并没有更好，反而更差。作者分析说用了相同的参数来做two-way可能会给训练给来更多的噪声影响，所以效果并不好。</p>
<p>整体上来说，seq2seq+attention的组合给很多研究领域带来了春天，给了研究者们更多的启发，attention的形式有多种，可能针对不同的问题，不同的attention会带来不同的效果，也不好说哪一种一定更加适合某一个特定的任务，所以需要去不断地探索。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-02T19:30:30.000Z"><a href="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">2016-06-02</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">Neural Machine Translation by Jointly Learning to Align and Translate #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>前面的两篇文章简单介绍了seq2seq在机器翻译领域的尝试，效果令人满意。上一篇也介绍到这一类问题可以归纳为求解P(output|context)的问题，不同的地方在于context的构建思路不同，上两篇中的seq2seq将context定义为encoder的last hidden state，即认为rnn将整个input部分的信息都保存在了last hidden state中。而事实上，rnn是一个有偏的模型，越靠后的单词在last state中占据的“比例”越高，所以这样的context并不是一个非常好的办法，本文将分享的文章来解决这个问题。题目是<a href="http://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a>，作者是来自德国雅各布大学的<a href="http://minds.jacobs-university.de/dima" target="_blank" rel="external">Dzmitry Bahdanau</a>，现在是Yoshua Bengio组的一个博士生，文章于2015年4月放在arxiv上。</p>
<p>本篇不再讨论seq2seq，如果您想了解seq2seq，可以去看<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>和<a href="http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a>两篇博客。本篇只讨论不同的地方。</p>
<p>本文用encoder所有hidden state的加权平均来表示context，权重表示decoder中各state与encoder各state的相关性，简单的seq2seq认为decoder中每一个state都与input的全部信息（用last state表示）有关，而本文则认为只与相关的state有关系，即在decoder部分中，模型只将注意力放在了相关的部分，对其他部分注意很少，这一点与人类的行为很像，当人看到一段话或者一幅图的时候，往往会将注意力放在一个很小的局部，而不是全部。具体看下图：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig1.png" width="400" height="400">
<p>decoder中预测每个输出的条件概率变为：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig2.png" width="300" height="300">
<p>这里每个time step的state变为：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig3.png" width="300" height="300">
<p>这里，context vector由下式计算：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig4.png" width="300" height="300">
<p>权重用了一个最简单的mlp来计算，</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig6.png" width="300" height="300">
<p>然后做归一化：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig5.png" width="300" height="300">
<p>这里的权重反应了decoder中的state s(i-1)和encoder中的state h(j)之间的相关性。本文在为了得到相对来说无偏的state，在encoder部分采用了BiRNN。</p>
<p>在机器翻译领域中，attention model可以理解为source和target words的soft alignment，像下图一样：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig7.png" width="500" height="500">
<p>上图是英语翻译成法语的一个结果。越亮的地方表示source和target中的words相关性越强（或者说对齐地越准），图中的每一个点的亮度就是前面计算出的权重。</p>
<p>本文最大的贡献在于提出了attention model，为今后研究对话生成，问答系统，自动文摘等任务打下了坚实的基础。context的定义也成为了一个非常有意思的研究点，rnn是一种思路，cnn同样也是一种思路，简单的word embedding也可以算是一种思路，交叉起来rnn+cnn也可以作为一种思路，将word替换成char可以作为一种思路，思路其实非常多，不同的组合有不同的模型，都可以去探索。</p>
<p>另外，不知道是不是Yoshua Bengio组的习惯，本文也在附录附上了详细的模型推导过程。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-01T15:15:17.000Z"><a href="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/">2016-06-01</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将分享的文章相比于昨天那篇<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>更早地使用了seq2seq的框架来解决机器翻译的问题，可能上一篇来自于Google，工程性更强一些，学术性有一些不足。本文来自于学术机构，学术范更浓一些。本文的题目是<a href="http://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder–Decoder for Statistical<br>  Machine Translation</a>，作者是来自蒙特利尔大学的<a href="http://www.kyunghyuncho.me/" target="_blank" rel="external">Kyunghyun Cho</a>博士（现在在纽约大学任教），2014年6月登在arxiv上。</p>
<p>本文最大的两个贡献是：</p>
<p>1、提出了一种类似于LSTM的GRU结构作为RNN的hidden unit，并且具有比LSTM更少的参数，更不容易过拟合。</p>
<p>2、较早地（据说2013年就有人提出用seq2seq思路来解决问题）将seq2seq应用在了机器翻译领域，并且取得了不错的效果。</p>
<p>自然语言生成（NLG）领域中有很多任务，比如机器翻译，<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>，自动问答，对话生成等，都是根据一个上下文来生成一个文本序列，这里分类两个过程，encoder部分将输入序列表示成一个context，decoder部分在context的条件下生成一个输出序列，联合训练两个部分得到最优的模型。这里的context就像是一个memory，试着保存了encoder部分的所有信息（但往往用较低的维度表示整个输入序列一定会造成信息损失）。本文的思路就是如此，具体可参看下图：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig1.png" width="600" height="600">
<p>本文模型将encoder部分的最后一个hidden state作为context输入给decoder，decoder中的每一个时间t的hidden state s(t)都与context,s(t-1),y(t-1)有关系，而每一个时间t的输出y(t)都与context,s(t),y(t-1)有关。当然，这种模型是非常灵活的，你的context可以有很多种选择，比如可以选encoder中所有hidden state组成的矩阵来作为context，可以用BiRNN计算出两个last hidden state进行拼接作为context；而s(t)和y(t)根据RNN结构不同，也可以将context作为s(0)依次向后传递，而不是每次都依赖于context。</p>
<p>说完了模型部分，来说说本文最大的贡献是提出了GRU，一种更轻量级的hidden unit，效果还不输LSTM，函数结构如下图：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig2.png" width="400" height="400">
<p>GRU有两个门函数，reset gate和update gate，公式如下：</p>
<p>reset gate：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig3.png" width="300" height="300">
<p>update gate：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig4.png" width="300" height="300">
<p>reset gate接近于0的时候，当前hidden state会忽略前面的hidden state，在当前输入处reset。reset gate控制了哪些信息可以通过，而update gate控制着多少信息可以通过，与LSTM中的cell扮演着相似的角色。计算出每一步的reset和update gate，即可计算出当前的hidden state，如下：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig5.png" width="300" height="300">
<p>这里，</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig6.png" width="300" height="300">
<p>实验部分，作者利用本文模型得到了满意的结果，不再赘述。</p>
<p>另外，本文在附录部分给出了一个比较详尽的encoder-decoder公式推导过程，大家可以参看原文。</p>
<p>从context预测output，是一件很神奇的事情。而context又是千变万化的，当下正流行的模型attention model正是在context上做了文章，得到了更好的结果。相信，对context的变化和应用会带来更多好玩的模型。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-31T15:22:43.000Z"><a href="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">2016-05-31</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>seq2seq+各种形式的attention近期横扫了nlp的很多任务，本篇将分享的文章是比较早（可能不是最早）提出用seq2seq来解决机器翻译任务的，并且取得了不错的效果。本文的题目是<a href="http://cn.arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a>，作者是来自Google的<a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a>博士（现在OpenAI）。可以说这篇文章较早地探索了seq2seq在nlp任务中的应用，后续的研究者在其基础上进行了更广泛的应用，比如自动文本摘要，对话机器人，问答系统等等。</p>
<p>这里看一张很经典的图，如下：</p>
<img src="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/fig1.png" width="600" height="600">
<p>图的左半边是encoder，右半边是decoder，两边都采用lstm模型，decoder本质上是一个rnn语言模型，不同的是在生成词的时候依赖于encoder的最后一个hidden state，可以用下式来表示：</p>
<img src="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/fig2.png" width="300" height="300">
<p>模型非常简单，就是最普通的多层lstm，实际实现的时候有几点不同：</p>
<ul>
<li><p>用了两种不同的lstm，一种是处理输入序列，一种是处理输出序列。</p>
</li>
<li><p>更深的lstm会比浅的lstm效果更好，所以本文选择了四层。</p>
</li>
<li><p>将输入的序列翻转之后作为输入效果更好一些。</p>
</li>
</ul>
<p>这里在decoder部分中应用了beam search来提升效果，beam search大概的思路是每次生成词是取使得整个概率最高的前k个词作为候选，这里显然beam size越大，效果越好，但是beam size越大会造成计算的代价也增大，所以存在一个trade off。</p>
<p>最后通过机器翻译的数据集验证了了seq2seq模型的有效性。</p>
<p>这里需要讨论的一点是，为什么将输入倒序效果比正序好？文中并没有说，只是说这是一个trick。但后面读了关于attention的文章之后，发现soft attention或者说alignment对于seq2seq这类问题有着很大的提升，我们都知道rnn是一个有偏模型，顺序越靠后的单词在最终占据的信息量越大，那么如果是正序的话，最后一个词对应的state作为decoder的输入来预测第一个词，显然在alignment上来看，这两个词并不是对齐的，反过来，如果用倒序的话，之前的一个词成了最后一个词，在last state中占据了主导，用这个词来预测decoder的第一个词，从某种意义上来说实现了alignment，所以效果会好一些。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-31T05:11:49.000Z"><a href="/2016/05/30/大牛主页/">2016-05-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/30/大牛主页/">大牛主页</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇将汇总PaperWeekly翻译过的作者的主页（持续更新中）：</p>
<p>1、Konstantin Lopyrev  <b><a href="https://github.com/klopyrev" target="_blank" rel="external">Github</a></b></p>
<p>2、<a href="http://people.seas.harvard.edu/~srush/" target="_blank" rel="external">Alexander M. Rush</a> </p>
<p>3、<a href="https://research.facebook.com/sumit-chopra" target="_blank" rel="external">Sumit Chopra</a></p>
<p>4、<a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-nallapati" target="_blank" rel="external">Ramesh Nallapati</a></p>
<p>5、<a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-zhou" target="_blank" rel="external">Bowen Zhou</a></p>
<p>6、<a href="http://nlp.stanford.edu/~jpennin/" target="_blank" rel="external">Jeffrey Pennington</a></p>
<p>7、<a href="https://research.facebook.com/tomas-mikolov" target="_blank" rel="external">Tomas Mikolov</a></p>
<p>8、<a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" target="_blank" rel="external">Yoshua Bengio</a></p>
<p>9、<a href="http://www.people.fas.harvard.edu/~yoonkim/" target="_blank" rel="external">Yoon Kim</a> <b><a href="https://github.com/yoonkim" target="_blank" rel="external">Github</a></b></p>
<p>10、<a href="http://cs.stanford.edu/~quocle/" target="_blank" rel="external">Quoc Le</a></p>
<p>11、<a href="http://www.cs.toronto.edu/~rkiros/" target="_blank" rel="external">Ryan Kiros</a>  <b><a href="https://github.com/ryankiros" target="_blank" rel="external">Github</a></b></p>
<p>12、<a href="http://www.cl.cam.ac.uk/~fh295/" target="_blank" rel="external">Felix Hill</a></p>
<p>13、<a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a></p>
<p>14、[Dzmitry Bahdanau] <b><a href="https://github.com/rizar" target="_blank" rel="external">Github</a></b></p>
<p>15、<a href="http://rockt.github.io/" target="_blank" rel="external">TIM ROCKTÄSCHEL</a> <b><a href="https://github.com/rockt" target="_blank" rel="external">Github</a></b></p>
<p>16、<a href="http://www1.icsi.berkeley.edu/~vinyals/" target="_blank" rel="external">Oriol Vinyals</a> </p>
<p>17、<a href="https://mila.umontreal.ca/en/person/iulian-vlad-serban/" target="_blank" rel="external">Iulian Vlad Serban</a> <b><a href="https://github.com/julianser" target="_blank" rel="external">Github</a></b></p>
<p>18、<a href="http://web.stanford.edu/~jiweil/" target="_blank" rel="external">Jiwei Li</a> <b><a href="https://github.com/jiweil" target="_blank" rel="external">Github</a></b></p>
<p>19、<a href="http://homepages.inf.ed.ac.uk/s0681274/About_Me.html" target="_blank" rel="external">Andrew M. Dai</a></p>
<p>20、<a href="http://www.ccs.neu.edu/home/luwang/" target="_blank" rel="external">Lu Wang</a></p>
<p>21、<a href="http://www.karlmoritz.com/" target="_blank" rel="external">Karl Moritz Hermann</a> <b><a href="https://github.com/karlmoritz" target="_blank" rel="external">Github</a></b></p>
<p>22、<a href="http://cs.stanford.edu/people/danqi/" target="_blank" rel="external">Danqi Chen</a> <b><a href="https://github.com/danqi" target="_blank" rel="external">Github</a></b></p>
<p>23、<a href="http://www.sc.isc.tohoku.ac.jp/~koba/" target="_blank" rel="external">Sosuke Kobayashi</a></p>
<p>24、<a href="http://people.csail.mit.edu/karthikn/" target="_blank" rel="external">Karthik Narasimhan</a> <b><a href="https://github.com/karthikncode" target="_blank" rel="external">Github</a></b></p>
<p>25、<a href="http://tejask.com/" target="_blank" rel="external">Tejas Kulkarni</a> <b><a href="https://github.com/mrkulk" target="_blank" rel="external">Github</a></b></p>
<p>26、<a href="http://www.site.uottawa.ca/~hguo028/mainpage.htm" target="_blank" rel="external">Hongyu Guo</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-30T20:49:58.000Z"><a href="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">2016-05-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">Learning Distributed Representations of Sentences from Unlabelled Data #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>sentence representation的文章已经分享了几篇，包括了supervise和unsupervise的方法，但并没有对各种model进行系统地对比和分析，今天分享的这篇文章对现有各种各样的distributed representations of sentences model进行了分类、对比和分析，为了增强对比效果，还提出了两种虚拟的模型。最后将所有的模型在supervised和unsupervised评价任务中进行对比，得出了一些有意义的结论。本文的题目是：<a href="http://arxiv.org/pdf/1602.03483v1.pdf" target="_blank" rel="external">Learning Distributed Representations of Sentences from Unlabelled Data</a>，作者是来自剑桥大学的<a href="https://www.cl.cam.ac.uk/~fh295/" target="_blank" rel="external">Felix Hill</a>博士。</p>
<p>首先对现有模型进行了分类描述。</p>
<ul>
<li><p>直接在纯文本上进行训练的模型，模型包括：<a href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vector</a>、<a href="http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Paragraph Vector</a>，两种模型都在之前分享过。</p>
</li>
<li><p>在结构化资源上进行训练的模型，这种模型借助了一些纯文本之外的资源进行辅助训练。模型包括：DictRep、CaptionRep、NMT。</p>
</li>
</ul>
<p><code>DictRep</code>是本文作者之前提出的一个模型，模型训练了一个从词典定义到预训练好的词向量之间的映射。</p>
<p><code>CaptionRep</code>模型架构与DictRep一样，采用的数据集不同而已，这里使用了COCO数据集，训练一个从图像vector representation到图像caption的映射。</p>
<p><code>NMT</code>是神经网络机器翻译，该模型架构与skip-thought vector模型相同，但训练数据换成了sentence-aligned翻译文本，WMT语料中的En-Fr和En-De。</p>
<ul>
<li>本文提出的一些新模型。为了解决当前存在模型的问题，本文设计了两种虚拟模型。包括：Sequential (Denoising) Autoencoders(SDAE、SAE)和FastSent。</li>
</ul>
<p><code>SDAE</code>模型是为了解决Skip-Thought Vector模型对语料中句子连贯性的依赖问题。传统的去噪自编码器（DAE）一般都是一个输入是固定尺寸图像数据的前馈神经网络，本文利用一个噪声函数将传统的DAE扩展到变长度句子中，噪声函数是N(S|p0,px)，S表示一个句子，p0,px都是一个[0,1]的数，表示概率。首先，对于每一个S中的word，N函数会以一个p0的概率来删除word，概率是相互对立的。然后，对于S中的每一对不重叠的bigram，w(i)w(i+1)，N函数会以一个px的概率来交换两个词的位置。最后用一个类似NMT的encoder-decoder模型进行训练，只不过不同的是目标函数变了，变成了使得噪声最小。这里，source是经过噪声函数处理过的sentence，target是原始的sentence。这个模型就是SDAE模型，相比于skip-thought vector，可以处理任意顺序的句子集。如果令px=0,p0=0，我们称为<code>SAE</code>模型。这里p0其实就是防止深度网络模型训练时过拟合的正则化方法<code>Dropout</code>。</p>
<p><code>FastSent</code>模型旨在解决Skip-Thought Vector模型计算速度慢的缺点，解决的思路与word2vec突破传统多层神经网络语言模型的思路类似，只用了一个简单的log-linear层。给定一个用词袋模型表示的句子，模型来预测该句子两边相邻的句子。该模型在训练时也会学习句中每个单词的词向量，并且将句子用句中所有词的词向量之和来表示。</p>
<p>下图给出了所有模型在性能上的比较：</p>
<img src="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/fig1.png" width="500" height="500">
<p>其中，OS是指是否需要保留句子在语料中的顺序；R表示需要结构化的训练资源；WO：对词序敏感；SD：句子向量维度；WD：词向量维度；TR：训练时间；TE：编码50w句子需要的时间。</p>
<p>任务评价一共分为两类，监督学习任务和无监督学习任务。通过大量实验的比较，得出了一下的结论：</p>
<ul>
<li><p>不同的任务适合不同的表示模型，这听起来像一句废话，也就是说没有哪种模型可以通吃所有的任务。比如：Skip-Thought Vector模型在TREC任务中最好，是因为句子和句子之间的衔接非常好，非常适合这个模型的特点。而Paraphrase detection任务更加适合于SDAE模型。</p>
</li>
<li><p>监督学习和无监督学习任务的表现存在差异，在监督学习任务中表现好的模型在无监督学习模型中表现的就会很一般，带有非线性网络结构的Skip Thought Vector、SDAE、NMT模型在监督学习中表现更好，而log-linear类的模型FastSent则在无监督学习任务中表现更好。</p>
</li>
<li><p>额外的资源会影响到训练处模型的通用性和实用性，比如一个在线demo需要很快的查询最近邻速度，用fastsent可能就没有问题，但用其他模型就达不到快速的要求。</p>
</li>
<li><p>词序的重要性并没有得到体现。本文的结果给出了一个与常识相左的结论，词序在决定句子意思表示时并没有想象中的那么重要。作者说到，可能是因为当前的评价方式并不能反映出词序的重要性，所以这个问题得不出一个明确的答案。（这点很有意思，在前面分享的一篇文章<a href="http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">How to Generate a Good Word Embedding</a>中，引用了一个结论，词序信息占了语义信息的20%，那么到底词序对于句子语义有多大的影响？需要好好研究一番）</p>
</li>
<li><p>评价指标存在缺陷，并不能绝对准确的对比出各个模型的差异。</p>
</li>
</ul>
<p>最后，展示一个各模型训练之后的应用效果。</p>
<img src="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/fig2.png" width="600" height="600">
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-29T21:09:18.000Z"><a href="/2016/05/29/我以为/">2016-05-29</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/29/我以为/">我以为</a></h1>
  

    </header>
    <div class="entry">
      
        <p>周末了，给自己放一个假，今天不分享paper了，分享一些别的东西。</p>
<p>一直认为人能够坚持并且努力做好一件事情的最大动力是热爱，是那种没有半点虚伪、没有半点功利的热爱。因为热爱，所以纯粹。于是，在经过几个月内心的煎熬和挣扎，我决定上路了，从此不再为那些不感兴趣、耗时耗力但却没有半点成就感的事情而纠结了，心中犹如放下一块巨石，终于可以静下心来做一些让自己内心感到充实的事情。</p>
<p>从小生活在一个小县城里，没有那些IT大牛传说一般的经历，小时候爸爸妈妈也并没有给我买电脑，让我成为一个电脑神童，只是希望我快乐地活在这个世界上。第一次接触电脑是小学毕业时的暑假，拿着妈妈给的零花钱在家附近的网吧打CS，那个时候第一次从电脑中获得了一些肤浅的快乐。对“编程”有一种魔性地冲动，是在初二看了一本盗版的比尔盖茨传，里面讲着盖茨各种传奇的故事，用一台电脑创造了一个帝国，当时对他的种种经历都非常崇拜，还跟着传记里的一些描述来模仿他，困了累了的时候开始前后摇晃自己的身体。那个时候讲盖茨视为自己的偶像，是那种真正的偶像，和现在很多女孩迷小鲜肉是一种感觉。那个时候经常会幻想自己有朝一日可以写代码，可以像偶像一样建立一个帝国。然而，梦想在不正确的时间可能只适合放在心里珍藏起来。</p>
<p>初中毕业之后，考上了省里最好高中之一，来到了省城读书，这个阶段为我打开了一扇很大很宽的门，让我走进了一个更大的世界。在这里接触到了visual basic，Pascal，机器人比赛，感受到了一个更加生动的世界，更加有趣的世界。原来学习可以不只是读书，不只是做题，还可以动手实现一个好玩的机器人，实现一个有趣的程序。三年的高中生活让我看到了一个更好玩的世界。没有辛苦地努力学习，只是因为高三参加的一次奥林匹克竞赛保送到了南方的一所985，因为一些不可控的原因，学习了一个一直想努力感兴趣却一直都没感兴趣的专业，四年除了一些竞赛成绩没有什么其他的亮点。大学的时候，抓住了一切业余时间来多写程序，也跟着刷一些学校自己online judge系统上的题目，但终究不是太系统。在大四下学期的时候，开始疯狂崇拜扎克伯格，尤其是看了《社交网络》之后，大概是因为当时自己的状态很低落，一方面考研成绩并不理想，一方面谈了三年的女朋友选择了更安稳的港湾，整个人的状态一落千丈，非常需要一些提气的东西来鼓励自己走出困境，于是用了一个开源的sns程序，php写的，在大学同学这个圈子里运营了一个社交网络，叫memory。那段时光，我开始自学一些php，每天给网站添加一些小的新功能，来满足同学们的种种好玩的需求，每天忙活到夜里一两点，却感觉到非常充实，非常有成就感。那段时光大概是大学四年里最快乐、最踏实的一段日子了。</p>
<p>后来，因为种种不可抗拒的原因没有在学校继续深造，去了一个自己也没法选择的单位工作。工作第一年在北京长期出差，有机会接触到了真正的IT圈，看到了真实的创业，真刀真枪，也看到了推荐系统从2011年开始在国内火起来，大大小小的网站都在搜罗推荐系统方面的人才，供不应求，关于推荐系统方面的交流活动也是一个接一个，媒体也在热炒，甚至都说推荐系统有望接管了搜索引擎的地位，现在想想真是可笑。而现在，人工智能处于一个非常火热的状态，有许多家专门报道人工智能相关资讯的媒体在社交网络上也非常活跃，还有大量的自媒体，每天也都在分享着各种各样和人工智能有关的信息。仿佛，真正的人工智能就快要实现了一样，尤其是阿尔法狗事件将人工智能推向了一个新的高度，人才市场上又开始吆喝着，严重缺乏人工智能、自然语言处理、深度学习、计算机视觉等方面的人才，媒体每天也在鼓吹着各个方面的论调，有的甚至在说AI威胁论，很多做其他研究的人也随之变成了人工智能专家，深度学习专家，自然语言处理专家。现在，这个世界上最不缺少的是专家，然后就是看热闹不嫌事大的吃瓜群众。网络上充斥各种形态的网红、大V，掌握着充分的话语权，许多不明真相的群众总是特别迷信他们说的话，形成了一种不健康的氛围。屌丝迷信小V，小V迷信大V，大V迷信大大V，一个人的title远比他的内容更能让人信服；如果学术中，总是这样迷信权威或者迷信title，学术该如何进步？长江后浪该如何推前浪？毕业之后的这些年，我看到了一个更加多元化的世界，一个兼容并包的世界，一个充满机会也充满挑战的世界。</p>
<p>我以为，一个人的胸怀和他的视野成正比，一个人的视野和他看到的世界有关系，一个人可以通过多旅行，到处看看来拓宽自己的视野，也可以通过多读书来丰富自己的内心世界。</p>
<p>我以为，人生短暂，不应浪费自己的时间在不感兴趣的事情上，时间宝贵，谁都不应该视别人的时间如粪土，大肆去浪费别人的时间。</p>
<p>我以为，生活的本质应该是生活本身，我们努力干活，努力拼搏，为的不就是生活中每一点一滴都过的很幸福嘛？荣誉也好、成就也罢终究是过眼云烟，敌不过内心持久的充实，充实并不是谁给你的，而是你自己对自己的一个肯定。</p>
<p>我以为，幸福就是不打扰到别人的快乐，可能是陪爱人一起看看大海，可能是一起拍拍照，可能是一起遛遛狗，也可能是一起发发呆。幸福也是将一个又一个的心愿完成，也是拥有一辆华丽的车子，尽管可能只是一辆淘宝购物车。</p>
<p>我以为，每天读一篇paper，写一篇博客，来丰富自己的内心和知识体系也是一件让我快乐和幸福的事情。每天可以有很多好玩的想法，并且可以通过自己的双手来实现这个想法也是一件让我快乐和幸福的事情。</p>
<p>我以为，编程让我感觉到自己像一个造物主，在程序世界里，每一行代码，每一个变量，每一个函数都是一个活生生的人，协同地一起工作着。</p>
<p>我以为，简单和纯粹才是这个世界上最真实的元素，纯真才是对人最大的夸奖，而不是什么帅气，漂亮，有才。</p>
<p>我以为，陪伴才是最长情的告白，感谢我家狗子hare童鞋每一个日日夜夜的陪伴，感谢我的爱人无条件地支持我做我喜欢做的事情，并且真的敢于放弃一些已有的、很好的待遇来跟着我去完成一个梦想。谢谢！</p>
<img src="/2016/05/29/我以为/1.jpg" width="600" height="600">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-28T15:51:24.000Z"><a href="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">2016-05-28</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vectors #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>已经分享过多种无监督的word level表示模型，和多种有监督的sentence level表示模型，以及与word2vec模型类似的paragraph vector模型。无监督模型比有监督模型带给大家更多的惊喜，本文将会分享一篇sentence level的无监督表示模型，模型中用到了当下流行的seq2seq框架。paper的题目是<a href="http://cn.arxiv.org/pdf/1506.06726v1.pdf" target="_blank" rel="external">Skip-Thought Vectors</a>，作者是来自多伦多大学的<a href="http://www.cs.toronto.edu/~rkiros/" target="_blank" rel="external">Ryan Kiros</a>博士。</p>
<p>word level表示已经有太多无监督模型，然而sentence level表示大多仍停留在监督模型的范畴内，比如之前分享过的RNN、CNN、RCNN等模型来表示一个句子，主要是针对具体的分类任务来构造句子向量，仅适用于本任务，不具有一般性。之前，Tomas Mikolov（word2vec的作者）提出了一种类似于Word2vec的paragraph vector，也是一种无监督模型，但并不能很好地扩展来用。</p>
<p>本文旨在提出一个通用的无监督句子表示模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。</p>
<p>当然，这里存在一个很大的问题是，如果测试数据中有未登录词，如何表示这个未登录词？针对这个问题，本文提出了一种词汇表扩展的方法来解决这个问题。</p>
<p>首先，介绍本文的模型，参考下图来理解：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig1.png" width="600" height="600">
<p>模型分为两个部分，一个是encoder，一个是两个decoder，分别decode出当前句子的上一句和下一句。</p>
<p>encoder-decoder框架已经介绍过太多次了，这里不再赘述。本文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hidden state作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。如下式：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig2.png" width="300" height="300">
<p>其次，介绍下本文的另一大亮点，词汇表扩展。</p>
<p>借鉴于Tomas Mikolov的一篇文章<a href="http://arxiv.org/pdf/1309.4168.pdf" target="_blank" rel="external">Exploiting Similarities among Languages for Machine Translation</a>中解决机器翻译missing words问题的思路，对本文训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，本文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。文中给出了一个例子，如下图：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig3.png" width="400" height="400">
<p>当然，词汇表扩展有很多方法，比如不同词，而用字符来作为基本元素，这种思路在语言模型中也常常被用到。</p>
<p>最后，作者在Semantic relateness、Paraphrase detection、Image-sentence ranking和classification任务中进行了测试和对比，验证了本文模型的效果。最后还给出了在多个数据集上对句子聚类的可视化结果，以及用decoder部分生成一段话。</p>
<p>关于未来的改进，作者有几点想法：</p>
<ul>
<li><p>用更深的encoder和decoder网络。</p>
</li>
<li><p>用更大的窗口，而不仅仅预测上一句和下一句。</p>
</li>
<li><p>试着将sentence替换成paragraph。</p>
</li>
<li><p>换一些别的encoder来做，比如用CNN。</p>
</li>
</ul>
<p>每个想法都可能会是未来另一篇牛paper的思路。</p>
<p><code>看过了很多的decoder，有char-level，word-level和sentence-level，我有一个小小的想法是，到底哪种level生成的paragraph更出色呢？速度方面，不必比较了，sentence-level一定要快一些，但是质量方面呢？</code>文中最后给出了一个本文模型生成的demo，如下：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig4.png" width="600" height="600">
<p>本文作者还开源了该模型的实现<a href="https://github.com/ryankiros/skip-thoughts" target="_blank" rel="external">代码</a>。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-27T15:24:12.000Z"><a href="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">2016-05-27</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">Recurrent Convolutional Neural Networks for Text Classification #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>介绍了CNN表示文本的模型之后，本篇将会分享一篇用CNN结合RNN的模型来表示文本。paper题目是<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="external">Recurrent Convolutional Neural Networks for Text Classification</a>，作者是来自中科院大学的来斯惟博士。</p>
<p>本文要解决的问题是文本分类，文本分类最关键的问题是特征表示，传统的方法经常会忽略上下文信息和词序，无法捕捉到词义。近几年随着深度学习的火热，研究者们通过借助神经网络模型来解决传统方法存在的问题。比如：Socher提出的Recursive Neural Network（递归神经网络）模型，通过一种树结构来捕捉句子语义，取得了不错的效果，但时间复杂度是O(n2)，并且无法用一棵树来表示两个句子之间的关系。再比如：Recurrent Neural Network（循环神经网络）模型，时间复杂度是O(n)，每个单词的表示都包含了之前所有单词的信息，有很强的捕捉上下文的能力，但该模型有偏，后面的单词比前面的单词更重要，但这与常识并不相符，因为句中关键的词不一定在最后面。为了解决RNN的有偏性问题，有的研究者提出了用CNN（卷积神经网络）来表示文本，并且时间复杂度也是O(n)，但是CNN存在一个缺陷，卷积窗口的大小是固定的，并且这个窗口大小如何设置是一个问题，如果设置小了，则会损失有效信息，如果设置大了，会增加很多的参数。</p>
<p>于是，针对上述模型存在的问题，本文提出了RCNN（循环卷积神经网络）模型，模型架构图如下：</p>
<img src="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig1.png" width="600" height="600">
<p>首先，构造CNN的卷积层，卷积层的本质是一个BiRNN模型，通过正向和反向循环来构造一个单词的下文和上文，如下式：</p>
<img src="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig2.png" width="300" height="300">
<p>得到单词的上下文表示之后，用拼接的方式来表示这个单词，如下式：</p>
<img src="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig3.png" width="300" height="300">
<p>将该词向量放入一个单层神经网络中，得到所谓的潜语义向量（latent semantic vector），这里卷积层的计算结束了，时间复杂度仍是O(n)。接下来进行池化层（max-pooling），即将刚刚得到的所有单词的潜语义向量中每个维度上最大的值选出组成一个新的向量，这里采用max-pooling可以将向量中最大的特征提取出来，从而获取到整个文本的信息。池化过程时间复杂度也是O(n)，所以整个模型的时间复杂度是O(n)。得到文本特征向量之后，进行分类。</p>
<p>为了验证模型的有效性，在四组包括中文、英文的分类任务中进行了对比实验，取得了满意的结果。</p>
<p>本文灵活地结合RNN和CNN构造了新的模型，利用了两种模型的优点，提升了文本分类的性能。这也提供了一种研究思路，因为每一种model都有其鲜明的优点和无法回避的缺点，如何利用别的model的优点来弥补自身model的缺点，是改进model的一种重要思路。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-26T14:17:24.000Z"><a href="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">2016-05-26</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">How to Generate a Good Word Embedding #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>之前介绍过几种生成word embedding的方法，那么针对具体的任务该如何选择训练数据？如何选择采用哪个模型？如何选择模型参数？本篇将分享一篇paper来回答上述问题，paper的题目是<a href="http://cn.arxiv.org/pdf/1507.05523.pdf" target="_blank" rel="external">How to Generate a Good Word Embedding</a>，作者是来自中科院大学的来斯惟博士。</p>
<p>当前，word embedding的模型有很多，性能几乎都是各说纷纭，每个模型在自己选定的数据集和任务上都取得了state-of-the-art结果，导致学术研究和工程应用上难以做出选择。不仅仅在word embedding这个子方向上存在这样的问题，很多方向都有类似的问题，如何公平客观地评价不同的模型是一个很困难的任务。本文作者试着挑战了一下这个难题，并且给出了一些有意义的结果。</p>
<p>本文所做研究都是一个同一个假设，即：出现在相似上下文的单词具有相似的意思。</p>
<p>下面来看下不同模型的比较，不同word embedding模型之间主要的区别在于两点：</p>
<p>1、目标词和上下文的关系</p>
<p>2、上下文的表示方法</p>
<p>本文提供探讨了6种模型，并从这两个方面对模型进行了对比，如下图：</p>
<img src="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/fig1.png" width="500" height="500">
<p>c表示上下文，w表示目标词。首先看w和c的关系，前五种模型均是用c来预测w，只有C&amp;W模型是给w和c的组合来打分。再看c的表示方法，Order模型是本文为了对比增加的一个虚拟模型，考虑了词序信息，将c中每个单词拼接成一个大向量作为输入，而word2vec的两个模型skip-gram和cbow都是将上下文处理为一个相同维度的向量作为输入，其中skip-gram选择上下文中的一个词作为输入，cbow将上下文的几个词向量作了平均，LBL、NNLM和C&amp;W模型都是在Order模型的基础上加了一层隐藏层，将上下文向量做了一个语义组合。具体见下表：</p>
<img src="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/fig2.png" width="500" height="500">
<p>据研究估计，文本含义信息的20%来自于词序，剩下的来自于词的选择。所以忽略词序信息的模型，将会损失约20%的信息。</p>
<p>本文做了包括三种类型的八组对比实验，分别是：</p>
<ul>
<li><p>研究词向量的语义特性。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy task：semantic和syntactic。</p>
</li>
<li><p>将词向量作为特征。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。</p>
</li>
<li><p>用词向量来初始化神经网络模型。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford Sentiment Treebank；后者用Wall Street Journal数据集进行了POS tagging任务。</p>
</li>
</ul>
<p>经过大量的对比实验，作者回答了以下几个问题：</p>
<p>Q：哪个模型最好？如何选择c和w的关系以及c的表示方法？</p>
<p>A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;W这种将c和w都放在输入层的模型。</p>
<p>Q：数据集的规模和所属领域对词向量的效果有哪些影响？</p>
<p>A：数据集的领域远比规模重要，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。</p>
<p>Q：在训练模型时迭代多少次可以有效地避免过拟合？</p>
<p>A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果，好的做法是用task data作为early stopping的数据。</p>
<p>Q：词向量的维度与效果之间的关系？</p>
<p>A：越大的维度就会有越好的效果，但在一般的任务中50就已经足够了。</p>
<p>本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果，对今后研究和使用词向量的童鞋们搭建了一个非常坚实的平台。并且在github上开源了<a href="https://github.com/licstar/compare" target="_blank" rel="external">实验结果</a>。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-25T19:56:45.000Z"><a href="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">2016-05-25</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">Convolutional Neural Networks for Sentence Classification #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将分享一个有监督学习句子表示的方法，文章是<a href="http://cn.arxiv.org/pdf/1408.5882.pdf" target="_blank" rel="external">Convolutional Neural Networks for Sentence Classification</a>，作者是Harvard NLP组的Yoon Kim，并且开源了代码 <a href="https://github.com/harvardnlp/sent-conv-torch" target="_blank" rel="external">sent-conv-torch</a>。</p>
<p>卷积神经网络（CNN）在计算机视觉中应用广泛，其捕捉局部feature的能力非常强，为分析和利用图像数据的研究者提供了极大额帮助。本文作者将CNN引用到了NLP的文本分类任务中。</p>
<p>本文模型架构图：</p>
<img src="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/fig1.png" width="600" height="600">
<p>熟悉CNN结构的童鞋们看这个图就会非常眼熟，单通道图像可以表示为一个矩阵，输入到CNN中，经过多组filter层和pooling层，得到图像的局部特征，然后进行相关任务。本文用拼接词向量的方法，将一个句子表示成为一个矩阵，这里矩阵的每一行表示一个word，后面的步骤仅采用一组filter、pooling层来得到句子的特征向量，然后进行分类。</p>
<p>这里，模型根据词向量的不同分为四种：</p>
<ul>
<li>CNN-rand，所有的词向量都随机初始化，并且作为模型参数进行训练。</li>
<li>CNN-static，即用word2vec预训练好的向量（Google News），在训练过程中不更新词向量，句中若有单词不在预训练好的词典中，则用随机数来代替。</li>
<li>CNN-non-static，根据不同的分类任务，进行相应的词向量预训练。</li>
<li>CNN-multichannel，两套词向量构造出的句子矩阵作为两个通道，在误差反向传播时，只更新一组词向量，保持另外一组不变。</li>
</ul>
<p>在七组数据集上进行了对比实验，证明了单层的CNN在文本分类任务中的有效性，同时也说明了用无监督学习来的词向量对于很多nlp任务都非常有意义。</p>
<p>这里需要注意的一点是，static模型中word2vec预训练出的词向量会把good和bad当做相似的词，在sentiment classification任务中将会导致错误的结果，而non-static模型因为用了当前task dataset作为训练数据，不会存在这样的问题。具体可参看下图：</p>
<img src="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/fig2.png" width="600" height="600">
<p>CNN最初应用在图像领域，将文本进行一些处理之后，也可以应用在nlp中，同样的思路，attention mechanism最初也是应用在图像识别领域中，现在seq2seq+attention的模型横扫了很多nlp task。其实很多问题在某个维度上看，是相似的问题，是可以用类似的方法进行解决的。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-24T19:34:00.000Z"><a href="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">2016-05-24</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Distributed Representations of Sentences and Documents #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>继分享了一系列词向量相关的paper之后，今天分享一篇句子向量的文章，<a href="http://cn.arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="external">Distributed Representations of Sentences and Documents</a>，作者是来自Google的Quoc Le和Tomas Mikolov，后者也是Word2Vec的作者。</p>
<p>用低维向量表示了word之后，接下来要挑战地就是表示句子和段落了。传统的表示句子的方式是用词袋模型，每个句子都可以写成一个特别大维度的向量，绝大多数是0，不仅没有考虑词序的影响，而且还无法表达语义信息。本文沿用了Word2Vec的思想，提出了一种无监督模型，将变长的句子或段落表示成固定长度的向量。不仅在一定上下文范围内考虑了词序，而且非常好地表征了语义信息。</p>
<p>首先简单回顾下word2vec的cbow模型架构图：</p>
<img src="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/fig1.png" width="600" height="600">
<p>给定上下文the cat sat三个词来预测单词on。</p>
<p>与cbow模型类似，本文提出了PV-DM（Distributed Memory Model of Paragraph Vectors），如下图：</p>
<img src="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/fig2.png" width="600" height="600">
<p>不同的地方在于，输入中多了一个paragraph vector，可以看做是一个word vector，作用是用来记忆当前上下文所缺失的信息，或者说表征了该段落的主题。这里，所有的词向量在所有段落中都是共用的，而paragraph vector只在当前paragraph中做训练时才相同。后面的过程与word2vec无异。</p>
<p>topic也好，memory也罢，感觉更像是一种刻意的说辞，本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。</p>
<p>另外一种模型，叫做PV-DBOW（Distributed Bag of Words version of Paragraph Vector），如下图：</p>
<img src="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/fig3.png" width="600" height="600">
<p>看起来和word2vec的skip-gram模型很像。</p>
<p>用PV-DM训练出的向量有不错的效果，但在实验中采用了两种模型分别计算出的向量组合作为最终的paragraph vector，效果会更佳。在一些情感分类的问题上进行了测试，得到了不错的效果。</p>
<p>本文的意义在于提出了一个无监督的paragraph向量表示模型，无监督的意义非常重大。有了paragraph级别的高效表示模型之后，解决类似于句子分类，检索，问答系统，文本摘要等各种问题都会带来极大地帮助。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-23T14:10:32.000Z"><a href="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">2016-05-23</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">Character-Aware Neural Language Models #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇分享的文章是<a href="http://cn.arxiv.org/pdf/1508.06615.pdf" target="_blank" rel="external">Character-Aware Neural Language Models</a>，作者是Yoon Kim、Alexander M. Rush。两位是HarvardNLP组的学生和老师，前者贡献了一些有意义的torch代码，比如<a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">seq2seq+attn</a>，后者第一次将seq2seq的模型应用到了文本摘要。</p>
<p>卷积神经网络之前常常用在计算机视觉领域，用来在图像中寻找features，前几年被研究者应用到了nlp任务中，在文本分类等任务中取得了不错的效果。传统的word embedding对低频词并没有太好的效果，而本文将char embedding作为CNN的输入，用CNN的输出经过一层highway层处理表示word embedding，然后作为RNNLM的输入，避免了这个问题。而且之前的神经网络语言模型中绝大多数需要优化的参数是word embedding，而本文的模型则会将优化参数减少非常多。</p>
<p>本文模型的架构图如下：</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/arch.png" width="600" height="600">
<p>可以分为三层，一层是charCNN，通过构建一个char embedding矩阵，将word表示成matrix，和图像类似，输入到CNN模型中提取经过filter层和max pooling层得到一个输出表示，然后将该输出放到Highway Network中，得到一个处理后的效果更好的word embedding作为输出，在第三层中是一个典型的RNN模型，后面的处理与传统方法一样了。</p>
<p>这里需要学习的参数中char embedding规模非常小，相对比之前的模型有非常明显的优势。这里需要说明的一点是HighWay Network，在Rupesh Kumar Srivastava的paper <a href="http://cn.arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="external">Training Very Deep Networks</a>被提出，受lstm解决rnn梯度衰减问题的思路启发，用来解决训练very deep networks，因为模型越深效果越好，但越难训练。本文的HighWay层如下：</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/fig1.png" width="400" height="400">
<p>其中</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/fig2.png" width="200" height="200">
<p>t被称为transform gate，1-t被称为carry gate。</p>
<p>最终的实验证明，使用HighWay层效果比使用普通的MLP或者不使用该层效果更好。</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/result.png" width="500" height="500">
<p>本文通过将传统的word embedding降级到char level，避免了大规模的embedding计算和低频词的问题，通过Highway network技术构建更深的网络，得到了不错的结果。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/" class="alignleft prev">上一页</a>
  
  
    <a href="/page/3/" class="alignright next">下一页</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Autoencoder/">Autoencoder</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/DQN/">DQN</a><small>4</small></li>
  
    <li><a href="/tags/Memory-Network/">Memory Network</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>88</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/RNNLM/">RNNLM</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/Reading-Comprehension/">Reading Comprehension</a><small>6</small></li>
  
    <li><a href="/tags/Representation/">Representation</a><small>1</small></li>
  
    <li><a href="/tags/Text-Comprehension/">Text Comprehension</a><small>1</small></li>
  
    <li><a href="/tags/api-ai/">api.ai</a><small>1</small></li>
  
    <li><a href="/tags/arXiv/">arXiv</a><small>2</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/attention/">attention</a><small>3</small></li>
  
    <li><a href="/tags/bot/">bot</a><small>21</small></li>
  
    <li><a href="/tags/chatbot/">chatbot</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>106</small></li>
  
    <li><a href="/tags/open-source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/paperweekly/">paperweekly</a><small>2</small></li>
  
    <li><a href="/tags/reading-comprehension/">reading comprehension</a><small>1</small></li>
  
    <li><a href="/tags/reinforcement-learning/">reinforcement learning</a><small>1</small></li>
  
    <li><a href="/tags/sentence-representations/">sentence representations</a><small>1</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>17</small></li>
  
    <li><a href="/tags/text-comprehension/">text comprehension</a><small>1</small></li>
  
    <li><a href="/tags/torch/">torch</a><small>1</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>2</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/创业/">创业</a><small>1</small></li>
  
    <li><a href="/tags/招聘/">招聘</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/综述/">综述</a><small>1</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>16</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>4</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 PaperWeekly
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>