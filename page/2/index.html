<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  
  <title>page</title>
  
  

  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="PaperWeekly"/>
 <meta property="og:image" content="undefined"/>
  
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="main">
    <div class="behind">
      <div class="back">
        <a href="/" class="black-color"><i class="fa fa-times" aria-hidden="true"></i></a>
      </div>
      <div class="description">
        &nbsp;
      </div>
    </div>
    <div class="container">
      <nav class="navigator"><nav class="navbar navbar-default navbar-fixed-top">
  <div class="nav-container">


  
  <div id="site_search" class="">
  <input type="text" id="local-search-input" name="q" placeholder="search" class="form-control"/>
</div>
<div id="local-search-result"></div>


  <ul class="nav navbar-nav navbar-right">
    <li class="dropdown">
      <a href="javascript:void(0);" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
      <i class="fa fa-link fa-lg"></i>
      </a>
      <ul class="dropdown-content">
        
          <li>
            <a href="">
        友情链接
      </a>
    </li>
     
          <li>
            <a href="https://github.com/XXX/">
        Github
      </a>
    </li>
     
          <li>
            <a href="http://weibo.com/XXX">
        微博
      </a>
    </li>
     
          <li>
            <a href="https://www.zhihu.com/people/XXX">
        知乎
      </a>
    </li>
     
          <li>
            <a href="/atom.xml">
        rss
      </a>
    </li>
     
          <li>
            <a href="/about">
        关于我
      </a>
    </li>
     
    </ul>
    </li>
    <li class="archives"><a href="/archives">
      <i class="fa fa-archive fa-lg"></i>
    </a></li>
  </ul>
  </div>
</nav></nav>
<ul class="posts">
  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/">
            THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    本文是机器阅读理解系列的第五篇文章，将会分享的题目是THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS，作者是来自剑
  
  <a href="/2016/06/16/THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-16T14:38:23.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-16
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/Reading-Comprehension/">Reading Comprehension</a>·<a href="/tags/Memory-Network/">Memory Network</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/14/Text-Understanding-with-the-Attention-Sum-Reader-Network-PaperWeekly/">
            Text Understanding with the Attention Sum Reader Network #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    本文是机器阅读系列的第四篇文章，本文的模型常出现在最新的机器阅读paper中related works部分，也是很多更好的模型的基础模型，所以很有必要来看下这篇paper，看得远往往不是因为长得高，而是因为站得高。本文的题目是Text Un
  
  <a href="/2016/06/14/Text-Understanding-with-the-Attention-Sum-Reader-Network-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-15T04:31:36.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-14
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/Reading-Comprehension/">Reading Comprehension</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/14/A-Thorough-Examination-of-the-CNN-Daily-Mail-Reading-Comprehension-Task-PaperWeekly/">
            A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    本篇是reading comprehension系列的第三篇，文章于2016年6月9号submit在arxiv上，比之前介绍的Gated-Attention Readers for Text Comprehension更晚地出现，但尴尬的是
  
  <a href="/2016/06/14/A-Thorough-Examination-of-the-CNN-Daily-Mail-Reading-Comprehension-Task-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-14T15:36:02.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-14
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/Reading-Comprehension/">Reading Comprehension</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">
            Teaching Machines to Read and Comprehend #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    昨天的文章text comprehension系列的第一篇，是最近刚刚submit的文章，今天分享一篇去年的文章，也是一篇非常经典的文章。我记得Yoshua Bengio在Quora Session一个问题中推荐这篇文章。本文的题目是Tea
  
  <a href="/2016/06/13/Teaching-Machines-to-Read-and-Comprehend-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-13T17:57:22.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-13
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/text-comprehension/">text comprehension</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/">
            当我们在谈论deep learning的时候，我们在谈论什么？
        </a>
    </h1>
  


  <div class="excerpt">
  
    标题起的有一点装x了，昨天看到微博上刘知远老师对关于deep learning哪年火的问题的讨论，突然有一些自己的感触就写了下来。
从接触nlp到现在大概过去了4个月的时间，最初的动机是要用word2vec工具包来给自己写的app(rsar
  
  <a href="/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-12T17:59:52.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-12
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/deep-learning/">deep learning</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">
            Gated-Attention Readers for Text Comprehension #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    完形填空一直是各大英语考试的常见题型，读一篇短文，填20个空。那么如果是机器来做完形填空，该如何来定义问题，提出模型呢？本周开始将会介绍一系列文本理解的模型。本文分享的题目是Gated-Attention Readers for Text 
  
  <a href="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-12T16:33:10.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-12
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/Text-Comprehension/">Text Comprehension</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/11/PaperWeekly文章分类导航/">
            PaperWeekly文章分类导航
        </a>
    </h1>
  


  <div class="excerpt">
  
    Neural Language Model[1] A Neural Probabilistic Language Model
[2] Character-Aware Neural Language Models
[3] Gated Word
  
  <a href="/2016/06/11/PaperWeekly文章分类导航/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-11T23:48:38.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-11
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/PaperWeekly/">PaperWeekly</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">
            Neural Network-Based Abstract Generation for Opinions and Arguments #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    本篇将会分享的是一篇工程性比较强的paper，如果您想做一个实实在在的意见摘要系统（比如：淘宝商品评论摘要、电影评论摘要）的话，可以仔细研读下本文的解决方案。本文的题目是Neural Network-Based Abstract Gener
  
  <a href="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-10T19:34:27.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-10
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">
            A Joint Model for Word Embedding and Word Morphology #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    大家端午节快乐！本文将分享一篇关于词向量模型最新研究的文章，文章于6月8号提交到arxiv上，题目是A Joint Model for Word Embedding and Word Morphology，作者是来自剑桥大学的博士生Kris
  
  <a href="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-09T21:34:03.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-09
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/word-embedding/">word embedding</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">
            Gated Word-Character Recurrent Language Model #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    本篇将会分享一篇最新的paper，2016年6月6日submit到arxiv上，paper的题目是Gated Word-Character Recurrent Language Model，作者是来自纽约大学的硕士生Yasumasa Miy
  
  <a href="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-08T16:00:50.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-08
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/RNNLM/">RNNLM</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">
            Semi-supervised Sequence Learning #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
之前分享过几篇有监督的sentence表示方法，比如Recurrent Convolutional Neural Networks for Text Classification、Convoluti
  
  <a href="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-07T14:11:34.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-07
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/seq2seq/">seq2seq</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">
            A Hierarchical Neural Autoencoder for Paragraphs and Documents #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
本篇将会分享一篇用自动编码器(AutoEncoder)来做文档表示的文章，本文的结果会给自然语言生成、自动文摘等任务提供更多的帮助。本文作者是来自斯坦福大学的博士生Jiwei Li，简单看了下其简历
  
  <a href="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-06T14:09:52.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-06
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/Autoencoder/">Autoencoder</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">
            Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
昨天介绍了一篇工程性比较强的paper，关于对话生成（bot）任务的，今天继续分享一篇bot方面的paper，6月2日刚刚submit在arxiv上。昨天的文章用了一种最最简单的端到端模型来生成对话
  
  <a href="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-05T17:57:48.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-05
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/bot/">bot</a>·<a href="/tags/seq2seq/">seq2seq</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">
            A Neural Conversational Model #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
前面介绍过几篇seq2seq在机器翻译、文本蕴藏、自动文摘领域的应用，模型上每篇稍有不同，但基本的思想是接近的。本文继续分享一篇seq2seq在对话生成任务上的应用，是一篇工业界的论文，因此并没有什
  
  <a href="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-04T17:57:31.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-04
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/bot/">bot</a>·<a href="/tags/seq2seq/">seq2seq</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">
            REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
前面几篇文章分享的都是seq2seq和attention model在机器翻译领域中的应用，在自动文摘系列文章中也分享了六七篇在自动文摘领域中的应用。本文将分享的这篇文章研究了seq2seq+att
  
  <a href="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-03T14:06:55.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-03
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/attention/">attention</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">
            Neural Machine Translation by Jointly Learning to Align and Translate #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
前面的两篇文章简单介绍了seq2seq在机器翻译领域的尝试，效果令人满意。上一篇也介绍到这一类问题可以归纳为求解P(output|context)的问题，不同的地方在于context的构建思路不同，
  
  <a href="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-02T19:30:30.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-02
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/attention/">attention</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/">
            Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
本篇将分享的文章相比于昨天那篇Sequence to Sequence Learning with Neural Networks更早地使用了seq2seq的框架来解决机器翻译的问题，可能上一篇来自
  
  <a href="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-06-01T15:15:17.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  06-01
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/seq2seq/">seq2seq</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">
            Sequence to Sequence Learning with Neural Networks #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
seq2seq+各种形式的attention近期横扫了nlp的很多任务，本篇将分享的文章是比较早（可能不是最早）提出用seq2seq来解决机器翻译任务的，并且取得了不错的效果。本文的题目是Seque
  
  <a href="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-31T15:22:43.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-31
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/seq2seq/">seq2seq</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/30/大牛主页/">
            大牛主页
        </a>
    </h1>
  


  <div class="excerpt">
  
    本篇将汇总PaperWeekly翻译过的作者的主页（持续更新中）：
1、Konstantin Lopyrev  Github
2、Alexander M. Rush 
3、Sumit Chopra
4、Ramesh Nallapati
5、
  
  <a href="/2016/05/30/大牛主页/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-31T05:11:49.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-30
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">
            Learning Distributed Representations of Sentences from Unlabelled Data #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
sentence representation的文章已经分享了几篇，包括了supervise和unsupervise的方法，但并没有对各种model进行系统地对比和分析，今天分享的这篇文章对现有各种
  
  <a href="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-30T20:49:58.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-30
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/29/我以为/">
            我以为
        </a>
    </h1>
  


  <div class="excerpt">
  
    周末了，给自己放一个假，今天不分享paper了，分享一些别的东西。
一直认为人能够坚持并且努力做好一件事情的最大动力是热爱，是那种没有半点虚伪、没有半点功利的热爱。因为热爱，所以纯粹。于是，在经过几个月内心的煎熬和挣扎，我决定上路了，从此不
  
  <a href="/2016/05/29/我以为/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-29T21:09:18.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-29
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/随笔/">随笔</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">
            Skip-Thought Vectors #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
已经分享过多种无监督的word level表示模型，和多种有监督的sentence level表示模型，以及与word2vec模型类似的paragraph vector模型。无监督模型比有监督模型带
  
  <a href="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-28T15:51:24.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-28
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">
            Recurrent Convolutional Neural Networks for Text Classification #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
介绍了CNN表示文本的模型之后，本篇将会分享一篇用CNN结合RNN的模型来表示文本。paper题目是Recurrent Convolutional Neural Networks for Text 
  
  <a href="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-27T15:24:12.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-27
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/CNN/">CNN</a>·<a href="/tags/RNN/">RNN</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">
            How to Generate a Good Word Embedding #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
之前介绍过几种生成word embedding的方法，那么针对具体的任务该如何选择训练数据？如何选择采用哪个模型？如何选择模型参数？本篇将分享一篇paper来回答上述问题，paper的题目是How 
  
  <a href="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-26T14:17:24.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-26
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/word-embeddings/">word embeddings</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">
            Convolutional Neural Networks for Sentence Classification #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
本篇将分享一个有监督学习句子表示的方法，文章是Convolutional Neural Networks for Sentence Classification，作者是Harvard NLP组的Yo
  
  <a href="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-25T19:56:45.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-25
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/CNN/">CNN</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">
            Distributed Representations of Sentences and Documents #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
继分享了一系列词向量相关的paper之后，今天分享一篇句子向量的文章，Distributed Representations of Sentences and Documents，作者是来自Goog
  
  <a href="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-24T19:34:00.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-24
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">
            Character-Aware Neural Language Models #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
本篇分享的文章是Character-Aware Neural Language Models，作者是Yoon Kim、Alexander M. Rush。两位是HarvardNLP组的学生和老师，前
  
  <a href="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-23T14:10:32.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-23
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/language-model/">language model</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/">
            GloVe: Global Vectors for Word Representation #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
Word2Vec虽然取得了很好的效果，但模型上仍然存在明显的缺陷，比如没有考虑词序，再比如没有考虑全局的统计信息。本篇分享的是GloVe: Global Vectors for Word Repre
  
  <a href="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-22T16:23:52.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-22
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/word-embedding/">word embedding</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/">
            Efficient Estimation of Word Representations in Vector Space #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)
词向量是语言模型的一个副产品，但这个副产品在2013年随着一个叫做word2vec的工具包而火了起来，大家在各种场合中都在用，并且取得了不错的效果。
本篇分享的文章是Efficient Estima
  
  <a href="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-21T21:43:54.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-21
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/word2vec/">word2vec</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">
            A Neural Probabilistic Language Model #PaperWeekly#
        </a>
    </h1>
  


  <div class="excerpt">
  
    (欢迎大家订阅本博客，订阅地址是RSS)

 站得高，望得远 

今天分享一篇年代久远但却意义重大的paper，A Neural Probabilistic Language Model。作者是来自蒙特利尔大学的Yoshua Bengio教
  
  <a href="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-21T04:37:06.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-20
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/deeplearning/">deeplearning</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/18/自动文摘（十三）/">
            自动文摘（十三）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
 天下武功，唯快不破 

今天分享的paper是Incorporating Copying Mechanism in Sequence-to-Sequence Learning，作者来自香港大学和华为诺亚方舟实验室。
本文的模型通过借鉴
  
  <a href="/2016/05/18/自动文摘（十三）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-19T00:56:53.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-18
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/paper/">paper</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/17/自动文摘（十二）/">
            自动文摘（十二）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
 心是自由的，世界就是自由的 

今天分享的paper是Neural Headline Generation with Minimum Risk Training。
本文通过将评价指标融入目标函数来训练模型，在中文和英文数据集上均取得了
  
  <a href="/2016/05/17/自动文摘（十二）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-17T23:24:26.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-17
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/13/Paper翻译列表/">
            Paper翻译列表
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
 读万卷书，行万里路 

最近精读了多篇自动文摘方面的paper，并且写成了博客，感觉受益良多。接下来会读更多的好paper，并且以一种更加精炼的形式摘译和评价。如果你有兴趣可以扫码关注微信账号：

Accomplised
Genera
  
  <a href="/2016/05/13/Paper翻译列表/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-14T05:12:01.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-13
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/paper/">paper</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/12/自动文摘（十一）/">
            自动文摘（十一）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
 除了诗和远方，还有眼前的评价方法 

之前的博客中提到过影响一个技术发展的重要因素有几个，其中一个就是评价方法。因为评价方法是牵引，是参考，是衡量各个模型孰优孰劣的尺子。评价指标是否科学可行直接影响着这个领域能否进入一个正常的研究方向
  
  <a href="/2016/05/12/自动文摘（十一）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-12T22:38:09.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-12
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/ROUGE/">ROUGE</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/12/自动文摘（十）/">
            自动文摘（十）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
 生活不只是眼前的苟且，还有paper和远方 

不知不觉坚持写自动文摘系列的博客已经50天了，本篇是系列的第十篇。其实说是系列文章并不准确，只是每篇博客与自动文摘有关系，但相互之间并没有递进的关系，只是get到了一些点顺手写下来，又懒
  
  <a href="/2016/05/12/自动文摘（十）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-12T17:20:59.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-12
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/11/自动文摘（九）/">
            自动文摘（九）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
坚持下去就是胜利。

今天分享一篇关于构造自动文摘数据集的paper，数据集的质量、内容和规模都是直接影响deep learning效果的最直接因素，作用非常重要。题目是LCSTS: A Large Scale Chinese Shor
  
  <a href="/2016/05/11/自动文摘（九）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-11T14:14:52.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-11
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/paper/">paper</a>·<a href="/tags/dataset/">dataset</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/10/自动文摘（八）/">
            自动文摘（八）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引最近读的几篇paper都是用seq2seq的思路来解决问题，有点读厌烦了，今天换个口味。分享一篇extractive式的paper，AttSum: Joint Learning of Focusing and Summarization 
  
  <a href="/2016/05/10/自动文摘（八）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-10T23:50:08.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-10
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/paper/">paper</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/05/07/自动文摘（七）/">
            自动文摘（七）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
再坚持一下，就会等到黎明破晓，重见光明。

今天继续分享一篇sentence level abstractive summarization相关的paper，出自于IBM Watson，Abstractive Text Summariz
  
  <a href="/2016/05/07/自动文摘（七）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-05-07T19:02:15.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  05-07
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/paper/">paper</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/04/30/自动文摘（六）/">
            自动文摘（六）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
万事开头难，其实之后的事情可能会更难，但开好了头，就会有充足的信心来面对后面的困难。

记得Andrew Ng在一个采访中曾经说过：“当我和研究人员，或是想创业的人交谈时，我告诉他们如果你不断地阅读论文，每周认真研究六篇论文，坚持两年。
  
  <a href="/2016/04/30/自动文摘（六）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-04-30T23:30:43.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  04-30
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/paper/">paper</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/04/24/自动文摘（五）/">
            自动文摘（五）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
读万卷书 行万里路

最近读了几篇关于deep learning在summarization领域应用的paper，主要的方法是借鉴机器翻译中seq2seq的技术，然后加上attention model提升效果。今天来分享其中一篇pape
  
  <a href="/2016/04/24/自动文摘（五）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-04-24T18:14:12.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  04-24
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/seq2seq/">seq2seq</a>·<a href="/tags/自动文摘/">自动文摘</a>·<a href="/tags/paper/">paper</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/04/17/自动文摘（四）/">
            自动文摘（四）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引这篇博客是自动文摘系列的第四篇，重点介绍近期abstractive summarization的一些研究情况。abstractive是学术界研究的热点，尤其是Machine Translation中的encoder-decoder框架和a
  
  <a href="/2016/04/17/自动文摘（四）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-04-18T02:06:18.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  04-17
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/04/06/自动文摘（三）/">
            自动文摘（三）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引
蜀道之难 难于上青天

虽然有很多SaaS提供Summarization的服务，虽然有很多App尤其是新闻类App标榜自己拥有多么牛的技术做Summarization，我们还是不得不承认自动文摘的技术离一个高水平的AI还有一段距离，很长
  
  <a href="/2016/04/06/自动文摘（三）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-04-06T18:31:55.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  04-06
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/03/30/自动文摘（二）/">
            自动文摘（二）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引自动文摘的方法主要分为两大类，extractive和abstractive。前者是目前最主流、应用最多、最容易的方法，后者相对来说更有一种真正人工智能的味道。还有另外一种分类方法是，单文档摘要和多文档摘要，前者是后者的基础，但后者不只是前
  
  <a href="/2016/03/30/自动文摘（二）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-03-31T02:39:58.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  03-30
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/03/26/RSarXiv-Web版/">
            RSarXiv Web版
        </a>
    </h1>
  


  <div class="excerpt">
  
    一直感觉app很难承载太多的功能，所以在app后台基础上，开发了这个web版，包括以下几个功能：
检索检索一直是查论文最常用的方式，web版提供了检索功能，查询结果只有一些简要信息，详细的信息需要进入detail页面进行查看。
反馈如果用户
  
  <a href="/2016/03/26/RSarXiv-Web版/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-03-27T00:20:02.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  03-26
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/RSarXiv/">RSarXiv</a>·<a href="/tags/推荐系统/">推荐系统</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/03/20/自动文摘（一）/">
            自动文摘（一）
        </a>
    </h1>
  


  <div class="excerpt">
  
    引最近人工智能随着AlphaGo战胜李世乭这一事件的高关注度，重新掀起了一波新的关注高潮，有的说人工智能将会如何超越人类，有的说将会威胁到人类的生存和发展，种种声音都在表明人工智能的又一个春天即将到来，但很多学者认为媒体的过度炒作，会引发民
  
  <a href="/2016/03/20/自动文摘（一）/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-03-20T14:44:46.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  03-20
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/03/06/RSarXiv前世今生/">
            RSarXiv前世今生
        </a>
    </h1>
  


  <div class="excerpt">
  
    RSarXiv是什么？RSarXiv是我的一个side project，甚至有一段时间几乎做成了main project，从开始利用课余时间做设计，写代码到后面的每天全天时间都在写代码。
RSarXiv是一个让人用起来很愉快的论文推荐App
  
  <a href="/2016/03/06/RSarXiv前世今生/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-03-06T23:40:40.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  03-06
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/arxiv/">arxiv</a>


  </div>
</li>
</div>
<hr>




  
    
<div>
<li class="post-item">
  
  
    <h1 class="index-title">
        <a href="/2016/02/27/intro/">
            RSarXiv
        </a>
    </h1>
  


  <div class="excerpt">
  
    功能介绍初始化用户第一次使用的时候需要进行初始化，初始化的过程十分简单，向左滑动或者向右滑动卡片，卡片上标记有三类标签，（1）某一个子研究领域，比如cs.CL表示计算语言学；（2）某一位作者，比如Yoshua Bengio；（3）某一个关键
  
  <a href="/2016/02/27/intro/">
    更多
  </a>
  </div>
  <div class="index-meta">
    
<time datetime="2016-02-27T20:27:40.000Z" itemprop="datePublished">
  <i class="fa fa-calendar"></i>&nbsp;
  02-27
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/推荐系统/">推荐系统</a>·<a href="/tags/arxiv/">arxiv</a>


  </div>
</li>
</div>
<hr>




  
</ul>
<ul class="pager">
    <a class="extend prev" rel="prev" href="/">Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
</ul>


    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot container">
    <div class="firstrow">
        <a href="#top" >
        <i class="fa fa-arrow-right"></i>
        </a>
        © XXX 2015-2016
    </div>
    <div class="secondrow">
        <a href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.js"></script>
<script type="text/javascript">

// comments below to disable loading animation
function revealOnScroll() {
  var scrolled = $(window).scrollTop();
  $(".excerpt, .index-title, .index-meta, p").each(function() {
    var current = $(this),
      height = $(window).outerHeight(),
      offsetTop = current.offset().top;
    (scrolled + height + 50 > offsetTop) ? current.addClass("animation"):'';
  });
}
$(window).on("scroll", revealOnScroll);
$(document).ready(revealOnScroll)

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

// back to top scripts
$("a[href='#top']").click(function() {
  $("html, body").animate({ scrollTop: 0 }, 500);
  return false;
});


var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
