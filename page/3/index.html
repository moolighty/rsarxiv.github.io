<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>第 3 页 | PaperWeekly</title>
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="PaperWeekly"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">PaperWeekly</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
      <li><a href="/about/index.html">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-24T18:14:12.000Z"><a href="/2016/04/24/自动文摘（五）/">2016-04-24</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/24/自动文摘（五）/">自动文摘（五）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>读万卷书 行万里路</strong></p>
</blockquote>
<p>最近读了几篇关于deep learning在summarization领域应用的paper，主要的方法是借鉴机器翻译中seq2seq的技术，然后加上attention model提升效果。今天来分享其中一篇paper，<b>Generating News Headlines with Recurrent Neural Networks</b></p>
<p><code>本篇文章是近期所读文章中最简单的一篇，没有太精彩的理论和创新，是一个工程性很强的paper，将实现过程中的思路和一些参数交代的很清楚，对于复现此paper提供了很大的帮助。</code></p>
<p><code>深度学习是一门研究表示学习的技术，用一张巨大的网来表征给入的数据，使得模型不依赖于领域的特征，是一种full data driven的模型，听起来像是一种银弹，尤其是近几年的在各大领域的都收获了state-of-the-art的结果，但模型的参数调优不没有太多的理论依据，之前的神经网络规模小调参数时间代价会小一些，但deep learning动不动就需要几天甚至几周的训练时间，调参数代价太大；中间层的表示如何解释，也是一个十分头疼的事情，对于cv领域来说还好，总可以将matrix显示成一幅图片来看效果，比较直观，但对于nlp领域，hidden state到底是什么，表示哪个词？表示哪种关系？词向量的每一个维度代表什么？具体真说不清楚，只有在输出的那一层才能看到真正的意义。</code></p>
<p><code>一个领域的发展需要很多种不同思路的试错，应该是一种百家争鸣的态势，而不是大家一股脑地都用一种技术，一种思路来解决问题，理论模型都趋于大同，这样对这个领域的发展不会有太积极的意义。</code></p>
<p><code>machine translation是最活跃的一个研究领域，seq2seq框架就是从该领域中提炼出来的，attention model也是借鉴于soft alignment，对于文本摘要这个问题来说，套用seq2seq只能解决headlines generation的问题，面对传统的single document summarization和multi document summarization任务便束手无策了，因为输入部分的规模远大于输出部分的话，seq2seq的效果不会很好，因此说abstractive summarization的研究还长路漫漫。不过这里可以将extractive和abstractive结合在一起来做，用extractive将一篇文档中最重要的一句话提取出来作为输入，套用seq2seq来做abstractive，本质上是一个paraphrase的任务，在工程中可以试一下这种思路。在后续的研究中也可以尝试将extractive和abstractive的思路结合在一起做文本摘要。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文的思路是用LSTM RNN作为encoder-decoder框架的模型，并且使用了attention模型来生成新闻文章的标题，效果很好。并且提出了一种简化版的attention mechanism，相比于复杂版的注意力机制在解决headline generation问题上有更好的效果。</p>
<p><code>本文定义的文本摘要问题是给新闻文章命题，为了套用seq2seq技术，一般都会将source定义为新闻的第一句话，target定义为标题。本文的亮点在于提出了一种简化版的注意力机制，并且得到了不错的结果。</code></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img src="/2016/04/24/自动文摘（五）/model.png" width="600" height="800">
<p>encoder使用文章内容作为输入，一个时间点表示一个单词，每个单词先通过embedding层将词转换为一个分布式向量（<code>word embedding</code>）。每个词向量都由前一个词向量生成，第一个词定义为0向量。</p>
<p>decoder将encoder中最后一个词向量作为输入，decoder本质是一个rnnlm，使用softmax和attention mechanism来生成每个词。</p>
<p>损失函数：</p>
<img src="/2016/04/24/自动文摘（五）/lossfunction.png" width="600" height="650">
<p>这里y是输出的词，x是输入的词。</p>
<p>本文采用了4层LSTM，每层有600个单元，使用Dropout控制过拟合，所有参数的初始值都服从-0.1到0.1的平均分布，训练方法是RMSProp，学习速率0.01，动量项0.9，衰减项0.9，训练9个回合，在第5个回合之后，每个回合都将训练速率减半。batch训练，384组训练数据为一个batch。</p>
<p><code>模型的定义和训练方法都是借鉴于其他文章，模型参数的不同并不是什么创新，别人用gru或者birnn，你用lstm，或者别人用2层，你用3层、4层更多层，不同的模型参数可能会有不同的state-of-the-art结果，但并不会对大家认识abstractive summarization问题有什么实质性的帮助，也不会促进这个领域的发展，只是用着现有的方法在这个领域刷了一篇paper罢了。</code></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>注意力机制可以用来帮助神经网络更好地理解输入数据，尤其是一些专有名词和数字。attention在decoder阶段起作用，通过将输出与所有输入的词建立一个权重关系来让decoder决定当前输出的词与哪个输入词的关系更大（即应该将注意力放到哪个词上）。</p>
<p>本文采用两种不同的注意力机制，第一种称作复杂注意力模型（<code>complex attention</code>），与Minh-Thang采用的点乘机制（<code>dot mechanism</code>）一样，看下图：</p>
<img src="/2016/04/24/自动文摘（五）/complex.png" width="400" height="650">
<p>第二种称作简单注意力模型（<code>simple attention</code>），是第一种模型的变种，该种模型使得分析神经网络学习注意力权重更加容易。看下图：</p>
<img src="/2016/04/24/自动文摘（五）/simple.png" width="400" height="650">
<p>对比两幅图可以看出区别在于隐藏层的最后一层的表示上，简单模型将encoder部分在该层的表示分为两块，一小块用来计算注意力权重（<code>attention weight</code>），另一大块用来作为上下文（<code>context vector</code>）；decoder部分在该层的表示也分为两块，一小块用来计算注意力权重，另一大块用来导入softmax，进行输出预测。</p>
<p><code>simple attention mechanism的提出可以算作本文的主要贡献，但是感觉贡献量并不大。修改所谓的理论模型，而不仅仅是对模型参数进行修改，本质上是对encoder的context vector进行了更换，用了一些技巧，比如文中的方法，将隐藏层最后一层的表示分为两部分，一部分用来表示context，一部分用来表示attention weight，就有了新的模型。</code></p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><p>本文用English Gigaword数据集，该数据集包括了六大主流媒体机构的新闻文章，包括纽约时报和美联社，每篇文章都有清晰的内容和标题，并且内容被划分为段落。经过一些预处理之后，训练集包括5.5M篇新闻和236M单词。</p>
<h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>headlines作为target，news text的第一段内容作为source，预处理包括：小写化，分词，从词中提取标点符号，标题结尾和文本结尾都会加上一个自定义的结束标记<code>&lt;eos&gt;</code>，那些没有标题或者没有内容或者标题内容超过25个tokens或者文本内容超过50个tokens都会被过滤掉，按照token出现频率排序，取top 40000个tokens作为词典，低频词用符号<code>&lt;unk&gt;</code>进行替换。</p>
<p>数据集被划分为训练集和保留集，训练集将会被随机打乱。</p>
<p><code>数据的预处理是一件重要的事情，处理的好坏直接影响结果的好坏。本文的每一个处理细节都交代的很清楚，有希望做相同实验的童鞋可以借鉴他的处理方法</code></p>
<h2 id="Dataset-Issues"><a href="#Dataset-Issues" class="headerlink" title="Dataset Issues"></a>Dataset Issues</h2><p>训练集中会出现标题与所输入文本关系不大的情况，比如：标题包括以下字样For use by New York Times service clients，或者包括一些代码，biz-cover-1等等，本文对此不作处理，因为一个理想的模型可以处理这些问题。‘</p>
<p><code>数据集本身会有一些错误，但一个好的模型是可以处理好这些错误的数据，所以本文对此种数据并不做处理。</code></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>模型的优劣用两种方法进行评价。第一种，将训练集和保留集<code>损失值</code>作为评价指标；第二种，将<code>BLEU</code>作为评价指标，为了保证效率，保留集仅仅用了384个样本进行计算。</p>
<p><code>评价指标也是常规的两种，两种数据集上的loss值直观地反应了训练和测试效果，BLEU是机器翻译领域中常用的评价标准。</code></p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>计算硬件是GTX 980 Ti GPU，每种模型的计算都会花费4.5天时间。效果直接看下图：</p>
<img src="/2016/04/24/自动文摘（五）/evaluation.png" width="600" height="800">
<p>在应用模型结果做保留集的预测时，不同新闻来源的文章预测效果不一样。比如：在BBC、华尔街日报、卫报的效果就非常好，但是在赫芬顿邮报和福布斯的效果就很差。</p>
<p><code>结果看上图也是一目了然，本文的simple attention mechanism更胜一筹。</code></p>
<h2 id="Understanding-information-stored-in-last-layer-of-the-neural-network"><a href="#Understanding-information-stored-in-last-layer-of-the-neural-network" class="headerlink" title="Understanding information stored in last layer of the neural network"></a>Understanding information stored in last layer of the neural network</h2><p>存在有许多思路来理解注意力机制函数，考虑下面的公式，从输入计算到softmax输出：</p>
<img src="/2016/04/24/自动文摘（五）/formula.png" width="600" height="650">
<p>第一个部分表示attention context vector对decoder输出的影响，由于context是从input计算得来的，可以理解为encoder的每个输入对decoder输出的影响；第二个部分表示decoder当前隐藏层最后一层对输出的影响；第三个部分表示偏置项。</p>
<h2 id="Understanding-how-the-attention-weight-vector-is-computed"><a href="#Understanding-how-the-attention-weight-vector-is-computed" class="headerlink" title="Understanding how the attention weight vector is computed"></a>Understanding how the attention weight vector is computed</h2><p><code>注意到这一点很重要，encoder部分的神经元对docoder部分的神经元起作用，也就是attention weight的本质。</code></p>
<h2 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h2><p>本文的模型中存在几种类型的错误，包括：</p>
<p>1、神经网络机制在填充细节时细节发生丢失。比如：target是 72 people died when a truck plunged into a gorge on Friday，而模型的预测是 72 killed in truck accident in Russia。这种错误经常出现在decoder beam很小的情况下。</p>
<p>2、生成的headline与输入的文本没有太大的关系，这些headline在训练集中出现太多次。这种错误常出现在decoder beam很大的情况下。</p>
<p>上述两种错误反映了本文的模型对decoder beam非常敏感。</p>
<p><code>个人感觉本文的重点在于动手实践seq2seq+attention在自动文摘中的应用，对很多模型层面上的研究很少，对效果分析上的研究也很浅。</code></p>
<h1 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h1><p>使用BiRNN来代替RNN配合attention model效果可能会更好一些。</p>
<p><code>将模型更换为Bi-RNN会得到一个新的结果，不知道会不会有人拿这个来刷paper，个人觉得好无趣。</code></p>
<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>本文提出的simple attention mechanism效果很不错。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1512.01712" target="_blank" rel="external">Generating News Headlines with Recurrent Neural Networks</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-18T02:06:18.000Z"><a href="/2016/04/17/自动文摘（四）/">2016-04-17</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/17/自动文摘（四）/">自动文摘（四）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>这篇博客是自动文摘系列的第四篇，重点介绍近期abstractive summarization的一些研究情况。abstractive是学术界研究的热点，尤其是Machine Translation中的encoder-decoder框架和attention mechanism十分火热，大家都试着将abstractive问题转换为sequence-2-sequence问题，套用上面两种技术，得到state-of-the-art结果，2015年来已经有许多篇paper都是这种套路，于是就有了下面的吐槽：</p>
<img src="http://ww4.sinaimg.cn/mw690/62caff97jw1f2qam97d3tj20sg0nd775.jpg" width="400" height="650">
<h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>Encoder-Decoder不是一种模型，而是一种框架，一种处理问题的思路，最早应用于机器翻译领域，输入一个序列，输出另外一个序列。机器翻译问题就是将一种语言序列转换成另外一种语言序列，将该技术扩展到其他领域，比如输入序列可以是文字，语音，图像，视频，输出序列可以是文字，图像，可以解决很多别的类型的问题。这一大类问题就是上图中的sequence-to-sequence问题。这里以输入为文本，输出也为文本作为例子进行介绍：</p>
<img src="/2016/04/17/自动文摘（四）/seq2seq.png" width="400" height="650">
<p>encoder部分是将输入序列表示成一个带有语义的向量，使用最广泛的表示技术是Recurrent Neural Network，RNN是一个基本模型，在训练的时候会遇到gradient explode或者gradient vanishing的问题，导致无法训练，所以在实际中经常使用的是经过改良的LSTM RNN或者GRU RNN对输入序列进行表示，更加复杂一点可以用BiRNN、BiRNN with LSTM、BiRNN with GRU、多层RNN等模型来表示，输入序列最终表示为最后一个word的hidden state vector。</p>
<p>decoder部分是以encoder生成的hidden state vector作为输入“解码”出目标文本序列，本质上是一个语言模型，最常见的是用Recurrent Neural Network Language Model(RNNLM)，只要涉及到RNN就会有训练的问题，也就需要用LSTM、GRU和一些高级的model来代替。目标序列的生成和LM做句子生成的过程类似，只是说计算条件概率时需要考虑encoder向量。</p>
<p>这里，每一种模型几乎都可以出一篇paper，尤其是在这个技术刚刚开始应用在各个领域中的时候，大家通过尝试不同的模型组合，得到state-of-the-art结果。</p>
<p>该框架最早被应用在Google Translation中，paper详情可以见[1]，2014年12月发在arxiv上。</p>
<h1 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h1><p>注意力机制在NLP中的使用也就是2015年的事情，也是从机器翻译领域开始。我们仔细看decoder中生成目标文本序列这部分，第一个word的生成完全依赖于encoder的last hidden state vector，而这个vector更多的是表示输入序列的最后一个word的意思，也就是说rnn一般来说都是一个有偏的模型。</p>
<p>打个比方，rnn可以理解为一个人看完了一段话，他可能只记得最后几个词说明的意思，但是如果你问他前面的信息，他就不能准确地回答，attention可以理解为，提问的信息只与之前看完的那段话中一部分关系密切，而其他部分关系不大，这个人就会将自己的注意力锁定在这部分信息中。这个就是所谓attention mechanism的原理，每个hidden state vector对于decoder生成每个单词都有影响，但影响分布并不相同，请看下图：</p>
<img src="/2016/04/17/自动文摘（四）/attention.png" width="400" height="650">
<p>图中行文本代表输出，列文本代表输入，颜色越深表示两个词相关性越强，即生成该词时需要多注意对应的输入词。不同的paper在使用attention上会有不同的技巧，这里不一一赘述了。</p>
<h1 id="Neural-Summarization"><a href="#Neural-Summarization" class="headerlink" title="Neural Summarization"></a>Neural Summarization</h1><p>使用deep learning技术来做abstractive summarization的paper屈指可数，大体的思路也类似，大概如下：</p>
<p>0、首先将自动文摘的问题构造成一个seq2seq问题，通常的做法是将某段文本的first sentence作为输入，headlines作为输出，本质上变成了一个headlines generative问题。</p>
<p>1、选择一个big corpus作为训练、测试集。自动文摘的技术没有太成熟的一个重要原因在于没有一个成熟的大规模语料。一般来说都选择Gigawords作为训练、测试集，然后用DUC的数据集进行验证和对比。</p>
<p>2、选择一个合适的encoder，这里可以选simple rnn，lstm rnn，gru rnn，simple birnn，lstm birnn，gru birnn，deep rnn，cnn，以及各种各样的cnn。不同model之间的组合都是一种创新，只不过创新意义不太大。用encoder将输入文本表示成一个向量。</p>
<p>3、选择一个合适的decoder，decoder的作用是一个language model，用来生成summary words。</p>
<p>4、设计一个合适的attention model。不仅仅基于encoder last hidden state vector和上文来预测输出文本序列，更要基于输入中“注意力”更高的词来预测相应的词。</p>
<p>5、设计一个copy net。只要是语言模型都会存在相同的问题，比如out-of-vocabulary词的处理，尤其是做新闻类摘要的生成时，很多词都是人名、机构名等专有名词，所以这里需要用copy net 将输入中的词copy过来生成输出。在生成中文摘要问题上，将words降维到characters可以避免oov的问题，并且取得不错的结果。</p>
<p>接下来想做的事情是将neural summarization相关的paper精读之后写成blog。</p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></p>
<p>[2] <a href="http://cn.arxiv.org/pdf/1509.00685" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a></p>
<p>[3] <a href="http://cn.arxiv.org/pdf/1506.05865" target="_blank" rel="external">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<p>[4] <a href="http://cn.arxiv.org/pdf/1603.06393" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-06T18:31:55.000Z"><a href="/2016/04/06/自动文摘（三）/">2016-04-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/06/自动文摘（三）/">自动文摘（三）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>蜀道之难 难于上青天</strong></p>
</blockquote>
<p>虽然有很多SaaS提供Summarization的服务，虽然有很多App尤其是新闻类App标榜自己拥有多么牛的技术做Summarization，我们还是不得不承认自动文摘的技术离一个高水平的AI还有一段距离，很长的一段距离。都说自动文摘很难，到底难在哪里？</p>
<h1 id="Abstractive"><a href="#Abstractive" class="headerlink" title="Abstractive"></a>Abstractive</h1><p>上一篇博客分享了Extraction方法的一些思路，本篇简单聊一点Abstractive的想法。<br>Abstractive是一个True AI的方法，要求系统理解文档所表达的意思，然后用可读性强的人类语言将其简练地总结出来。这里包含这么几个难点：</p>
<p>1、理解文档。所谓理解，和人类阅读一篇文章一样，可以说明白文档的中心思想，涉及到的话题等等。</p>
<p>2、可读性强。可读性是指生成的摘要要能够连贯（Coherence）与衔接（Cohesion），通俗地讲就是人类读起来几乎感觉不出来是AI生成的（通过图灵测试）。</p>
<p>3、简练总结。在理解了文档意思的基础上，提炼出最核心的部分，用最短的话讲明白全文的意思。</p>
<p>上述三个难点对于人类来说都不是一件容易的事情，何况是发展没太多年的自然语言处理技术。人工智能领域中AI能够领先人类的例子很多，包括前不久很火的Alpha狗，图片识别，主要是利用计算机远强于人类的计算能力，但也有很多的领域，AI离人类的水平还有很远，比如paper的survey，summarization，机器翻译等等。</p>
<p>近几年随着Deep Learning的火爆，研究者们利用一些最新的研究成果来做summarization，比如attention model，比如rnn encoder-decoder框架，在一定程度上实现了abstractive，但还是处于研究初期，效果还不算很好。</p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>自动文摘最大的一个难点是评价问题，如何有效地、合理地评价一篇文摘的效果是一个很难的问题。</p>
<h2 id="人工评价"><a href="#人工评价" class="headerlink" title="人工评价"></a>人工评价</h2><blockquote>
<p><strong>一千个读者，有一千个哈姆雷特</strong></p>
</blockquote>
<p>不同的人理解一篇文档会有很大的不同，基于人工评价的方法有类似于评价开放的文科辨析题目答案一样，需要从答案中寻找一些所谓的要点，计算要点覆盖率，打分。人工评价结果在很大程度上都是可信的，因为人可以推理、复述并使用世界知识将具有类似意思但形式不同的文本单元关联起来，更加灵活一些，但时间成本太高，效率太低。</p>
<h2 id="自动评价"><a href="#自动评价" class="headerlink" title="自动评价"></a>自动评价</h2><p>计算机评价效果，需要给定参考摘要作为标准答案，通过制定一些规则来给生成的摘要打分。目前，使用最广泛的是ROUGH系统（Recall-Oriented Understudy for Gisting Evaluation），基本思想是将待审摘要和参考摘要的n元组共现统计量作为评价依据，然后通过一系列标准进行打分。包括：ROUGH-N、ROUGH-L、ROUGH-W、ROUGH-S和ROUGH-SU几个类型。通俗地将就是通过一些定量化的指标来描述待审摘要和参考文摘之间的相似性，维度考虑比较多，在一定程度上可以很好地评价Extracive产生的摘要。</p>
<p>这里涉及到一个重要的问题，就是标注语料问题。自动评价需要给定一系列文档已经他们的参考文摘，用来测试不同的算法效果。TAC（Text Analysis Conference）和TREC（Text REtrieval Conference）两个会议提供了相关的评测数据集，自动文摘领域的paper都是以这些数据集为baseline，与其他paper的算法进行对比。会议的数据集毕竟有限，新的领域中做自动文摘需要建立自己的数据集作为标准。</p>
<p>现有的评价标准存在的一个重要问题在于没有考虑语义层面上的相似，评价extractive还好，但评价abstractive就会效果不好了。Deep Learning其实就是一个representation learning，将世界万物表示成数字，然后作分析。在词、句子甚至段落这个层面上的表示学习研究的非常多，也有很多的state-of-the-art的结果，所以做语义层面上的评价并不难。</p>
<h2 id="重要性"><a href="#重要性" class="headerlink" title="重要性"></a>重要性</h2><p>评价对于一个研究领域非常重要，是牵引这个领域前进的首要因素，评价需要制定标准，标准的好坏关系到这个领域的研究质量，尤其是研究者们的paper质量，因为大家相互比较算法的优劣就十分依赖这样的标准。标准数据集的建立以及baseline的提出，是最首要的任务。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-31T02:39:58.000Z"><a href="/2016/03/30/自动文摘（二）/">2016-03-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/30/自动文摘（二）/">自动文摘（二）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>自动文摘的方法主要分为两大类，extractive和abstractive。前者是目前最主流、应用最多、最容易的方法，后者相对来说更有一种真正人工智能的味道。还有另外一种分类方法是，单文档摘要和多文档摘要，前者是后者的基础，但后者不只是前者结果简单叠加那么简单。本文只介绍单文档的extractive方法。</p>
<h1 id="Extractive-Summarization"><a href="#Extractive-Summarization" class="headerlink" title="Extractive Summarization"></a>Extractive Summarization</h1><p>抽取式的方法基于一个假设，一篇文档的核心思想可以用文档中的某一句或几句话来概括。那么摘要的任务就变成了找到文档中最重要的几句话，也就是一个排序的问题。</p>
<p>排序是一个非常经典的问题，也是一个非常多解决方案的问题。比如：Google根据用户的query生成的网页列表，就是一个排序之后的结果；再比如Amazon的推荐系统推荐给用户的N个可能感兴趣的产品，也都是通过算法做了排序输出的。</p>
<p>排序针对不同的问题，需要提出不同的指标，比如有的应用关心的是相关性，有的关心的是时效性，有的关心的是新颖性等等，在这个层面上来讨论排序，会有不同的模型。</p>
<p>一般的抽取式摘要问题，会考虑相关性和新颖性两个指标。相关性是指摘要所用的句子最能够代表本文档的意思，而新颖性是指候选句子包含的冗余信息要少，尽可能每句话都可以独立地表达出一种独立的意思。</p>
<p>下面简单介绍一些思路。</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>NLP任务的标准流程中第一步都是预处理，将拿到的文本做分句，这里有两种可能性，一是用句点或者其他可以表达一句话结尾的符号作为分隔，另外一种是用逗号作为分隔符获取句子。</p>
<h2 id="词、句表示"><a href="#词、句表示" class="headerlink" title="词、句表示"></a>词、句表示</h2><p>这一步的思路是：将词、句子表示成计算机能理解的量，然后计算一些指标进行排序。这个地方也是各种算法、模型最大的不同之处：</p>
<p>1、Bag Of Words。词袋模型将词定义为一个维度，一句话表示成在所有词张成的空间中的一个高维稀疏向量。<br>2、TFIDF。可以理解为带权重的词袋模型，计算出每个词的TFIDF值，作为该词的权重。<br>3、LDA/LSI。将整篇文档利用TFIDF模型表示成一个矩阵，做SVD降维分解，生成两个矩阵，一个是文档-话题矩阵、另一个是词-话题矩阵。得到词-话题矩阵之后，可以得到句子-话题矩阵。<br>4、Word Embedding。Tomas Mikolov提出的Word2Vec，用了很多技巧和近似的思路让word很容易地表示成一个低维稠密向量，在很多情况下都可以达到不错的效果。词成为了一个向量，句子也可有很多种方法表示成一个向量。</p>
<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>这里介绍两种常见的方法。</p>
<p>1、基于图排序</p>
<p>将文档的每句话作为节点，句子之间的相似度作为边权值构建图模型，用pagerank算法进行求解，得到每个句子的得分。</p>
<p>代表算法有TextRank和LexRank。</p>
<p>2、基于特征</p>
<p>特征工程在深度学习火之前是解决特定领域问题的良药，这里用到的特征包括：</p>
<p>1）句子长度，长度为某个长度的句子为最理想的长度，依照距离这个长度的远近来打分。</p>
<p>2）句子位置，根据句子在全文中的位置，给出分数。（比如每段的第一句是核心句的比例大概是70%）</p>
<p>3）句子是否包含标题词，根据句子中包含标题词的多少来打分。</p>
<p>4）句子关键词打分，文本进行预处理之后，按照词频统计出排名前10的关键词，通过比较句子中包含关键词的情况，以及关键词分布的情况来打分。</p>
<p>代表算法是TextTeaser。</p>
<h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>排序之后的结果只考虑了相关性并没有考虑新颖性，非常有可能出现排名靠前的几句话表达的都是相似的意思。所以需要引入一个惩罚因子，将新颖性考虑进去。对所有的句子重新打分，如下公式：</p>
<p>a x score(i) + (1-a) x similarity(i,i-1), i = 2,3,….N</p>
<p>序号i表示排序后的顺序，从第二句开始，排第一的句子不需要重新计算，后面的句子必须被和前一句的相似度进行惩罚。</p>
<p>这个算法就是所谓的MMR（Maximum Margin Relevance）</p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>输出的结果一般是取排序后的前N句话，这里涉及到一个非常重要的问题，也是一直自动文摘质量被诟病的问题，可读性。因为各个句子都是从不同的段落中选择出来的，如果只是生硬地连起来生成摘要的话，很难保证句子之间的衔接和连贯。保证可读性是一件很难的事情。</p>
<p>这里有一个取巧的方法，就是将排序之后的句子按照原文中的顺序输出，可以在一定程度下保证一点点连贯性。</p>
<blockquote>
<p><strong>路漫漫其修远兮，吾将上下而求索</strong></p>
</blockquote>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="https://gist.github.com/rsarxiv/11470a8d763b2845f671061c21230435" target="_blank" rel="external">TextRank源码阅读笔记</a><br>[2] <a href="https://gist.github.com/rsarxiv/4e949264b3bda98828b84cf2991e57e4" target="_blank" rel="external">TextTeaser源码阅读笔记</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-27T00:20:02.000Z"><a href="/2016/03/26/RSarXiv-Web版/">2016-03-26</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/26/RSarXiv-Web版/">RSarXiv Web版</a></h1>
  

    </header>
    <div class="entry">
      
        <p>一直感觉app很难承载太多的功能，所以在app后台基础上，开发了这个web版，包括以下几个功能：</p>
<h1 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h1><p>检索一直是查论文最常用的方式，web版提供了检索功能，查询结果只有一些简要信息，详细的信息需要进入detail页面进行查看。</p>
<h1 id="反馈"><a href="#反馈" class="headerlink" title="反馈"></a>反馈</h1><p>如果用户希望用到一些更深层次的功能，比如推荐功能，可以在paper详细信息页中进行显式地反馈，包括对某篇paper点个赞，某个作者点个赞，某个研究领域点个赞，某个关键词点个赞；当然也包括一些隐式反馈，对某篇paper进行review。</p>
<h1 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h1><p>这个名词在电商中出现的多一些，用来形容一个用户的各个维度的特征，然后根据用户的特点进行商品推荐。web版沿用了app版本中的用户画像，是一张人脸标签云，当你的tag越多，人脸就越明显。</p>
<h1 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h1><p>推荐主要来自三个维度：</p>
<p>[1]TopN推荐基于用户感兴趣的tag，包括subject、authors、keywords、paper等信息。</p>
<p>[2]单篇paper的相似paper由elasticsearch的more like this功能实现。</p>
<p>[3]对某个tag点赞之后，会对用户推荐与该tag相关的tag，这个结果由word2vec实现。</p>
<h1 id="与App比较"><a href="#与App比较" class="headerlink" title="与App比较"></a>与App比较</h1><p>Web版与App最大的不同在于用户的输入，App提供了一个类似于tinder的滑动卡片进行初始化的功能，用户不需要输入一个字，通过tag推荐进行初始tag的选择，生成初始的用户画像和推荐结果，再通过不断地反馈来丰富画像和更新推荐结果；而Web的输入是来自于检索，通过用户主动地查询所感兴趣的tag来生成用户画像和推荐结果。</p>
<h1 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h1><p>目前的应用，不管是web端也好，还是app，甚至是之前测试用的微信公众号，都没有涉及到语义这个层面，包括检索，也包括推荐。</p>
<p>paper的质量没有一个很好地保证，arxiv上的paper质量是这个平台的一个短板，下一步有时间把paper之间的引用关系爬下来，跑个pagerank，给每个领域中的paper排个序，尽量给用户多多推荐高质量的paper。</p>
<p>因为最近在关注自动文摘，所以想做这么一个事情，就是给query结果生成多个paper的综述（survey），但是这并不是一件很容易的事情，因为自动文摘有着很多的难点（下一期自动文摘blog写这个主题）。</p>
<p>应用没有太多的利用到所谓的群体智慧，比如各个平台上对各篇paper的review，比如协同过滤之类的东西，前者需要花费太多的精力而且效果应该不会太好，后者需要积累一些用户数据。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p>[1] App下载可以在app store中搜索rsarxiv</p>
<p>[2] Web版网址是<a href="http://rsarxiv.science/web" target="_blank" rel="external">http://rsarxiv.science/web</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-20T14:44:46.000Z"><a href="/2016/03/20/自动文摘（一）/">2016-03-20</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/20/自动文摘（一）/">自动文摘（一）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>最近人工智能随着AlphaGo战胜李世乭这一事件的高关注度，重新掀起了一波新的关注高潮，有的说人工智能将会如何超越人类，有的说将会威胁到人类的生存和发展，种种声音都在表明人工智能的又一个春天即将到来，但很多学者认为媒体的过度炒作，会引发民众对人工智能不切实际地期待，从而导致人工智能寒冬的又一次到来。Yann Lecun作为上一个人工智能寒冬时期还在坚持做冷门的神经网络研究的人，他对AI有一个非常理性的认知。<br><img src="http://ww2.sinaimg.cn/mw690/5396ee05jw1f21a1ncxl4j20do0b376e.jpg" width="400" height="650"></p>
<p>最近几年在人工智能领域中大热的工程技术deep learning，将机器对图像，语音，人类语言的认知能力都提升了不少，前前后后也涌现出不少不仅仅是很cool而且是非常实用的应用，比如人脸识别，猫脸识别，无人车，语义搜索等等。其中，深度学习技术对图像和语音的影响最大，但对人类语言的理解（NLP）做的没有那么那么好。所以，不必太过鼓吹人工智能将会如何如何，民众的期待不应太过接近科幻电影，不然只能换来无尽的失望，从而导致寒冬的来临。</p>
<p>NLP是一个非常难的task，至今有很多的子task都没有得到太好的解决。虽然每天我们在arxiv上都可以看到update的paper，但大多数都是一些model上的小trick，在个别数据集上跑一些example，和baseline做一些对比，得到所谓的state-of-the-art结果，并没有真正深刻理解要解决的问题，所谓的唯model论。不久前，Christopher D. Manning在文章中写了这么一句话：</p>
<blockquote>
<p><strong><br>However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.</strong></p>
</blockquote>
<p>以上是一些简单的背景介绍，下面进入正题。</p>
<p>自动文摘（auto text summarization）是NLP中较难的技术，难点很多，至今并没有一个非常让人满意的、成熟的技术来解决这个问题。</p>
<h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><p>大家在查文献的时候，输入一个关键词之后，会返回一个paper列表，如果你只看paper的title可能会被一些标题党蒙骗，如果每篇paper都看abstract，时间会花太久，看着很烦。所以我在想，给rsarxiv添加一个功能，基于query的research survey生成。当你输入一个keyword之后，返回的结果不仅仅是paper列表，还有一个非常精炼的survey，你可以通过阅读survey了解到每篇paper的最核心工作，如果你感兴趣的话，可以进一步查看paper的具体内容。</p>
<p>基于这个idea，开始逐步地了解自动文摘技术，所以这一系列blog的目的是为了记录我在学习自动文摘过程中的一些点滴心得。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>第一篇blog对自动文摘做一个简单的介绍。</p>
<p>自动文摘技术应用最广的领域在于新闻，由于新闻信息的过载，人们迫切地希望有这么一个工具可以帮助自己用最短的时间了解最多的最有用的新闻（为什么不直接看标题呢？因为很多新闻为了哗众取宠，故意将标题起的特别吸引人眼球，但却名不副实），因此就有了Yahoo 3000w$$ 收购summly的交易。另外，搜索引擎也是应用之一，基于query的自动文摘会帮助用户尽快地找到感兴趣的内容。前者是单文档摘要技术，后者是多文档摘要技术，后者较于前者会更加复杂一些。</p>
<p>自动文摘出现的重要原因之一是信息过载问题的困扰，（当然个性化推荐系统是解决信息过载的另外一个好的办法）另外一个重要原因是人工文摘的成本较高。可以想象，如果计算机有能力写出一个topic下的综述paper，也就不需要survey作者去花大量的时间来读和写了。</p>
<p>自动文摘要解决的问题描述很简单，就是用一些精炼的话来概括整篇文章的大意，用户通过阅读文摘就可以了解到原文要表达的意思。问题包括两种解决思路，一种是extractive，抽取式的，从原文中找到一些关键的句子，组合成一篇摘要；另外一种是abstractive，摘要式的，这需要计算机可以读懂原文的内容，并且用自己的意思将其表达出来。现阶段，相对成熟的是抽取式的方案，有很多很多的算法，也有一些baseline的测试，但得到的摘要效果差强人意，对后者的研究并不是很多，人类语言包括字、词、短语、句子、段落、文档这几个level，研究难度依次递增，理解句子、段落尚且困难，何况是文档，这是自动文摘最大的难点。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p>[1] <a href="https://www.quora.com/profile/Zhang-Jun-3/Auto-text-summarization/What-are-the-challenges-of-automatic-text-summarization?srid=3gtv&share=6c9f8e3a" target="_blank" rel="external">Quora上的问答</a><br>[2] <a href="https://www.zhihu.com/question/41465328" target="_blank" rel="external">知乎上的问答</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-06T23:40:40.000Z"><a href="/2016/03/06/RSarXiv前世今生/">2016-03-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/06/RSarXiv前世今生/">RSarXiv前世今生</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="RSarXiv是什么？"><a href="#RSarXiv是什么？" class="headerlink" title="RSarXiv是什么？"></a>RSarXiv是什么？</h1><p>RSarXiv是我的一个side project，甚至有一段时间几乎做成了main project，从开始利用课余时间做设计，写代码到后面的每天全天时间都在写代码。</p>
<p>RSarXiv是一个让人用起来很愉快的论文推荐App，在其简单的操作背后，蕴藏着复杂的、有趣的智能算法作为支撑。</p>
<p>RSarXiv的目的是让用户可以轻松、快乐地获得感兴趣的论文。</p>
<h1 id="为什么要做RSarXiv？"><a href="#为什么要做RSarXiv？" class="headerlink" title="为什么要做RSarXiv？"></a>为什么要做RSarXiv？</h1><blockquote>
<p><strong>博观而约取 厚积而薄发</strong></p>
</blockquote>
<p>11年开始，断断续续地关注过推荐系统算法及相关的应用，也亲眼见证了从11年开始国内互联网界如雨后春笋般地涌现出大量的推荐系统应用，最早的是豆瓣猜，猜你喜欢的电影、书籍、歌曲，亚马逊、淘宝等一些电商的猜你可能喜欢的商品，无觅网、指阅到今日头条猜你感兴趣的新闻事件，以及12年一位该领域的大神为了写书而推出的paperlens项目，也是推荐用户论文等等。</p>
<p>大约两三年的时间，推荐系统几乎成了各大网站的标配系统，甚至一度出现了一种声音，推荐系统可能会将搜索引擎替代了。实践证明，将推荐系统作为搜索引擎的一种补充和增强是一种更加理智的选择。那个时候，也可以看得出概念的炒作，很多的人写博客、写书来鼓吹某一项技术可以打遍天下无敌手。现在，回头看看都是一个笑话。</p>
<p>世界上没有完美的技术，只有完美的产品。千万不要有一招武功吃遍天下鲜的想法，只有产品真真正正、踏踏实实地好用，给用户提供了方便才是真理！给自己身上贴满了热门标签的产品，只能表现出其浮夸的内心。</p>
<p>当时，做RSarXiv这个产品的想法其实早几年就在心里种下了种子，可能没有做出来那么具体，只是想着能够做一个好用的推荐系统，让自己真正地享受一把人工智能带来的乐趣。</p>
<p>时间定格在15年9月底，在烟台出差的无聊日子中，终于忍不住想做一些实实在在的事情了，于是写下了一个很粗糙的计划，包括搭建爬虫系统，学习swift ios app开发，API服务器的开发等等相关的东西。要推荐什么内容？这个问题想了一段时间，后来决定是论文，因为那段时间需要多读一些论文准备开题的工作，真正地感觉找论文，尤其是真正感兴趣的论文和质量高的论文是一件很难的事情，所以这个事情就这样开始了。</p>
<h1 id="如何做RSarXiv？"><a href="#如何做RSarXiv？" class="headerlink" title="如何做RSarXiv？"></a>如何做RSarXiv？</h1><h2 id="Version-1-0"><a href="#Version-1-0" class="headerlink" title="Version 1.0"></a>Version 1.0</h2><p>第一个版本的RSarXiv其实主要是做了一个前端的APP，功能非常简单，输入自己感兴趣的tag，提交给服务器，返回与这几个tag相关的paper，如果对哪篇paper感兴趣，通过点赞的方式反馈给系统，系统将这篇paper的tag分解出来，推荐给用户，用户通过管理这些tag，删除不感兴趣的tag，留下感兴趣的tag，重新提交给服务器，获取感兴趣的paper，就这样循环迭代地使用这个APP。</p>
<p>当时主要的目的是练习写APP，检验一下前一段时间学习swift的成果，最后还提交了一个版本到App Store，因为各种不规范的原因，没有能够发布出来。</p>
<p>这个版本非常简单，因为只需要做前端的APP，后端的算法使用的 <a href="http://lateral.io" target="_blank" rel="external">http://lateral.io</a> 这个网站的API，因为人家服务器在欧洲，所以每次返回的结果都需要花费3s以上，根本无法忍受。</p>
<h2 id="Version-2-0"><a href="#Version-2-0" class="headerlink" title="Version 2.0"></a>Version 2.0</h2><p>通过1.0的版本学习到了如何写一个简单的APP，但离真正地实现自己想做的事情还有好大一步。</p>
<p>于是，酝酿了几天的时间，终于下了狠心说，这次要做就做一个大的，决不草草了事。从11月初开始，从构建一个高可靠的爬虫系统开始，开始了一段main project生涯。那个时候开始，用teambition来管理任务和添加每天的感受，每天做完前一天晚上定下的task，然后晚上再定出第二天的task，day by day，task by task，隔三差五地写一篇心得，记录一下最近的想法和进步。</p>
<p>回过头看，那段时间打下了很扎实的基础，动手使用了很多以前望而却步的技术，攻克了许多以前想都不敢想的难题。感觉自己每天都在进步，很大的进步，人生简直充满了力量，当然体重也跟着增长了起来。</p>
<p>当然，现在看来，那段时间的很多实现方法还是有些幼稚，但正说明现在还在进步。</p>
<p>2.0版本最终的产品形式是微信公众号，提供了查询服务和推荐服务，还有一些其他好玩的东西。在后端技术上有了一个非常大的飞跃，打下了坚实的基础，极大地提升了自信心。</p>
<p>做完这个版本之后，我深深地感受到了一点，自信是做好事情最重要的一个因素。</p>
<h2 id="Version-2-3"><a href="#Version-2-3" class="headerlink" title="Version 2.3"></a>Version 2.3</h2><p>之所以没有将版本改为3.0，是因为后端的技术大多数沿用了2.0的基础，并没有做出太多的新的内容。这个版本最大的特点是技术性，这里说的技术不是开发的技术，而是对算法的学习、理解和应用，是一个将应用推向了一个新高度的技术。</p>
<p>这段时间里，我读了大量的paper、博客和代码，很多也很杂，有语言模型相关的，关键词提取相关的，词表示相关的，句子表示相关的，深度学习框架相关的，情感分析相关的等等等等。因为，这个阶段，我想让自己的APP真正的具有智能，让APP中用的信息都是高质量的，比如一篇paper中的关键词提取，用了很多方法；比如paper之间按照title来计算相似度，不仅仅是基于词袋模型来做，而是用了一些语义上的表示，比如用了word2vec将keywords，authors，subjects映射到了同一个空间中，输入一个tag，可以得到多个与之相关的tag，这里的tag是keywords，authors，subjects。</p>
<p>真正地接触到了这些知识之后，才慢慢感觉到了人工智能这个神秘的词汇在我们生活中的每一个角落里都发着光，每一次使用Google的时候，看新闻的时候，回复邮件的时候，都在使用着人工智能带给我们的方便。以前觉得CNN，RNN，LSTM这些词多么牛逼，多么难懂，看过paper，看过code，看过别人的分享之后，尤其是自己动手用过之后，觉得这些神奇就在不远处。用w2v训练出paper graph的时候，让我感到疯狂，那是我一直想做的事情，原来做起来就是那么地简单。</p>
<p>这个版本，开创了一个新纪元，不仅仅具备上述的技术含量，而且成功地在App Store中发布了，并且报名参加了一个App比赛。</p>
<h1 id="RSarXiv的未来如何？"><a href="#RSarXiv的未来如何？" class="headerlink" title="RSarXiv的未来如何？"></a>RSarXiv的未来如何？</h1><p>万事开头难，开了一个好头，现在说一说RSarXiv的未来如何。</p>
<ul>
<li><p>丰富数据源。目前RSarXiv只包括arXiv上cs领域的paper，因为之前的2.0版本包括了所有领域，感觉很多功能无法融合进去，比如code。所以，我考虑只做cs方向，但是不仅仅是arXiv的数据，还要包括ACM、citeseer等其他一些数据源。而且，如果只有paper，服务还是不够，因为在看paper的时候，很希望知道这个算法如何实现的，github或者一些其他代码收录平台的数据就会很重要，paper的评论和讨论，相关的news和blog也是一些非常好的数据。</p>
</li>
<li><p>提升技术性。因为刚刚迈入NLP的大门，刚刚找到一点门道，很多问题的理解和解决方法并不是最好的，所以这里我仍然需要利用大量的业余时间来学习相关的理论和解决方法，丰富自己的认知。比如，关键词的提取怎么做到更加高质量；title的表示可以不仅仅是word表示的加权线性组合，而是通过借助CNN来提取特征来表示title，来找相似paper；如何将N篇paper做一个自动摘要，就像专家写的survey一样，将用户的查询结果，以一个概括性的总结呈现给用户等等。一个东西做到很宽其实并不难，真正挖掘很深很细致才是最难的。</p>
</li>
<li><p>前端多样性。现在的产品版本其实是参加比赛的一个简化版本，很多有用的功能都没做进去，而且只有ios版本。当然手机操作有自己的好处，简单方便，但却真的不如电脑上可以做的那么丰富，所以之后会添加一个网站版本，提供更多更强大的功能。</p>
</li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-02-27T20:27:40.000Z"><a href="/2016/02/27/intro/">2016-02-27</a></time>
      
      
  
    <h1 class="title"><a href="/2016/02/27/intro/">RSarXiv</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>用户第一次使用的时候需要进行初始化，初始化的过程十分简单，向左滑动或者向右滑动卡片，卡片上标记有三类标签，<br>（1）某一个子研究领域，比如cs.CL表示计算语言学；<br>（2）某一位作者，比如Yoshua Bengio；<br>（3）某一个关键词，比如long short-term memory。<br>如果用户喜欢某一个卡片上的标签的话，只需要向左滑动，否则向右滑动。如果用户向左滑动，系统会将与该标签相关的标签展示在后面的卡片上，比如某位用户喜欢了long short-term memory这个关键词，系统在后面的卡片上可能会显示recurrent neural network,machine learning,data mining等相关关键词。<br>用户可以一直选择下去，直到喜欢的标签达到20条或者没有候选的标签为止。</p>
<img src="/2016/02/27/intro/1.png" width="400" height="650">
<h3 id="用户肖像"><a href="#用户肖像" class="headerlink" title="用户肖像"></a>用户肖像</h3><p>用户进行完初始化之后，会进入产品的首页，可以看到形象的用户肖像，即用户感兴趣的标签，标签越大，所占权重也越大，系统推荐的文章都是基于这些标签。</p>
<img src="/2016/02/27/intro/2.png" width="400" height="650">
<h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><p>推荐的结果来自于arxiv所有数据中与用户肖像最相关的10篇文章。推荐结果是一系列文章标题的列表，用户感兴趣可以点击进入文章详情页。下拉推荐列表可以获取实时推荐结果。</p>
<img src="/2016/02/27/intro/3.png" width="400" height="650">
<h3 id="用户反馈"><a href="#用户反馈" class="headerlink" title="用户反馈"></a>用户反馈</h3><p>进入文章详情页，可以看到文章标题、所属领域、作者、摘要、关键词、10篇相似文章、原文PDF链接。<br>（1）用户如果对整篇文章感兴趣的话，可以在标题处点赞；<br>（2）如果用户对某个领域感兴趣，可以点赞，系统将推荐与该领域相关的领域、作者、关键词<br>（3）如果用户对某位作者感兴趣的话，可以点赞，系统将推荐与该作者相关的领域、作者、关键词；<br>（4）如果用户对某个关键词感兴趣的话，可以点赞，系统将推荐与该关键词相关的领域、作者、关键词；<br>（5）如果用户想查看与该篇文章相似的文章，可以点Similar Paper中的文章进行查看。</p>
<img src="/2016/02/27/intro/4.png" width="400" height="650">
<p>用户反馈行为将被记录在日志中，所占权重与时间相关，越新的行为对用户肖像的结果影响越大。用户使用APP越多，产生的反馈行为也就越多，系统推荐给用户的文章就越准确。</p>
<img src="/2016/02/27/intro/5.png" width="400" height="650">
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><a href="http://www.tudou.com/programs/view/JZINheteVq4/" target="_blank" rel="external">使用方法视频</a>
<h2 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a>下载链接</h2><img src="/2016/02/27/intro/qcode.png" width="100" height="100">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/page/2/" class="alignleft prev">上一页</a>
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Autoencoder/">Autoencoder</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/DQN/">DQN</a><small>4</small></li>
  
    <li><a href="/tags/Memory-Network/">Memory Network</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>77</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/RNNLM/">RNNLM</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/Reading-Comprehension/">Reading Comprehension</a><small>6</small></li>
  
    <li><a href="/tags/Representation/">Representation</a><small>1</small></li>
  
    <li><a href="/tags/Text-Comprehension/">Text Comprehension</a><small>1</small></li>
  
    <li><a href="/tags/api-ai/">api.ai</a><small>1</small></li>
  
    <li><a href="/tags/arXiv/">arXiv</a><small>2</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/attention/">attention</a><small>3</small></li>
  
    <li><a href="/tags/bot/">bot</a><small>21</small></li>
  
    <li><a href="/tags/chatbot/">chatbot</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>95</small></li>
  
    <li><a href="/tags/open-source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/paperweekly/">paperweekly</a><small>2</small></li>
  
    <li><a href="/tags/reading-comprehension/">reading comprehension</a><small>1</small></li>
  
    <li><a href="/tags/reinforcement-learning/">reinforcement learning</a><small>1</small></li>
  
    <li><a href="/tags/sentence-representations/">sentence representations</a><small>1</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>17</small></li>
  
    <li><a href="/tags/text-comprehension/">text comprehension</a><small>1</small></li>
  
    <li><a href="/tags/torch/">torch</a><small>1</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>2</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/创业/">创业</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/综述/">综述</a><small>1</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>16</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>4</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 PaperWeekly
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>