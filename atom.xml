<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PaperWeekly</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://rsarxiv.github.io/"/>
  <updated>2016-12-04T18:22:03.000Z</updated>
  <id>http://rsarxiv.github.io/</id>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>本周值得读(2016.11.28-2016.12.02)</title>
    <link href="http://rsarxiv.github.io/2016/12/04/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-11-28-2016-12-02/"/>
    <id>http://rsarxiv.github.io/2016/12/04/本周值得读-2016-11-28-2016-12-02/</id>
    <published>2016-12-04T18:06:33.000Z</published>
    <updated>2016-12-04T18:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation"><a href="#A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation" class="headerlink" title="A Simple, Fast Diverse Decoding Algorithm for Neural Generation "></a><a href="http://t.cn/Rfj8F3k" target="_blank" rel="external">A Simple, Fast Diverse Decoding Algorithm for Neural Generation </a></h2><p>【beam search】在用seq2seq做一些nlp任务的时候，解码器负责将结果一个个地解出来。解码出的结果应该具有多样性的特点，尤其是在chatbot应用中体现更为突出。传统的beam search算法在解码时常常会解出一些非常安全但是没有实际意义的response，类似于“呵呵呵”，“我认为是这样的”这里的话。本文的工作针对这一问题，提出了一种改进版的beam search算法，通过引入一个惩罚因子来影响排序结果，从而使得解码出的结果更加多样性。建议研究seq2seq或者尝试用它来解决一些问题的童鞋可以精读此文。本文来自Jiwei Li。</p>
<h2 id="Dialogue-Learning-With-Human-In-The-Loop"><a href="#Dialogue-Learning-With-Human-In-The-Loop" class="headerlink" title="Dialogue Learning With Human-In-The-Loop "></a><a href="http://t.cn/Rf8XOcr" target="_blank" rel="external">Dialogue Learning With Human-In-The-Loop </a></h2><p>【对话系统】【在线学习】在线学习是chatbot在与人交互的过程中自动学习的一种方法，快速而且有效。本文给出了一种基于强化学习的在线交互学习方案，作者是Jiwei Li。建议研究或者做chatbot应用的童鞋可以好好研读此文。</p>
<h2 id="Visual-Dialog"><a href="#Visual-Dialog" class="headerlink" title="Visual Dialog "></a><a href="http://t.cn/RfH7BWW" target="_blank" rel="external">Visual Dialog </a></h2><p>【多模态对话】多模态问答（VQA）是一个比较好玩的任务，本文在此基础上提出了一个更加复杂而且有意思的任务，即给定一张图像，给出若干个问和答的历史对话，提出一个新问题，要求给出正确答案。问题不仅仅需要理解图片，而且需要理解历史对话。新的任务意味着新的坑，文中给出了一些常见NN模型作为baseline，感兴趣的童鞋可以入坑。</p>
<h2 id="Neural-Machine-Translation-with-Latent-Semantic-of-Image-and-Text"><a href="#Neural-Machine-Translation-with-Latent-Semantic-of-Image-and-Text" class="headerlink" title="Neural Machine Translation with Latent Semantic of Image and Text "></a><a href="http://t.cn/RfjRQMP" target="_blank" rel="external">Neural Machine Translation with Latent Semantic of Image and Text </a></h2><p>【Visual NMT】就在NMT被讨论地如火如荼的时候，还有一部分工作是结合多模态（图片）来做机器翻译，因为人类获取信息不仅仅可以通过文字，图片也是一个重要的学习资源。</p>
<h2 id="Scalable-Bayesian-Learning-of-Recurrent-Neural-Networks-for-Language-Modeling"><a href="#Scalable-Bayesian-Learning-of-Recurrent-Neural-Networks-for-Language-Modeling" class="headerlink" title="Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling "></a><a href="http://t.cn/RfjELTY" target="_blank" rel="external">Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling </a></h2><p>【贝叶斯学习】通过BPTT来训练的RNN在解决问题上会存在过拟合的问题，一个主要原因是随机优化训练无法给出模型权重的概率估计，本文通过最近stochastic gradient Markov Chain Monte Carlo的研究来试着学习模型权重的概率。语言模型的实验结果和其他的相关实验结果表明本文所用的方法确实有效。</p>
<h2 id="Learning-Python-Code-Suggestion-with-a-Sparse-Pointer-Network"><a href="#Learning-Python-Code-Suggestion-with-a-Sparse-Pointer-Network" class="headerlink" title="Learning Python Code Suggestion with a Sparse Pointer Network "></a><a href="http://t.cn/RfjEjY5" target="_blank" rel="external">Learning Python Code Suggestion with a Sparse Pointer Network </a></h2><p>【代码补全】本文研究的问题非常有意思，就是大家常见的IDE代码补全功能。现有的IDE对静态编程语言支持的比较好，对于动态编程语言支持的一般，而且一般都是补全某个函数或者方法之类的，而不能给出更复杂的代码。本文针对这个问题，构造了一个大型的python code数据集，并且用了比较流行的Pointer Network模型来做端到端的训练，取得了不错的效果。代码补全在实际应用中非常有用，但想做到很复杂、很智能的补全还有很长的路。不过这个topic还是一个非常有意思的东西。</p>
<h2 id="Joint-Copying-and-Restricted-Generation-for-Paraphrase"><a href="#Joint-Copying-and-Restricted-Generation-for-Paraphrase" class="headerlink" title="Joint Copying and Restricted Generation for Paraphrase "></a><a href="http://t.cn/RfHAsim" target="_blank" rel="external">Joint Copying and Restricted Generation for Paraphrase </a></h2><p>【NLG】本文的思路与Pointer Network或者Copynet类似，在用seq2seq做自然语言生成时，增加一个判断的环节，来决定接下来的这个词是从source来copy还是用decoder来rewrite。</p>
<h2 id="Context-aware-Natural-Language-Generation-with-Recurrent-Neural-Networks"><a href="#Context-aware-Natural-Language-Generation-with-Recurrent-Neural-Networks" class="headerlink" title="Context-aware Natural Language Generation with Recurrent Neural Networks "></a><a href="http://t.cn/RfEClfC" target="_blank" rel="external">Context-aware Natural Language Generation with Recurrent Neural Networks </a></h2><p>【NLG】论文的方法、模型没有太多的值得说的地方，倒是应用的点非常有意思，根据商品的上下文来伪造评论，人工评判时有50%以上的伪造评论都通过了，90%以上骗过了现有的识别算法。有点道高一尺魔高一丈的感觉，如果这篇paper的结果确实这么牛的话，确实很有意思，值得研究一下。</p>
<h2 id="MS-MARCO-A-Human-Generated-MAchine-Reading-COmprehension-Dataset"><a href="#MS-MARCO-A-Human-Generated-MAchine-Reading-COmprehension-Dataset" class="headerlink" title="MS MARCO: A Human Generated MAchine Reading COmprehension Dataset "></a><a href="http://t.cn/RfH2eXu" target="_blank" rel="external">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset </a></h2><p>【机器阅读理解】【数据福利】微软放出了一个100k规模的机器阅读理解数据集，数据来源于真实的Bing搜索query。数据包括：query、10个相关的passage和query对应的answer。</p>
<h2 id="NewsQA-A-Machine-Comprehension-Dataset"><a href="#NewsQA-A-Machine-Comprehension-Dataset" class="headerlink" title="NewsQA: A Machine Comprehension Dataset "></a><a href="http://t.cn/Rf8MPnf" target="_blank" rel="external">NewsQA: A Machine Comprehension Dataset </a></h2><p>【机器阅读理解】【数据福利】Maluuba公司放出一个新的机器阅读理解的数据集，规模在100k左右，数据来源为CNN新闻。通过用多个之前表现比较好的NN模型和人工结果对比，发现F1指标存在25.3%的差距，说明本数据集需要更好的模型来进行研究。数据集已公开，地址是：<a href="http://datasets.maluuba.com/NewsQA" target="_blank" rel="external">http://datasets.maluuba.com/NewsQA</a></p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="中国人工智能学会通讯"><a href="#中国人工智能学会通讯" class="headerlink" title="中国人工智能学会通讯"></a><a href="http://t.cn/Rf8Yvwn" target="_blank" rel="external">中国人工智能学会通讯</a></h2><p>《中国人工智能学会通讯》，本期为学会优秀博士论文专刊</p>
<h2 id="C-wrapper"><a href="#C-wrapper" class="headerlink" title="C++ wrapper"></a><a href="http://t.cn/RfEIAdZ" target="_blank" rel="external">C++ wrapper</a></h2><p>TensorFlow使用swig作为C++ wrapper，最近Google又推出了pyclif，宣称“it’s much cleaner and easier” </p>
<h2 id="智能时代的自然语言处理"><a href="#智能时代的自然语言处理" class="headerlink" title="智能时代的自然语言处理"></a><a href="http://t.cn/Rfm4hJb" target="_blank" rel="external">智能时代的自然语言处理</a></h2><p>今天ADL前沿讲习班《智能时代的自然语言处理》Zhengdong Lu的报告，题目Recent Progress on Deep Learning for NLP。</p>
<h2 id="自然语言处理中深度学习活跃领域的课程讲义"><a href="#自然语言处理中深度学习活跃领域的课程讲义" class="headerlink" title="自然语言处理中深度学习活跃领域的课程讲义"></a><a href="http://www.zishu010.com/z/newdetail/9404521.html" target="_blank" rel="external">自然语言处理中深度学习活跃领域的课程讲义</a></h2><p>本文是纽约大学助理教授 Sam Bowman 关于自然语言处理中深度学习活跃领域的课程讲义PPT。对深度学习NLP领域最近较为活跃的研究进行了综述，其中包括Attention 模型、结构化记忆、词水平以上的无监督学习等等。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-G
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十六期</title>
    <link href="http://rsarxiv.github.io/2016/12/03/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%85%AD%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/03/PaperWeekly-第十六期/</id>
    <published>2016-12-03T18:08:45.000Z</published>
    <updated>2016-12-03T18:36:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本期PaperWeekly将带着大家来看一下ICLR 2017的六篇paper，其中包括当下非常火热的GAN在NLP中的应用，开放域聊天机器人如何生成更长更丰富的回答，如何用强化学习来构建树结构的神经网络和层次化的记忆网络等内容。六篇paper分别是：</p>
<p>1、A SELF-ATTENTIVE SENTENCE EMBEDDING<br>2、Adversarial Training Methods for Semi-Supervised Text Classification<br>3、GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS<br>4、Hierarchical Memory Networks<br>5、Mode Regularized Generative Adversarial Networks<br>6、Learning to compose words into sentences with reinforcement learning</p>
<h1 id="A-SELF-ATTENTIVE-SENTENCE-EMBEDDING"><a href="#A-SELF-ATTENTIVE-SENTENCE-EMBEDDING" class="headerlink" title="A SELF-ATTENTIVE SENTENCE EMBEDDING"></a><a href="http://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="external">A SELF-ATTENTIVE SENTENCE EMBEDDING</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou &amp; Yoshua Bengio</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>IBM Watson<br>Universit´e de Montr´eal</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>self-attention, sentence embedding, author profiling, sentiment classification, textual entailment</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>本文提出一种在没有额外输入的情况下如何利用attention来提高模型表现的句子表示方法。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>本文提出的模型结构分为两部分，</p>
<ol>
<li>BLSTM<br>这部分采用双向LSTM对输入的文本进行处理，最后得到BLSTM的所有隐层状态H。</li>
<li>Self-attention mechanism<br>同attention机制类似，我们需要计算一个权重向量a，然后通过对隐层状态H加权求和得到句子的表示向量。这个过程如下公式所示：<br><img src="media/equation1.png" alt="equation1"><br>但是实际任务中，我们通常可能会对一个句子语义的多个方面感兴趣，因此我们可以通过下面的公式，获得多个权重向量组成的矩阵A。<br><img src="media/equation2.png" alt="equation2"><br>然后每一个权重向量a都可以得到一个句子表示向量v，所有句子表示向量组合在一起就可以获得句子表示矩阵M。<br><img src="media/equation3.png" alt="equation3"><br>本文的模型在author profiling, sentiment classification和textual entailment三个任务上进行验证，都取得了较好的效果。</li>
</ol>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>1、[Yelp]<br>(<a href="https://www.yelp.com/dataset" target="_blank" rel="external">https://www.yelp.com/dataset</a> challenge)<br>2、 [SNLI]<br>(<a href="http://nlp.stanford.edu/projects/snli/" target="_blank" rel="external">http://nlp.stanford.edu/projects/snli/</a>)</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>A large annotated<br>corpus for learning natural language inference</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本文提出的self-attention方法用一个matrix表示一个句子，并且matrix中的每一个vector都是句子语义某一方面的表示，增强了sentence embedding的可解释性。</p>
<h1 id="Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification"><a href="#Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification" class="headerlink" title="Adversarial Training Methods for Semi-Supervised Text Classification"></a><a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="external">Adversarial Training Methods for Semi-Supervised Text Classification</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Takeru Miyato, Andrew M. Dai, Ian Goodfellow</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Google Brain, Kyoto University和OpenAI</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Adversarial training, text classification, semi-supervised learning</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>Adversarial training和virtual adversarial training都需要对输入的数字形式做小的perturbation，不适用于高维稀疏输入，比如one-hot word representations。文章扩展图像领域流行的这两种方法到文本领域，对word embedding进行perturbation来作为LSTM的输入，取代原本的输入向量。可以把这两种方法看做是正则化的方法，为输入加入噪声，可以用来实现semi-supervised的任务。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>以adversarial training为例，文章对word embeddings进行adversarial perturbation，而不是直接应用在输入上。假设normalized之后的输入序列为s，给定s，y的条件概率为p(y|s;theta)，其中theta为模型参数，则s上的adversarial perturbation r_adv为：<br><img src="media/16-1-1.png" alt="16-1"></p>
<p>应用在LSTM上，如下图(b)所示。定义其adversarial loss如下：</p>
<p><img src="media/adversarial1.png" alt="adversaria"></p>
<p><img src="media/16-2.png" alt="16-2"></p>
<p>其中N为labeled的例子的数目。通过随机梯度下降来进行training。</p>
<p>文章也提供了virtual adversarial training的方法。</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="external">IMDB</a><br>2、<a href="http://riejohnson.com/cnn_data.html" target="_blank" rel="external">Elec</a><br>3、<a href="http://snap.stanford.edu/data/web-Amazon.html" target="_blank" rel="external">Rotten Tomatoes</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>主要列三篇work：<br>1、2015年NIPS, SA-LSTM。Semi-supervised sequence learning<br>2、2015年NIPS，One-hot CNN。Semi-supervised convolutional neural networks for text categorization via region<br>embedding<br>3、2016年ICML，One-hot bi-LSTM。Supervised and semi-supervised text categorization using LSTM for region<br>embeddings</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>作者将图像领域的adversarial training应用在了文本领域，改善了word embedding。传统的word embedding被语法结构影响，即使两个完全相反的词（比如”good”和”bad”）在表示形式上也是相近的，没有表示出词本身的意思。Adversarial training使得有相近语法结构但是不同意义的词能够被分开，可以用来做情感分类和sequence model等。</p>
<h1 id="GENERATING-LONG-AND-DIVERSE-RESPONSES-WITH-NEURAL-CONVERSATION-MODELS"><a href="#GENERATING-LONG-AND-DIVERSE-RESPONSES-WITH-NEURAL-CONVERSATION-MODELS" class="headerlink" title="GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS"></a><a href="http://openreview.net/pdf?id=HJDdiT9gl" target="_blank" rel="external">GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray Kurzweil1</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>Google Research, Google Brain</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Long and Diverse Responses</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>开放域聊天机器人如何生成更长且较为丰富的回答？</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>本文模型是基于经典的seq2seq+attention框架，在其基础上进行了若干修改，得到了满意的效果。不同于之前模型的地方有两点：</p>
<p>1、encoder不仅仅包括整个source，还包括一部分target，这样attention不仅仅考虑了source，而且考虑了部分target。</p>
<p><img src="media/16-3.png" alt="16-3"></p>
<p>经典的seq2seq+attention在decoding部分会将source中的每个token都考虑到attention中来，之前有一种做法是将整个target部分也加入到attention中，效果上虽然有一定的提升，但随着数据规模地增加，内存代价太大。本文正是针对这一个问题，提出了所谓的“glimpse”模型，如上图所示，在encoder部分加入了target的前几个token，相当于是上面两种方案的一种折中。</p>
<p>2、提出了一种基于sampling的beam search decoding方案。</p>
<p>经典的beam search在decoding部分，是基于MAP（最大后验概率）进行贪婪解码的，这种方案生成的responses具有简短、无信息量以及高频的特点，通俗地讲会生成很多的类似“呵呵”的话，没有太多营养和价值。(Jiwei Li,2015)在解决这个问题时，在decoding部分通过MMI（互信息）对N-best结果进行重排序，这种方法对于生成短文本效果显著，但对于生成长文本效果不佳。因为，基于MAP的beam search天然存在这样的问题，N-best和重排序都解决不了根本性的问题。针对这一问题，本文提出了一种基于sampling的beam search解码方案，sampling即在每一步解码时都sample出D个token作为候选，搜索完毕或达到预设的长度之后，生成B个候选responses，然后进行重排序。</p>
<p>本文的另外一大亮点是用了大量的对话数据，用了很大规模参数的模型进行了实验。实验评价标准，在自动评价这部分，设计了一个N选1的实验，给定一个输入，将正确输出和错误输出混在一起，模型需要从中选择正确的输出，用选择准确率来作为自动评价指标。本文没有用到经典的BLEU指标，因为这个指标确实不适合评价对话的生成质量。为了更有说服力，本文用人工对结果进行评价。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>本文用到的对话数据：<br>1、<a href="https://redd.it/3bxlg7" target="_blank" rel="external">Reddit Data</a><br>2、<a href="http://opus.lingfil.uu.se/OpenSubtitles.php" target="_blank" rel="external">2009 Open Subtitles data</a><br>3、<a href="https://data.stackexchange.com/" target="_blank" rel="external">Stack Exchange data</a><br>4、本文作者从Web抽取的对话数据（待公开）</p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>用seq2seq方法研究生成对话的质量（包括长度、多样性）的工作并不多，具有代表性的有下面两个工作：<br>1、Wu,2016 提出了用length-normalization的方案来生成更长的对话<br>2、Jiwei Li,2015 提出了在解码阶段用MMI（互信息）对N-best结果进行重排序，旨在获得信息量更大的对话。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文模型部分并没有太多的创新，因为是工业部门的paper，所以更多的是考虑实用性，即能否在大规模数据集上应用该模型，集中体现在glimpse模型上。为了生成更加长、更加多样性的对话，在原有beam search + 重排序的基础上，引入了sampling机制，给生成过程增加了更多的可能性，也是工程上的trick。对话效果的评价是一件很难的事情，人类希望bot可以生成类人的对话，回复的长度可以定量描述，但多样性、生动性、拟人化等等都难以定量描述，所以在探索生成对话的这个方向上还有很长的路要走。</p>
<h1 id="Hierarchical-Memory-Networks"><a href="#Hierarchical-Memory-Networks" class="headerlink" title="Hierarchical Memory Networks"></a><a href="https://arxiv.org/pdf/1605.07427v1.pdf" target="_blank" rel="external">Hierarchical Memory Networks</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>1、Université de Montréal, Canada.<br>2、Twitter Cortex, USA.<br>3、IBM Watson Research Center, USA.<br>4、CIFAR, Canada.  </p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Hierarchical Memory Networks，Maximum Inner Product Search (MIPS)</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>记忆网络主要包括hard attention和soft attenion两种，然而hard不能用于反向传播算法进行端到端训练，所以只能使用强化学习的方法进行训练；soft所涉及的计算参数又很大，只适合于少量Memory。本文提出Hierarchical Memory Networks(HMN)模型，算是soft和hard的一个混合模型，计算量减少且训练更加容易，<br>实验结果也很好。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>soft attention是对所有的memory都要进行attention的计算，对全集计算使计算量很大。HMN利用层次化结构使得attention的集合缩小，利用MaximumInner Product Search(MIPS)的方法从全集中获得一个最优子集，在子集上面去做attention就大大降低计算量。这样的方式又和hard attention预测关注点的方法有些类似，将注意力放在最相关的那部分，这个的做法也更接近于人的注意力思维。  文章的核心部分在于如何获取与query最相近的子集。</p>
<p>主实验主要包括两个:<br>1、Exact K-MIPS：计算复杂度依然和soft attention差不多。<br>2、Approximate K-MIPS：利用Maximum Cosine Similarity Search(MCSS)的方法代替MIPS的方法，牺牲一些精确度，降低复杂度和加快训练速度。  </p>
<p>MIPS有三种方法，分别是基于hash,基于tree,基于clustering，基于上述三种方法文中又做了几组组对比实验，最后实验结果显示基于clustering的效果是最好的。</p>
<p>文章得到的实验结果如下：<br><img src="media/HMN_Result.png" alt="HMN_Result"></p>
<h2 id="资源-（可选）"><a href="#资源-（可选）" class="headerlink" title="资源 （可选）"></a>资源 （可选）</h2><p>1、<a href="https://www.dropbox.com/s/tohrsllcfy7rch4/SimpleQuestions_v2.tgz" target="_blank" rel="external">The SimpleQuestions dataset</a>(使用的是Large-scale simple question answering with memory networks文章中的数据集)<br>2、<a href="https://research.facebook.com/research/babi/" target="_blank" rel="external">babi</a></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、arXiv 2014, soft attention,《Neural turing machines》<br>2、CoRR 2015, hard attention,《Reinforcement learning neural turing machine》<br>3、ICLR 2015, memory network,《Memory networks》<br>4、arXiv 2015,《End-to-end memory networks》,引入半监督记忆网络可以自学所需要的facts。<br>5、CoRR 2016, DMN, 《Dynamic memory networks for visual and textual question<br>answering》,增加了一个episodic memory 使得可以动态更新memory里面的内容。</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>文章的创新主要在于修改了两个模块：Memory和Reader。<br>1、将memory的结构从a flat of array变成了hierarchical memory structure。将memory分成若干groups,这些groups又可以在进行更高级别的组合。<br>2、reader是从MIPS选出的子集中使用soft attention。MIPS从memory中选出一<br>个group子集作为最相关的子集。</p>
<h1 id="Mode-Regularized-Generative-Adversarial-Networks"><a href="#Mode-Regularized-Generative-Adversarial-Networks" class="headerlink" title="Mode Regularized Generative Adversarial Networks "></a><a href="http://openreview.net/pdf?id=HJKkY35le" target="_blank" rel="external">Mode Regularized Generative Adversarial Networks </a></h1><h2 id="作者-4"><a href="#作者-4" class="headerlink" title="作者"></a>作者</h2><p>Tong Che; Yanran Li</p>
<h2 id="单位-4"><a href="#单位-4" class="headerlink" title="单位"></a>单位</h2><p>Montreal Institute for Learning Algorithms;<br>Department of Computing, The Hong Kong Polytechnic University</p>
<h2 id="关键词-4"><a href="#关键词-4" class="headerlink" title="关键词"></a>关键词</h2><p>GAN, Regularizers</p>
<h2 id="文章来源-4"><a href="#文章来源-4" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h2><p>本文针对的问题是：1、GAN 的训练过程很不稳定 2、GAN 生成的样本局限于训练样本中的大 model 上，不能平衡数据的分布（missing model problem）。<br>两个问题互相影响，导致训练结果不好。</p>
<h2 id="模型-4"><a href="#模型-4" class="headerlink" title="模型"></a>模型</h2><p>针对上面的问题，作者提出了两种 regularizers 去控制 GAN 的训练过程。<br>第一个 regularizer 也被作者称为 Regularized-GAN。作者认为可以从 generator 入手，给 generator 增加 regularizer，使得其具有更好的 gradient ，这样 G 和 D 都能稳定训练。<br>具体的方法是增加一个 encoder E(x) : X → Z.即把原先的 noise vector z 改为 z = encoder(X) ，即然后再 G(encoder(X))。如下图：<br><img src="media/16-5.png" alt="16-5"></p>
<p>这样做有两个好处。第一，原始的模型很容易出现梯度消失的情况，因为 discriminator D 特别容易区分真实数据和生成数据导致 generator 就得不到 D 的梯度。作者的模型多了一个 reconstruction 的部分，这样生成出来数据不再那样容易被 D 识别出来。所以 D 和 G 就都能一直有 gradient 去训练，从而提高稳定性。第二，对于 x ，G(E(x)) 会尽量去生成 x 原本所属的类，从而一定程度解决了 missing model problem。<br>第二个 regularizer 基于第一个 regularizer 旨在改进训练的方法，也被作者称为 manifold-diffusion GAN。分为两步，第一步 manifold step 训练 discriminator D1 ，目的是减少 G(Enc(X)) 和 X 的的差别；第二步 diffusion 就是训练 D2 让 G(Enc(X)) 和 G(z) 分布的距离接近。如下图：</p>
<p><img src="media/16-6.png" alt="16-6"></p>
<p>最后，作者把 GAN 的网络训练坍塌的情况考虑进去，提出了新的 evaluation metric。</p>
<h2 id="相关工作-4"><a href="#相关工作-4" class="headerlink" title="相关工作"></a>相关工作</h2><p>本篇文章的作者李嫣然写过一篇非常棒的<a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;mid=2650325352&amp;idx=1&amp;sn=90fb15cee44fa7175a804418259d352e&amp;mpshare=1&amp;scene=1&amp;srcid=0829ixEhnGChNKAl5kgz6b9V#rd" target="_blank" rel="external">综述</a> ,在这里就不累赘阐述了。</p>
<h2 id="简评-4"><a href="#简评-4" class="headerlink" title="简评"></a>简评</h2><p>当下 GAN 的研究非常火爆，出现了许许多多对 GAN 的改进，本篇文章的提出的两种 regularizers 非常有效的提高了 GAN 的稳定性（其中 regularizer 的思想也受到了监督学习的启发），值得对 GAN 感兴趣的同学研读。</p>
<h2 id="完成人信息"><a href="#完成人信息" class="headerlink" title="完成人信息"></a>完成人信息</h2><p>professorshui@gmail.com</p>
<h1 id="Learning-to-compose-words-into-sentences-with-reinforcement-learning"><a href="#Learning-to-compose-words-into-sentences-with-reinforcement-learning" class="headerlink" title="Learning to compose words into sentences with reinforcement learning"></a><a href="https://openreview.net/forum?id=Skvgqgqxe" target="_blank" rel="external">Learning to compose words into sentences with reinforcement learning</a></h1><h2 id="作者-5"><a href="#作者-5" class="headerlink" title="作者"></a>作者</h2><p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling</p>
<h2 id="单位-5"><a href="#单位-5" class="headerlink" title="单位"></a>单位</h2><p>Google</p>
<h2 id="关键词-5"><a href="#关键词-5" class="headerlink" title="关键词"></a>关键词</h2><p>Tree-LSTM, Reinforcement Learning</p>
<h2 id="文章来源-5"><a href="#文章来源-5" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-5"><a href="#问题-5" class="headerlink" title="问题"></a>问题</h2><p>使用强化学习来构建树结构的神经网络Tree-LSTM，学习自然语言的句子表示</p>
<h2 id="模型-5"><a href="#模型-5" class="headerlink" title="模型"></a>模型</h2><p>模型分为两部分：Tree-LSTM和强化学习模型<br>应用Tree-LSTM(可以通过LSTM的忘记门机制，跳过整棵对结果影响不大的子树)，并结合{SHIFT，REDUCE}操作，SHIFT操作对应将一个节点压入栈，REDUCE对应将两个元素组合，从而建立树结构</p>
<p>强化学习用来寻找最佳的节点组合情况，RL模型中的状态s即当前构建的树结构，a为{SHIFT，REDUCE}操作，reward对应不同downstream<br> task(例：若是用该句子表示进行分类任务，则r对应从策略网络中采样得到句子表示的分类准确性的概率)</p>
<h2 id="资源-3"><a href="#资源-3" class="headerlink" title="资源"></a>资源</h2><p>作者将该工作进行了四组实验，情感分类，语义相关性判断，自然语言推理，句子生成<br>分别应用Stanford Sentiment Treebank，Sentences Involving Compositional Knowledge corpus，Stanford Natural Language Inference corpus，IMDB movie review corpus</p>
<h2 id="相关工作-5"><a href="#相关工作-5" class="headerlink" title="相关工作"></a>相关工作</h2><p>与Socher等人之前提出的Recursive NN,MV-RNN,RNTN，Tree-LSTM等工作一脉相承，本文又加入了RL方式构建树形结构</p>
<h2 id="简评-5"><a href="#简评-5" class="headerlink" title="简评"></a>简评</h2><p>将强化学习引入句子表示学习之中，学习构建树的不同方式，从左向右，从右向左，双向，有监督、半监督、预先无结构等方式去构建树结构，但是训练时间较长，在几个任务上效果提升不是特别明显。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>GAN是当下的研究热点之一，在图像领域中研究较多，本期的两篇paper探讨了GAN在NLP中的应用，值得关注和期待。最后感谢@destinwang、@gcyydxf、@chunhualiu、@tonya、@suhui和@zhangjun六位童鞋的辛勤工作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;本期PaperWeekly将带着大家来看一下ICLR 2017的六篇paper，其中包括当下非常火热的GAN在NLP中的应用，开放域聊天机器
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>中文分词工具测评</title>
    <link href="http://rsarxiv.github.io/2016/11/29/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E6%B5%8B%E8%AF%84/"/>
    <id>http://rsarxiv.github.io/2016/11/29/中文分词工具测评/</id>
    <published>2016-11-29T17:49:50.000Z</published>
    <updated>2016-11-29T19:07:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>分词对于研究和应用中文自然语言处理的童鞋来说，都是一个非常非常基础的部件，分词的质量直接影响到后续词性标注、命名实体识别、句法分析等部件的准确性。作为一个基础部件，学术界对分词的研究已经非常久了，市面上流行的几大开源分词工具也被工业界的各大公司应用很多年了。最近，中文分词随着一篇博文的发表被推到了风口浪尖，引发众多大牛在微博、微信群里的激烈讨论。本文并不想对这篇博文进行过多评论，只是想用公开的数据集对各大分词工具进行一个客观地测评，以供大家在选择工具时有所依据。</p>
<h1 id="中文分词工具"><a href="#中文分词工具" class="headerlink" title="中文分词工具"></a>中文分词工具</h1><p>本文选择了4个常见的分词工具，分别是：哈工大LTP、中科院计算所NLPIR、清华大学THULAC和jieba，为了对比分词速度，选择了这四个工具的c++版本进行评测。</p>
<p>1、LTP <a href="https://github.com/HIT-SCIR/ltp" target="_blank" rel="external">https://github.com/HIT-SCIR/ltp</a><br>2、NLPIR <a href="https://github.com/NLPIR-team/NLPIR" target="_blank" rel="external">https://github.com/NLPIR-team/NLPIR</a><br>3、THULAC <a href="https://github.com/thunlp/THULAC" target="_blank" rel="external">https://github.com/thunlp/THULAC</a><br>4、jieba <a href="https://github.com/yanyiwu/cppjieba" target="_blank" rel="external">https://github.com/yanyiwu/cppjieba</a></p>
<h1 id="测试数据集"><a href="#测试数据集" class="headerlink" title="测试数据集"></a>测试数据集</h1><p>1、SIGHAN Bakeoff 2005 MSR, 560KB  <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/</a><br>2、SIGHAN Bakeoff 2005 PKU, 510KB  <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/</a><br>3、人民日报 2014, 65MB  <a href="https://pan.baidu.com/s/1hq3KKXe" target="_blank" rel="external">https://pan.baidu.com/s/1hq3KKXe</a></p>
<p>前两个数据集是SIGHAN于2005年组织的中文分词比赛所用的数据集，也是学术界测试分词工具的标准数据集，本文用于测试各大分词工具的准确性，而最后一个数据集规模较大，用于测试分词速度。</p>
<h1 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h1><p>用SIGHAN Bakeoff 2005比赛中所自带的score脚本、test gold数据和training words数据对4个工具进行准确性测试，具体使用方法可参考：<a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip</a> 中的readme文件。</p>
<h1 id="测试硬件"><a href="#测试硬件" class="headerlink" title="测试硬件"></a>测试硬件</h1><p>Intel Core i7-6700 CPU@3.40GHz*8</p>
<h1 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h1><p>1、MSR测试结果<br><img src="media/1.png" alt="1"></p>
<p>2、PKU测试结果<br><img src="media/2.png" alt="2"></p>
<p>3、人民日报测试结果<br><img src="media/3.png" alt="3"></p>
<h1 id="测试结论"><a href="#测试结论" class="headerlink" title="测试结论"></a>测试结论</h1><p>1、一个好的分词工具不应该只能在一个数据集上得到不错的指标，而应该在各个数据集都有很不错的表现。从这一点来看，thulac和ltp都表现非常不错。</p>
<p>2、因为分词是个基础部件，分词速度对于一个分词工具来说也至关重要。从这一点来看，thulac和jieba表现的不错。</p>
<p>3、大家都知道，基本的分词依赖模型，但真正想用分词工具来解决应用层面上的问题，都需要借助于词库，本文测试的4个工具均支持用户自定义词库。</p>
<p>4、特别需要强调的一点是，哈工大的ltp支持分词模型的在线训练，即在系统自带模型的基础上可以不断地增加训练数据，来得到更加丰富、更加个性化的分词模型。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>争论是一个好的事情，尤其是不同背景的人站在不同的角度对同一个事情进行争论，常常会碰撞出知识的火花，对于这个领域的发展有更好地推动作用。希望类似的争论可以多一些，让刚刚入门的或者准备入门的童鞋可以更加客观地看到一个领域的发展现状，而不是盲目地被一些热门的词蒙蔽双眼，失去判断。对于分词来说，最近几年大热的深度学习模型，并不会比之前传统的crf模型有多大性能上的突破，所以大家应该理性地看待深度学习以及人工智能，捧得越高可能摔得越惨。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1、Zhongguo Li, Maosong Sun. Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics, vol. 35, no. 4, pp. 505-512, 2009.<br>2、Meishan Zhang, Yue Zhang, Guohong Fu. Transition-Based Neural Word Segmentation<br><a href="http://www.aclweb.org/anthology/P/P16/P16-1040.pdf" target="_blank" rel="external">http://www.aclweb.org/anthology/P/P16/P16-1040.pdf</a><br>3、Meishan Zhang, Zhilong Deng，Wanxiang Che, and Ting Liu. Combining Statistical Model and Dictionary for Domain Adaption of Chinese Word Segmentation. Journal of Chinese Information Processing. 2012, 26 (2) : 8-12 (in Chinese)<br>4、Wanxiang Che, Zhenghua Li, and Ting Liu. LTP: A Chinese Language Technology Platform. In Proceedings of the Coling 2010:Demonstrations. 2010.08, pp13-16, Beijing, China.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;分词对于研究和应用中文自然语言处理的童鞋来说，都是一个非常非常基础的部件，分词的质量直接影响到后续词性标注、命名实体识别、句法分析等部件的准
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>本周值得读(2016.11.21-2016.11.25)</title>
    <link href="http://rsarxiv.github.io/2016/11/26/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-11-21-2016-11-25/"/>
    <id>http://rsarxiv.github.io/2016/11/26/本周值得读-2016-11-21-2016-11-25/</id>
    <published>2016-11-27T06:12:34.000Z</published>
    <updated>2016-11-27T06:31:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Generative-Deep-Neural-Networks-for-Dialogue-A-Short-Review"><a href="#Generative-Deep-Neural-Networks-for-Dialogue-A-Short-Review" class="headerlink" title="Generative Deep Neural Networks for Dialogue: A Short Review"></a><a href="http://t.cn/RfX2bms" target="_blank" rel="external">Generative Deep Neural Networks for Dialogue: A Short Review</a></h2><p>【对话系统】本文对seq2seq方法在对话系统中的应用做了一个简短的对比和综述，主要是针对几位作者提出的三种深度学习模型：HRED、VHRED和MrRNN，实验数据用了Ubuntu Dialogue Corpus和Twitter Corpus。不管是用seq2seq生成也好，还是套用模板也罢，对话系统的难点仍是上下文的理解和如何输出一些高质量的对话，有些应用场景对response的要求没那么高，只要可以达到一定实际效果即可，而有的则需要生成更加接近人类的对话。本文适合研究深度seq2seq的童鞋以及想看看各种seq2seq效果如何的童鞋来读。本文总结的三个模型原文链接：</p>
<p>(a) MrRNN: <a href="https://arxiv.org/pdf/1606.00776.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1606.00776.pdf</a><br>(b) VHRED: <a href="https://arxiv.org/pdf/1605.06069v3.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1605.06069v3.pdf</a><br>(c) HRED: <a href="https://arxiv.org/pdf/1507.04808v3.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1507.04808v3.pdf</a></p>
<h2 id="Coherent-Dialogue-with-Attention-based-Language-Models"><a href="#Coherent-Dialogue-with-Attention-based-Language-Models" class="headerlink" title="Coherent Dialogue with Attention-based Language Models"></a><a href="http://t.cn/Rfag1Jx" target="_blank" rel="external">Coherent Dialogue with Attention-based Language Models</a></h2><p>【对话系统】考虑并理解上下文是Chatbot的一大难点，也是目前绝大多数chatbot不智能的主要原因之一。本文提出了一种动态的attention模型，在理解用户请求的时候，动态地考虑历史信息。本文用到了两个开放数据集，分别是MovieTriples和Ubuntu Troubleshoot dataset。建议对chatbot感兴趣的同学可以精读此文。</p>
<h2 id="Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks"><a href="#Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks" class="headerlink" title="Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks"></a><a href="http://t.cn/RfXLHIP" target="_blank" rel="external">Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks</a></h2><p>【课程学习】Curriculum Learning是一类模拟小孩子学习过程的学习算法，简单地说是指在训练模型是从简单的样本开始，逐渐增加学习样本的难度。本文以情感分析为研究对象，对Curriculum Learning如何提升LSTM模型在情感分析任务上的效果进行了实验研究，并给出了可视化的结果。本文适合研究Curriculum Learning的童鞋以及在训练模型中想尝试下Curriculum Learning思路的童鞋研读。</p>
<h2 id="Variable-Computation-in-Recurrent-Neural-Networks"><a href="#Variable-Computation-in-Recurrent-Neural-Networks" class="headerlink" title="Variable Computation in Recurrent Neural Networks"></a><a href="http://t.cn/RfXUmyN" target="_blank" rel="external">Variable Computation in Recurrent Neural Networks</a></h2><p>【RNN研究】RNN在解决序列建模问题有着天然的优势，但有些序列数据存在周期性的变化，或者短时间内变化并不明显，比如视频数据，因此固定不变的RNN训练方案会浪费计算资源，本文针对这一问题，提出了一种RNN变计算训练方案，即在计算下一个time step的hidden state时，不需要上一个time step所有的维度，只取一部分来计算，其他的维度复制过来即可。这篇工作的相关的前人研究包括：2014年的A Clockwork RNN，链接如下：<a href="https://arxiv.org/pdf/1402.3511.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1402.3511.pdf</a></p>
<h2 id="Learning-to-Distill-The-Essence-Vector-Modeling-Framework"><a href="#Learning-to-Distill-The-Essence-Vector-Modeling-Framework" class="headerlink" title="Learning to Distill: The Essence Vector Modeling Framework"></a><a href="http://t.cn/RfoWk0K" target="_blank" rel="external">Learning to Distill: The Essence Vector Modeling Framework</a></h2><p>【表示学习】本文研究的内容包括两个点，一个是无监督学习，一个是文档表示。词表示、句子表示都有比较多的解决方案，但实际应用中文档级别的表示非常重要，比如情感分析、文本摘要等任务。本文提出了一种无监督的方法对文档以及背后所蕴藏的背景知识进行低维表示。自然语言随着元素级别地提升（从字、词、短语、句子到文档），研究的难度随之增加，实用程度随之减少。建议想从无监督学习方法有所突破以及想试试文档表示的童鞋可以来读本文。</p>
<h2 id="Unsupervised-Learning-of-Sentence-Representations-using-Convolutional-Neural-Networks"><a href="#Unsupervised-Learning-of-Sentence-Representations-using-Convolutional-Neural-Networks" class="headerlink" title="Unsupervised Learning of Sentence Representations using Convolutional Neural Networks"></a><a href="http://t.cn/Rf9VNdk" target="_blank" rel="external">Unsupervised Learning of Sentence Representations using Convolutional Neural Networks</a></h2><p>【句子表示】本文的贡献在于提出了一种新的CNN-LSTM auto-encoder，作为一种无监督的句子学习模型。</p>
<h2 id="Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers"><a href="#Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers" class="headerlink" title="Emergent Logical Structure in Vector Representations of Neural Readers"></a><a href="http://t.cn/Rf9Vwi7" target="_blank" rel="external">Emergent Logical Structure in Vector Representations of Neural Readers</a></h2><p>【问答系统】针对最近提出的各种各样的attention based reader models,本文作者做了一个比较全面的总结和分析，并且通过数学分析和实验展示了模型之间的相关性。PaperWeekly第十四期的文章有相关的paper note可以参考<a href="http://rsarxiv.github.io/2016/11/19/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%9C%9F/">地址</a></p>
<h1 id="公益广告"><a href="#公益广告" class="headerlink" title="公益广告"></a>公益广告</h1><p>美国国立卫生研究院招博士后，研究领域包括：NLP、text mining和machine learning，感兴趣的童鞋可以看过来，详情请戳<a href="https://www.stat.washington.edu/jobs/archive/2013/may/05.20.13_NIH_E_B_NLP_Post_Doc_Ad_PDF_May_20_2013.pdf" target="_blank" rel="external">这里</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Generative-Deep-Neural-Networks-for-Dialogue-A-Short-
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十五期</title>
    <link href="http://rsarxiv.github.io/2016/11/26/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%BA%94%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/26/PaperWeekly-第十五期/</id>
    <published>2016-11-26T18:37:08.000Z</published>
    <updated>2016-11-26T18:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>NMT是热门研究领域之一，尤其是Google和百度都推出了自己的NMT翻译系统，在工业界、学术界和翻译界都引起了轩然大波，一时间对NMT技术的研究和讨论达到了顶峰。Attention模型在NLP中最早的使用正是在NMT领域出现的，包括横扫很多领域的seq2seq+attention解决方案，都是在NMT模型的基础上进行相应的一些小改动而成的。所以，本期PaperWeekly带大家看一看最近两年Attention模型在NMT领域中的研究进展，本文包括以下paper：</p>
<p>1、Neural Machine Translation by Jointly Learning to Align and Translate, 2015<br>2、Effective approaches to attention-based neural machine translation, 2015<br>3、Modeling Coverage for Neural Machine Translation,  2016<br>4、Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation, 2016<br>5、Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation, 2016</p>
<h1 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate" class="headerlink" title="Neural Machine Translation by Jointly Learning to Align and Translate"></a><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Jacobs University Bremen, Germany<br><br>Universite ́ de Montre ́al</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>NMT, attention</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2015</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这篇论文首次提出在NMT中使用attention的机制，可以使模型自动确定源句子中和目标词语最相关的部分，相比于基本的encoder-decoder方法提高了翻译效果。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>该论文使用的基本模型是一个双向RNN的encoder-decoder的结构。在这篇论文之前，encoder部分都是直接把输入句子encode成一个固定长度的上下文向量c，然后decoder再根据该向量来产生翻译。但是由于句子长度不定，这种做法对长句子的效果不理想。<br><img src="media/model.png" alt="mode"></p>
<p>上图是这篇论文提出的模型结构，作者首次提出了在decoder中加入一种attention的机制。直观上理解，就是decoder可以决定更多地注意原句子中的某些部分，从而不必把原句子中的所有信息都encode成一个固定的向量。具体来讲，上下文向量ci由下式计算得出：<br><img src="media/ci.png" alt="ci"></p>
<p>其中，<br><img src="media/aij.png" alt="aij"></p>
<p>其中，<br><img src="media/eij.png" alt="eij"></p>
<p>上式中的a便是alignment model，可以用来估计位置j附近的输入和位置i的输出之间的匹配程度。本论文中的alignment model是一个前馈神经网络，它和模型中的其它部分一起进行训练。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>1、英法翻译数据集 <a href="http://www.statmt.org/wmt14/translation-task.html" target="_blank" rel="external">ACL WMT ’14</a></p>
<p>2、一个基本的RNN encoder-decoder模型的实现 <a href="https://github.com/lisa-groundhog/GroundHog." target="_blank" rel="external">GroundHog</a></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、2013年，一个类似的aligning的方法被提出用于手写体生成。论文：Graves(2013) Generating sequences with recurrent neural networks<br>2、2014年，seq2seq的神经网络模型用于机器翻译。论文：Sutskever(2014) Sequence to sequence learning with neural networks</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本论文创新性地在NMT中提出了attention的机制，可以使模型在每一步注意到源句子中不同的部分，从而提高了NMT的效果，该效果的提升对于长句子的翻译尤其明显。</p>
<h1 id="Effective-approaches-to-attention-based-neural-machine-translation"><a href="#Effective-approaches-to-attention-based-neural-machine-translation" class="headerlink" title="Effective approaches to attention-based neural machine translation"></a><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="external">Effective approaches to attention-based neural machine translation</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Minh-Thang Luong, Hieu Pham, Christopher D. Manning</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Computer Science Department, Stanford University</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>NMT;Global Attention;Local Attention</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>EMNLP 2015</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>Attention机制引入极大提升了NMT的翻译质量，但对于Attention实现架构的讨论还很少，尤其是全局Attention的计算效率问题。本文就是讨论各种优化策略，包括Global Attention, Local Attention，Input-feeding方法等。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>Global Attenion，生成上下文向量c_t时，考虑原文编码过程中的所有隐状态。<br>  <img src="media/1GlobalAttention.png" alt="1GlobalAttention"></p>
<p>Local Attention，对于每个正在生成的译词，预测一个原文对齐的位置，只考虑该位置前后一个窗口范围内的原文编码隐状态。  </p>
<p><img src="media/2LocalAttention.png" alt="2LocalAttention"></p>
<p>Input-feeding，用一个额外的向量，来记住哪些词是已经翻译过的，即考虑了coverage的问题。  </p>
<p><img src="media/3Input-Feeding.png" alt="3Input-Feeding"></p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、训练数据：WMT14 (4.5M句对，116M 英文词，110M德文词)<br>2、开发集：newstest2013 (3000句)<br>3、测试集：newstest2014(2737句)和newstest2015(2169句)<br>4、代码和模型共享在：<a href="http://nlp.stanford.edu/projects/nmt/" target="_blank" rel="external">http://nlp.stanford.edu/projects/nmt/</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>主要是follow了(Bahdanau et al., 2015; Jean et al., 2015)的工作，对Attention的机制进行了探讨和改进。  </p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>English-German的实验结果，较不用attention的方法提升了5个多点BLEU，充分证明了attention的有效性。<br>实验结果的表格详细列出了各种改进方法带来的收益，跟进者不妨仔细看看（以及第5节的分析），可以很快了解各种折腾的方向。</p>
<p><img src="media/4ExperimentResult.png" alt="4ExperimentResult"></p>
<h2 id="完成人信息"><a href="#完成人信息" class="headerlink" title="完成人信息"></a>完成人信息</h2><p>微博 @MyGod9，语智云帆创始人，机器翻译老兵，NMT追随者，weiyongpeng@lingosail.com </p>
<h1 id="Modeling-Coverage-for-Neural-Machine-Translation"><a href="#Modeling-Coverage-for-Neural-Machine-Translation" class="headerlink" title="Modeling Coverage for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1601.04811v6.pdf" target="_blank" rel="external">Modeling Coverage for Neural Machine Translation</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>诺亚方舟实验室，清华大学</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>NMT</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL2016</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>解决经典神经机器翻译模型中存在的over-translation（过度翻译）和under-translation(翻译不足）的问题。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>在传统NMT模型中，加入统计机器翻译策略中的coverage方法，来追踪、判断原始句子是否被翻译，如下图、公式所示。<br><img src="media/pic1.png" alt="pi"><br><img src="media/pic2.png" alt="pi"><br><img src="media/pic3.png" alt="pi"><br>其中，C为新引入的coverage向量。</p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>前序文章：Neural Machine Translation by Jointly Learning to Align and Translate</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>该文是基于Neural Machine Translation by Jointly Learning to Align and Translate之上的工作，引入了统计机器翻译中的Coverage方法来尝试避免NMT中的一些问题。根据文章的试验结果，这种方法能够提升翻译效果。由于写作此文时笔者未作实验，因此实际效果有待进一步衡量。</p>
<h1 id="Agreement-based-Joint-Training-for-Bidirectional-Attention-based-Neural-Machine-Translation"><a href="#Agreement-based-Joint-Training-for-Bidirectional-Attention-based-Neural-Machine-Translation" class="headerlink" title="Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation"></a><a href="https://arxiv.org/abs/1512.04650" target="_blank" rel="external">Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>Tsinghua University</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Bidirectional NMT; Attention</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>IJCAI 2016</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>由于自然语言错综复杂的结构，单向的注意力模型只能引入注意力机制的部分regulization。文章提出了联合训练双向的注意力模型，尽可能使注意力在两个方向上保持一致。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>模型的中心思想就是对于相同的training data，使source-to-target和target-to-source两个模型在alignment matrices上保持一致。这样能够去掉一些注意力噪声，使注意力更加集中、准确。更确切地说，作者引入了一个新的目标函数：</p>
<p><img src="media/1.png" alt="1"></p>
<p>其中<br><img src="media/2.png" alt="2">表示source-to-target基于注意力的翻译模型，而<img src="media/3.png" alt="3">表示target-to-source的模型。<img src="media/4.png" alt="4">表示对于句子s source-to-target的alignment matrix，而<img src="media/5.png" alt="5">表示target-to-source的。<img src="media/6.png" alt="6">是损失函数，可以衡量两个alignment matrix之间的disagree程度。</p>
<p>对于<img src="media/6.png" alt="6">,有几种不同的定义方法：<br>1、Square of addition(SOA)<br><img src="media/7.png" alt="7"></p>
<p>2、Square of subtraction(SOS)<br><img src="media/9.png" alt="9"></p>
<p>3、Multiplication(MUL)<br><img src="media/10.png" alt="10"></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p>作者文中说的是bidirectional translation的alignment matrices要一致；还有另外一篇文章“Agreement on Target-bidirectional Neural Machine Translation”是说decoding的时候可以正向或者反向产生目标句子，把这二者进行联合训练。另外，最近也有很多关于bidirectional training或者类似思想的文章，比如“Dual Learning for Machine Translation. Computation and Language”将reinforcement的概念引入了bidirectional training当中，“Neural Machine Translation with Reconstruction” 希望能从target hidden state恢复出source sentence</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>这篇文章胜在idea,很巧妙地想到了让正反向的注意力一致来改进attention。</p>
<h1 id="Improving-Attention-Modeling-with-Implicit-Distortion-and-Fertility-for-Machine-Translation"><a href="#Improving-Attention-Modeling-with-Implicit-Distortion-and-Fertility-for-Machine-Translation" class="headerlink" title="Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation"></a><a href="https://arxiv.org/abs/1601.03317" target="_blank" rel="external">Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation</a></h1><h2 id="作者-4"><a href="#作者-4" class="headerlink" title="作者"></a>作者</h2><p>Shi Feng, Shujie Liu, Nan Yang, Mu Li, Ming Zhou, Kenny Q.Zhu</p>
<h2 id="单位-4"><a href="#单位-4" class="headerlink" title="单位"></a>单位</h2><p>Shanghai Jiao Tong University, Microsoft Research</p>
<h2 id="关键词-4"><a href="#关键词-4" class="headerlink" title="关键词"></a>关键词</h2><p>NMT, Attention, Fertility, Distortion</p>
<h2 id="文章来源-4"><a href="#文章来源-4" class="headerlink" title="文章来源"></a>文章来源</h2><p>COLING 2016</p>
<h2 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h2><p>使用attention机制解决NMT中调序和繁衍率的问题。</p>
<h2 id="模型-4"><a href="#模型-4" class="headerlink" title="模型"></a>模型</h2><p>模型非常简单，即在attention机制中将前一时刻的context vector c作为输入传入当前时刻attention中（命名为RecAtt）。如图：</p>
<p><img src="media/coling.jpg" alt="coling"></p>
<p>通过这样的RecAtt机制，attention部分的网络相当于记忆了之前时刻的context。</p>
<h2 id="相关工作-4"><a href="#相关工作-4" class="headerlink" title="相关工作"></a>相关工作</h2><p>ACL 2016李航老师组的工作 Modeling Coverage for Neural Machine Translation利用了attention机制来解决了NMT中“欠翻译”和“过翻译”的问题。</p>
<h2 id="简评-4"><a href="#简评-4" class="headerlink" title="简评"></a>简评</h2><p>该文章的创新之处在于提出将attention计算得到的context vector c作为attention的输入，这样就是的attention机制带有一种recurrent的意味。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本期PaperWeekly精选了5篇Attention模型在NMT任务上的研究工作，Attention模型的发展不仅仅推动着NMT的进步，同时也可以借鉴于其他的任务中，比如QA，比如chatbot。感谢@MyGod9 @雨神 @susie-nmt @李争 @magic282 五位童鞋的辛勤付出。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;NMT是热门研究领域之一，尤其是Google和百度都推出了自己的NMT翻译系统，在工业界、学术界和翻译界都引起了轩然大波，一时间对NMT技术
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>悉尼科技大学博士后招聘信息</title>
    <link href="http://rsarxiv.github.io/2016/11/19/%E6%82%89%E5%B0%BC%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E5%90%8E%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/"/>
    <id>http://rsarxiv.github.io/2016/11/19/悉尼科技大学博士后招聘信息/</id>
    <published>2016-11-20T04:47:55.000Z</published>
    <updated>2016-11-20T04:49:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, I am recruiting a Postdoc in Machine Learning for two (2) years. In brief, the candidate should:</p>
<p>1 Hold a PhD in machine learning and have a good research track record.</p>
<p>2 Have a genuine interest in mathematical modeling.</p>
<p>3 Good communication skills and is willing to help PhD students resolving their mathematical issues.</p>
<p>4 Excellent in programming and experimentation.</p>
<p>The candidate will work with Dr Richard Xu (Yida.Xu@uts.edu.au), where his team’s recent research themes include: Bayesian Non-Parametric (BNP), Monte-Carlo inference, Matrix (and Tensor) factorization and Deep Learning. The application areas include both computer vision and document. There is no strict requirement that the candidate must align his/her research exactly to Richard’s existing work, i.e., the candidate can continue to work in his/her established field as long as the group benefit from his/her presence.</p>
<p>Please contact Richard to obtain further information, and remember to check out his website:</p>
<p><a href="http://www-staff.it.uts.edu.au/~ydxu/" target="_blank" rel="external">http://www-staff.it.uts.edu.au/~ydxu/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi, I am recruiting a Postdoc in Machine Learning for two (2) years. In brief, the candidate should:&lt;/p&gt;
&lt;p&gt;1 Hold a PhD in machine learn
    
    </summary>
    
    
      <category term="招聘" scheme="http://rsarxiv.github.io/tags/%E6%8B%9B%E8%81%98/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.11.14-2016.11.18</title>
    <link href="http://rsarxiv.github.io/2016/11/19/cs-CL-weekly-2016-11-14-2016-11-18/"/>
    <id>http://rsarxiv.github.io/2016/11/19/cs-CL-weekly-2016-11-14-2016-11-18/</id>
    <published>2016-11-20T04:30:55.000Z</published>
    <updated>2016-11-20T04:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="UTCNN-a-Deep-Learning-Model-of-Stance-Classificationon-on-Social-Media-Text"><a href="#UTCNN-a-Deep-Learning-Model-of-Stance-Classificationon-on-Social-Media-Text" class="headerlink" title="UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text"></a><a href="http://t.cn/RfGR4GE" target="_blank" rel="external">UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text</a></h2><p>【文本分类】【社交网络】社交网络中蕴藏着大量的非结构化的文本，对社交网络的挖掘也是一块重要的研究内容。本文研究内容为立场分类，创新之处在于做分类时不仅仅考虑该文本信息本身，而且考虑了与该文本相关的评论、反馈、用户信息、话题等各种文本信息。模型部分没有太多的新意，是经典的CNN。本文适合做社交网络文本挖掘的童鞋来读。</p>
<h2 id="A-Way-out-of-the-Odyssey-Analyzing-and-Combining-Recent-Insights-for-LSTMs"><a href="#A-Way-out-of-the-Odyssey-Analyzing-and-Combining-Recent-Insights-for-LSTMs" class="headerlink" title="A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs"></a><a href="http://t.cn/RfVSmk7" target="_blank" rel="external">A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs</a></h2><p>【文本分类】RNN及其扩展LSTM在文本分类中得到了广泛的应用，本文探究了多种LSTM的小变种对分类结果的影响，一些小改变对结果还是有一定影响的。本文适合工程上用LSTM解决文本分类问题的童鞋精读。</p>
<h2 id="Linguistically-Regularized-LSTMs-for-Sentiment-Classification"><a href="#Linguistically-Regularized-LSTMs-for-Sentiment-Classification" class="headerlink" title="Linguistically Regularized LSTMs for Sentiment Classification"></a><a href="http://t.cn/Rf56bpv" target="_blank" rel="external">Linguistically Regularized LSTMs for Sentiment Classification</a></h2><p>【情感分析】本文最大的亮点在于将语言学资源，比如情感词典，否定词，表示程度的词等等以约束条件的形式融入到了现有的句子级别的LSTM分类模型中，取得了不错的效果。深度学习火起来之后，大家都推崇数据驱动的模型，希望找到一种简单粗糙的解决方案，而忽视了经典的自然语言资源和语言学的知识。经典的这些资源都是非常宝贵的东西，如何将这些知识融入到现有的深度学习模型中，是个很难但却非常有意义的事情，本文在句子级别的情感分类任务中做了相关的探索。推荐研究情感分析的童鞋精读。</p>
<h2 id="Google’s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation"><a href="#Google’s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation" class="headerlink" title="Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"></a><a href="http://t.cn/Rf5I4nw" target="_blank" rel="external">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a></h2><p>【机器翻译】Google NMT系统支持Zero-Shot翻译，从训练集充足的A-&gt;B，B-&gt;C两个翻译模型中可以推出一个质量不错的A-&gt;C模型，A和C并不需要很充足的训练集。</p>
<h2 id="Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot"><a href="#Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot" class="headerlink" title="Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot"></a><a href="http://t.cn/Rf5IB9P" target="_blank" rel="external">Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot</a></h2><p>【多模态】【机器翻译】现在的人工智能还达不到很高的智能水平，但是处理或者理解一些稍微初级的东西可能还行，比如3岁孩子的故事之类的。之前萌生一个想法，能不能针对小孩学习，比如学习英语，传统的方法可能是给一句中文，教一句英语，感觉用到的信息量还是少，能不能一边看图，一边学习英语，相当于用到了图像这个信息，孩子在学习的过程中会多一些信息维度，学习效果可能会更好一些。本文正是做了这么一件事情，借助图像作为机器翻译的桥梁。</p>
<h2 id="Neural-Machine-Translation-with-Pivot-Languages"><a href="#Neural-Machine-Translation-with-Pivot-Languages" class="headerlink" title="Neural Machine Translation with Pivot Languages"></a><a href="http://t.cn/RftFqja" target="_blank" rel="external">Neural Machine Translation with Pivot Languages</a></h2><p>【机器翻译】很多语言的机器翻译都面临着一个语言对数据集匮乏的问题，一个比较直观的思路是，用一种常见的语言作为“桥梁”，连接起两种语言对数据集匮乏的语言。昨天Google的zero-shot也正是这么一种思路，本文也在这方面进行了研究工作，即想做A-&gt;C的翻译，需要拿一个热门语言B作为桥梁，构造一个A-&gt;B,B-&gt;C的联合训练模型，本文用英语作为B，用德语、法语、西班牙语分别作为A和C进行了两组实验，验证了模型的有效性。这种“串行”seq2seq的思路，其实可以尝试一些其他的任务，做一些seq2seq2seq…的模型出来。</p>
<h2 id="Joint-Representation-Learning-of-Text-and-Knowledge-for-Knowledge-Graph-Completion"><a href="#Joint-Representation-Learning-of-Text-and-Knowledge-for-Knowledge-Graph-Completion" class="headerlink" title="Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion"></a><a href="http://t.cn/Rf5J12U" target="_blank" rel="external">Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion</a></h2><p>【知识表示】“联合学习”是个热门词，“联合学习”可以避免一些语言分析过程（比如：句法依存分析）带来的误差。本文在学习词、实体和关系表示时同时用到了text和知识图谱信息，得到了不错的效果。</p>
<h2 id="Multi-lingual-Knowledge-Graph-Embeddings-for-Cross-lingual-Knowledge-Alignment"><a href="#Multi-lingual-Knowledge-Graph-Embeddings-for-Cross-lingual-Knowledge-Alignment" class="headerlink" title="Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment"></a><a href="http://t.cn/Rf5XVuD" target="_blank" rel="external">Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment</a></h2><p>【知识图谱】一个新模型来填坑，Trans系列的新成员——MTransE </p>
<h2 id="End-to-End-Neural-Sentence-Ordering-Using-Pointer-Network"><a href="#End-to-End-Neural-Sentence-Ordering-Using-Pointer-Network" class="headerlink" title="End-to-End Neural Sentence Ordering Using Pointer Network"></a><a href="http://t.cn/RftkYoF" target="_blank" rel="external">End-to-End Neural Sentence Ordering Using Pointer Network</a></h2><p>【句子排序】【文本摘要】句子排序是一项重要的基本工作，尤其是在做单文档和多文档文本抽取式摘要时显得特别重要。经典的排序方法几乎都是考虑单个句子所包含的信息进行排序，忽略了句子的上下文信息。本文工作借鉴了Pointer Network的思路，提出了一种端到端排序模型，在排序时考虑句子的上下文。通过两组实验验证了本文算法的有效性。机器翻译中的seq2seq+attention已经成功的应用在了很多任务上，但针对具体任务不同的特点进行针对性地修正会带来比较理想的结果。建议研究文本摘要的童鞋读本文。</p>
<h2 id="The-Amazing-Mysteries-of-the-Gutter-Drawing-Inferences-Between-Panels-in-Comic-Book-Narratives"><a href="#The-Amazing-Mysteries-of-the-Gutter-Drawing-Inferences-Between-Panels-in-Comic-Book-Narratives" class="headerlink" title="The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives"></a><a href="http://t.cn/RfVoHOF" target="_blank" rel="external">The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives</a></h2><p>【问答系统】基于上下文的问答已经有很多数据集了，基于图像的问答也有一些数据集了。漫画是一类大家小时候都喜欢的读物，包含了丰富的图像和文本数据（对话）。本文给出了一个大型数据集，包括了丰富的图像和文本，规模在120万（120GB）左右。数据给出了几个任务，基于图像的问答任务，基于对话文本的问答任务和文本排序任务。对问答感兴趣，想找一些新数据来刷一刷榜的童鞋可以看过来。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="Highlights-of-EMNLP-2016-Dialogue-deep-learning-and-more"><a href="#Highlights-of-EMNLP-2016-Dialogue-deep-learning-and-more" class="headerlink" title="Highlights of EMNLP 2016: Dialogue, deep learning, and more"></a><a href="http://blog.aylien.com/highlights-emnlp-2016-dialogue-deeplearning-and-more/" target="_blank" rel="external">Highlights of EMNLP 2016: Dialogue, deep learning, and more</a></h2><p>NLP技术服务公司Aylien写的EMNLP 2016总结</p>
<h1 id="一句话公益广告"><a href="#一句话公益广告" class="headerlink" title="一句话公益广告"></a>一句话公益广告</h1><p>悉尼科技大学Dr Richard Xu招机器学习博士后，感兴趣的童鞋看过来。具体信息请点阅读原文<a href="http://rsarxiv.github.io/2016/11/19/%E6%82%89%E5%B0%BC%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E5%90%8E%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/">查看链接</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;UTCNN-a-Deep-Learning-Model-of-Stance-Classificationo
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十四期</title>
    <link href="http://rsarxiv.github.io/2016/11/19/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/19/PaperWeekly-第十四期/</id>
    <published>2016-11-19T17:58:02.000Z</published>
    <updated>2016-11-19T18:34:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>PaperWeekly已经介绍过不少Question Answering的相关工作。主要有DeepMind Attentive Reader，FAIR Memory Networks，Danqi’s Stanford Reader, Attention Sum Reader, Gated Attention Sum Reader, Attention Over Attention Reader, etc. 这些模型关联性很大，或多或少存在相似之处。本文给大家介绍一下Toyota Technological Institute at Chicago (TTIC)在Question Answering方面的相关工作，共有3篇paper：</p>
<p>1、Who did What: A Large-Scale Person-Centered Cloze Dataset, 2016<br>2、Broad Context Language Modeling as Reading Comprehension, 2016<br>3、Emergent Logical Structure in Vector Representations of Neural Readers, 2016</p>
<h1 id="Who-did-What-A-Large-Scale-Person-Centered-Cloze-Dataset"><a href="#Who-did-What-A-Large-Scale-Person-Centered-Cloze-Dataset" class="headerlink" title="Who did What: A Large-Scale Person-Centered Cloze Dataset"></a><a href="https://tticnlp.github.io/who_did_what/" target="_blank" rel="external">Who did What: A Large-Scale Person-Centered Cloze Dataset</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David McAllester</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>EMNLP 2016</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>文章构建了一个新的Question Answering dataset，”Who did What”。</p>
<p>sample instance如下图所示。<br><img src="media/example.png" alt="example"></p>
<p>问题的句子总是挖掉了一些named entities，然后给出在文中出现过的别的named entities作为选项。这一个dataset的难度要高于之前的CNN/DM dataset，可以作为创建新模型的参考数据集。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>构建此数据集的方法与CNN/DM不同，问题并不是context passge的一个summary。问题与context均来自Gigaword Corpus，他们是两篇非常相关的文章。</p>
<p>具体来说，我们先找到一篇文章，作为question文章。然后提取出文中第一句话的named entities，删除其中的一个named entity作为将要被预测的答案。然后利用这一句question sentence，我们可以利用一些Information Retrieval系统从Gigaword Corpus找到一篇相关的文章作为passage。这篇文章与question文章不同，但是包含着与question sentence非常类似的信息。</p>
<p>有了passage之后，我们再从passage中找出named entities作为candidate answers。</p>
<p>为了使任务难度更大，我们用一些简单的baseline (First person in passage, etc) 将一些很容易做出的问题删掉，只留下比较困难的instances。这样构建的数据比CNN/DM会困难不少。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>相信作者创建的新数据集会给Machine comprehension带来一些新的问题与挑战，是很有价值的资源。文章采用的baseline suppresion方法可以用比较小的代价加大问题的难度，值得参考。</p>
<h1 id="Broad-Context-Language-Modeling-as-Reading-Comprehension"><a href="#Broad-Context-Language-Modeling-as-Reading-Comprehension" class="headerlink" title="Broad Context Language Modeling as Reading Comprehension"></a><a href="https://arxiv.org/abs/1610.08431" target="_blank" rel="external">Broad Context Language Modeling as Reading Comprehension</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>不久前发布的<a href="https://arxiv.org/abs/1606.06031" target="_blank" rel="external">LAMBADA dataset</a>中，作者尝试的各种baseline models都给出了比较差的结果。</p>
<p>每一个LAMBADA instance如下图所示。</p>
<p><img src="media/LAMBADA.png" alt="LAMBADA"></p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>在观察了LAMBADA dataset之后，我们认为可以利用Reading comprehension models来提升准确率，而不必使用传统的language model。</p>
<p>由于state of the art reading comprehension models需要给出candidate answers，然后从中选出一个作为预测的答案，我们就将所有在context中出现过的单词都作为一个candidate answer。</p>
<p>LAMBADA给出的训练集是一些小说的文本。为了使训练集与测试集的数据类型保持一致，我们构建了一个biased training set。具体的做法是，我们将training set划分成4-5句话的context，然后保证target word在context passage中出现，只保留这样的训练数据。我们在新构建的training set上训练各种attention based models,得到了比原作者好得多的测试结果。</p>
<p><img src="media/zewei-results.png" alt="zewei-results"></p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>这篇文章中，作者利用了简单的方法和模型将LAMBADA dataset的准确率从7.3%提高到45.4%，非常简单有效。</p>
<h1 id="Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers"><a href="#Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers" class="headerlink" title="Emergent Logical Structure in Vector Representations of Neural Readers"></a><a href="http://openreview.net/pdf?id=ryWKREqxx" target="_blank" rel="external">Emergent Logical Structure in Vector Representations of Neural Readers</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017 Submission</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>最近提出的各种各样的attention based reader models,本文作者做了一个比较全面的总结和分析，并且通过数学分析和实验展示了模型之间的相关性。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>本文作者认为，当前的attention based models可以分为两类，aggregation readers(包括attentive readers和stanford readers)以及explicit reference readers(包括attention sum reader和gated attention sum reader)。</p>
<p>这两种reader可以用如下的公式联系在一起。</p>
<p><img src="media/formula1.png" alt="formula1"></p>
<p>要满足上述等式，只需要满足下面的公式。</p>
<p><img src="media/formula2.png" alt="formula2"></p>
<p>也就是说，只有正确答案所在的hidden vector和question vector得到的inner product才能给出不为零的常数。以下实验结论支持了这一假设。</p>
<p><img src="media/experiment.png" alt="experiment"></p>
<p>由于CNN/DM在训练和测试中经过了anonymization，作者认为此inner product其实可以分为两部分，一部分与anonymized token ID有关，另一部分与ID无关。与ID相关的那一部分在inner product应该直接给出0的答案。如下述公式所示。</p>
<p><img src="media/formula3.png" alt="formula3"></p>
<p>本文的另一部分工作是在attention readers上加入一些linguistic features提升各个数据集的准确读，这里不仔细描述。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文是对于各个attetion based neural reader models很好的总结，它很好地连接了各个不同的model，说明了为何看似不同的model能够给出非常类似的结果。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>问答系统是一类大的问题，也是目前NLP应用的研究热点之一。本文作者介绍了TTIC在QA研究中的一些成果，其中第二篇是本文作者近期的paper。感谢来自芝加哥大学的@ZeweiChu童鞋辛勤的劳动。</p>
<h1 id="公益广告"><a href="#公益广告" class="headerlink" title="公益广告"></a>公益广告</h1><p>清华大学计算机系自然语言处理实验室招聘博士后</p>
<h2 id="将从事的研究方向"><a href="#将从事的研究方向" class="headerlink" title="将从事的研究方向"></a>将从事的研究方向</h2><p>围绕自然语言处理、语义分析、统计机器翻译或社会计算开展深入的研究工作。实验室具体信息见：<a href="http://nlp.csai.tsinghua.edu.cn" target="_blank" rel="external">http://nlp.csai.tsinghua.edu.cn</a></p>
<h2 id="应聘条件"><a href="#应聘条件" class="headerlink" title="应聘条件"></a>应聘条件</h2><p>1、具有计算机科学技术或相关学科博士学位（博士毕业两年内）；<br>2、熟悉自然语言处理或机器学习的基本理论、模型与算法，曾在国内外重要学术刊物或重要国际会议（CCF A类）上发表（含已录用）高水平学术论文；<br>3、在句法分析、语义分析方面有较好研究基础者优先；<br>4、具有较强的编程能力及项目研发能力；<br>5、责任心强，具有较好的团队合作精神和创新意识，英语阅读及写作能力较强；<br>6、符合清华大学博士后招收条件。</p>
<h2 id="工资待遇"><a href="#工资待遇" class="headerlink" title="工资待遇"></a>工资待遇</h2><p>享受清华大学博士后待遇及课题组津贴。全力支持申请国家自然科学基金、全国博士后管委会、北京市、清华大学的相关研究计划。</p>
<h2 id="申请材料"><a href="#申请材料" class="headerlink" title="申请材料"></a>申请材料</h2><p>1、个人简历、学位证书及成绩单复印件；<br>2、最具代表性的论文2篇；<br>3、博士后期间研究设想（简明扼要）；<br>4、其它任何支持材料。</p>
<h2 id="导师及联系方式"><a href="#导师及联系方式" class="headerlink" title="导师及联系方式"></a>导师及联系方式</h2><p>合作导师：孙茂松教授、刘知远助理教授<br>联系人：刘知远<br>电子邮件： liuzy@tsinghua.edu.cn</p>
<p>有意者请将申请材料发至电子邮箱，请在邮件主题中注明姓名和“申请博士后”。材料通过初选者进行面谈（面谈时间另行通知），然后走清华大学博士后申请程序。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;PaperWeekly已经介绍过不少Question Answering的相关工作。主要有DeepMind Attentive Reader
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.11.07-2016.11.11</title>
    <link href="http://rsarxiv.github.io/2016/11/13/cs-CL-weekly-2016-11-07-2016-11-11/"/>
    <id>http://rsarxiv.github.io/2016/11/13/cs-CL-weekly-2016-11-07-2016-11-11/</id>
    <published>2016-11-13T19:10:19.000Z</published>
    <updated>2016-11-13T19:23:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Learning-Recurrent-Span-Representations-for-Extractive-Question-Answering"><a href="#Learning-Recurrent-Span-Representations-for-Extractive-Question-Answering" class="headerlink" title="Learning Recurrent Span Representations for Extractive Question Answering"></a><a href="https://arxiv.org/pdf/1611.01436v1.pdf" target="_blank" rel="external">Learning Recurrent Span Representations for Extractive Question Answering</a></h2><p>【机器阅读】不同的阅读理解数据集产生答案的方式不同，有的是给定N个候选答案，有的是规定从原文中的entity中进行选择，有的是从原文中的任意token进行选择等等。本文所用的数据集是SQuAD，候选答案是原文中的任意字符串，难度较大，答案可能是一个词或者几个词都有可能。本文在前人研究的基础上提出了一种显式表示answer span的模型，取得了不错的效果。</p>
<h2 id="Answering-Complicated-Question-Intents-Expressed-in-Decomposed-Question-Sequences"><a href="#Answering-Complicated-Question-Intents-Expressed-in-Decomposed-Question-Sequences" class="headerlink" title="Answering Complicated Question Intents Expressed in Decomposed Question Sequences"></a><a href="https://arxiv.org/pdf/1611.01242v1.pdf" target="_blank" rel="external">Answering Complicated Question Intents Expressed in Decomposed Question Sequences</a></h2><p>【复杂问答】基于语义分析的问答系统最近流行于解决长、难问题，本文研究的内容是如何处理多个相互关联的简单问题？（即将复杂问题分解成多个相关简答问题）并给出了一个任务数据集。这个问题的一大难点在于相互关联的问题需要共指消解的工作。本文将单轮问答对话分解成多轮问题过程，上下文的处理非常重要。建议研究聊天机器人的童鞋来精读此文。</p>
<h2 id="Unsupervised-Pretraining-for-Sequence-to-Sequence-Learning"><a href="#Unsupervised-Pretraining-for-Sequence-to-Sequence-Learning" class="headerlink" title="Unsupervised Pretraining for Sequence to Sequence Learning"></a><a href="https://arxiv.org/pdf/1611.02683v1.pdf" target="_blank" rel="external">Unsupervised Pretraining for Sequence to Sequence Learning</a></h2><p>【seq2seq】【文本摘要】seq2seq是一种效果非常不错的框架，尤其是输入-输出数据非常充分的时候。但很多语言翻译问题并不能拿到非常多的训练数据，效果就会打折扣。本文针对这一问题，在原有seq2seq框架上提出了一种小改动。 encoder和decoder的初始值用训练好的语言模型来赋值，用可以获得的少量训练数据对模型进行训练和调优，本文方法的效果在机器翻译任务和abstractive式摘要任务中得到了验证。从本文中也可以看出，一个好的初值不仅可以使得训练更快，而且可以得到更好的结果。本文适合用seq2seq解决工程问题的童鞋读。</p>
<h2 id="Sentence-Ordering-using-Recurrent-Neural-Networks"><a href="#Sentence-Ordering-using-Recurrent-Neural-Networks" class="headerlink" title="Sentence Ordering using Recurrent Neural Networks"></a><a href="https://arxiv.org/pdf/1611.02654v1.pdf" target="_blank" rel="external">Sentence Ordering using Recurrent Neural Networks</a></h2><p>【句子排序】【文本摘要】句子排序任务对于研究文档的连贯性非常有意义，而连贯性对于很多任务非常重要，比如文本摘要。本文在这个任务上用了流行的seq2seq方法，并且给出了一种可视化的句子表示效果。建议研究摘要的童鞋读。</p>
<h2 id="Modeling-Coverage-for-Neural-Machine-Translation"><a href="#Modeling-Coverage-for-Neural-Machine-Translation" class="headerlink" title="Modeling Coverage for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1601.04811v6.pdf" target="_blank" rel="external">Modeling Coverage for Neural Machine Translation</a></h2><p>【机器翻译】针对神经网络机器翻译（NMT）译文中经常出现的遗漏翻译（under-translation）和过度翻译（over-translation）问题，华为诺亚方舟实验室首次提出对覆盖率（coverage）进行建模。该方法的主要思想是为每个源端词维护一个coverage vector以表示该词被翻译（或覆盖）的程度。在解码过程中该覆盖率信息会传入attention model，以使它更关注于未被翻译的源端词，实验表示该方法能显著减少遗漏翻译和过度翻译错误数量，该工作发表在ACL 2016上。</p>
<h2 id="Efficient-Summarization-with-Read-Again-and-Copy-Mechanism"><a href="#Efficient-Summarization-with-Read-Again-and-Copy-Mechanism" class="headerlink" title="Efficient Summarization with Read-Again and Copy Mechanism"></a><a href="https://arxiv.org/pdf/1611.03382.pdf" target="_blank" rel="external">Efficient Summarization with Read-Again and Copy Mechanism</a></h2><p>【文本摘要】本文适合研究文本摘要，尤其是用seq2seq来解决句子级摘要的童鞋进行研读。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="刘知远老师在将门的talk"><a href="#刘知远老师在将门的talk" class="headerlink" title="刘知远老师在将门的talk"></a><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/index_cn.html" target="_blank" rel="external">刘知远老师在将门的talk</a></h2><p>对“表示学习和知识获取”感兴趣的童鞋可以看过来。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Learning-Recurrent-Span-Representations-for-Extractiv
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十三期</title>
    <link href="http://rsarxiv.github.io/2016/11/10/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B8%89%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/10/PaperWeekly-第十三期/</id>
    <published>2016-11-11T02:50:42.000Z</published>
    <updated>2016-11-11T04:21:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本期的PaperWeekly一共分享四篇最近arXiv上放出的高质量paper，包括：机器翻译、表示学习、推荐系统和聊天机器人。人工智能及其相关研究日新月异，本文将带着大家了解一下以上四个研究方向都有哪些最新进展。四篇paper分别是：</p>
<p>1、A General Framework for Content-enhanced Network Representation Learning, 2016.10</p>
<p>2、Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks, 2016.11</p>
<p>3、Dual Learning for Machine Translation, 2016.11</p>
<p>4、Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems, 2016.10</p>
<h1 id="A-General-Framework-for-Content-enhanced-Network-Representation-Learning"><a href="#A-General-Framework-for-Content-enhanced-Network-Representation-Learning" class="headerlink" title="A General Framework for Content-enhanced Network Representation Learning"></a><a href="https://arxiv.org/abs/1610.02906" target="_blank" rel="external">A General Framework for Content-enhanced Network Representation Learning</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Xiaofei Sun, Jiang Guo, Xiao Ding and Ting Liu</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>network representation, content-enhanced</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>同时利用网络结构特征和文本特征来学习网络中节点的embedding</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>总的来说这篇paper的思路比较清晰，学习的方法上很大程度上参考了word2vec的方法。对于一个节点v，将与v相连的节点当做正例，不想连的节点当做负例。那么如何融入内容呢？在网络中设置虚拟的内容节点c，将描述v节点的文本内容c_v当做正例，其他的当做负例c_v’。在优化时同时考虑网络相似性和文本相似性，让v的向量靠近正例远离负例。</p>
<p><img src="media/network-illustration.png" alt="network-illustration"></p>
<p>总的优化函数如下所示，由两个部分L_nn(节点与节点连接)和L_nc(节点与内容连接)线性组合而成，alpha越大则考虑网络结构越多文本内容越少。</p>
<p><img src="media/joint-learning.png" alt="joint-learning"></p>
<p>L_nn和L_nc大体思想如上面所言，两者损失函数一致，尽量接近正例远离反例。但是两者在描述节点概率（相似度）上会有所不同。</p>
<p><img src="media/node-node-link.png" alt="node-node-link"></p>
<p>对于节点与节点之间的概率，由于网络结构要考虑有向性，因此将节点的embedding切分成in和out两半，用sigmoid算两个节点的相似度。</p>
<p><img src="media/node-node-probability.png" alt="node-node-probability"></p>
<p>节点与内容的概率也是类似，不过内容节点的embedding是固定的，通过额外的文本模型训练出来的。这里尝试的文本model包括word2vec，RNN和BiRNN。</p>
<p><img src="media/node-content-probability.png" alt="node-content-probability"></p>
<p>最后在节点分类任务上进行了评测，同时结合网络结构特征和文本特征确实带来了明显的提高。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>用到的数据集是DBLP（cn.aminer.org/citation）和自己采集的知乎用户网络。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>这两年network representation的工作如雨后春笋，在DeepWalk之后有十余篇论文出现。这篇文章在相关工作里有相对全面的覆盖，对这方面工作有兴趣的同学值得参考。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>尽管相关模型层出迭见，但略感遗憾的是感觉目前并没有在network embedding之上的较为成功的应用，大多benchmark都是节点分类和链接预测，应用价值有限。十分期待一些更为新颖的benchmark的出现。</p>
<h1 id="Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks"><a href="#Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks" class="headerlink" title="Collaborative Recurrent Autoencoder Recommend while Learning to Fill in the Blanks"></a><a href="https://arxiv.org/pdf/1611.00454v1.pdf" target="_blank" rel="external">Collaborative Recurrent Autoencoder Recommend while Learning to Fill in the Blanks</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Hao Wang, Xingjian Shi, Dit-Yan Yeung</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>HKUST</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Recommendation, Collaborative Filtering, RNN</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>Arxiv, to appear at NIPS’16</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>本文的主要贡献是提出collaborative recurrent autoencoder (CRAE)，将CF (collaborative filtering)跟RNN结合在一起，提高推荐的准确率，并且可以用于sequence generation task。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>传统的LSTM模型没有考虑进噪声，对不足的训练数据稳定性不好，文章提出RRN (robust recurrent networks)，为RNN的加噪版本，RRN中的噪声直接在网络中向前或者向后传播，不需要分开的网络来估计latent variables的分布，更容易实现且效率高。CARE的模型如下图所示，序列处理的信息保存在cell state s_t和输出状态h_t中，两个RRN可以组合形成编码译码结构。</p>
<p><img src="media/collaborative1.png" alt="collaborative1"></p>
<p>Wildcard denoising的目的是缓解overfitting，做法是随机选择一些词，替换成<wildcard>，而不是直接扔掉词，实验验证准确率会提成20%左右。Beta-pooling的目的是将向量序列pool成固定长度为2K_W的单向量，帮助rating matrix的矩阵分解；因为不同序列可能需要不同大小的权重，所以需要变长的beta向量来帮助pooling，文章采用beta分布。</wildcard></p>
<p>Learning的过程采用MAP，类似于CDL和DTR。学到矩阵U和V之后，我们可以预计评分矩阵R。</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、<a href="http://www.citeulike.org/faq/data.adp" target="_blank" rel="external">CiteULike</a><br>2、<a href="http://www.wanghao.in/" target="_blank" rel="external">Netflix</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>选取当中两个比较有意思的work。<br>1、CTR (collaborative topic reguression)<br>将topic model和probabilistic matrix factorization (PMF)，但是CTR采用bag-of-words的表示形式，忽略了词序和每个词的局部语境，而这些对文章表示和word embeddings能提供有价值的信息。<br>2、CDL (collaborative deep learning)<br>将CF和probabilistic stacked denoising autoencoder (SDAE)结合起来，是一个以bag-of-words为输入的feedforward模型，并不能解决sequence generation的问题。</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>这篇文章将RNN用于recommendation，并且与rating matrix结合起来，比较有意思，而且考虑了数据稀疏的情况，pooling的方法也值得借鉴。</p>
<h1 id="Dual-Learning-for-Machine-Translation"><a href="#Dual-Learning-for-Machine-Translation" class="headerlink" title="Dual Learning for Machine Translation"></a><a href="https://arxiv.org/pdf/1611.00179.pdf" target="_blank" rel="external">Dual Learning for Machine Translation</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Yingce Xia1, Di He, Tao Qin, Liwei Wang, Nenghai Yu1, Tie-Yan Liu, Wei-Ying Ma</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>1.University of Science and Technology of China<br>2.Key Laboratory of Machine Perception (MOE), School of EECS, Peking University<br>3.Microsoft Research</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Dual Learning, Machine Translation, Deep Reinforcement Learning</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 1 Nov 2016 </p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>文章针对机器翻译时需要的人工标注的双语平行语料获取代价高的问题，提出了Dual Learning Model使用单语语料来进行训练，取得了比使用双语平行语料训练的模型更好的结果。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>模型的核心思想见下图：<br><img src="media/Dual_Learning.png" alt="Dual_Learning"></p>
<p>(注:上图来自CCL2016马维英老师PPT)<br>对上图的详细解释：<br>模型中有两个Agent，Agengt_A和Agent_B,Agent_A只能够理解A语言，Agent_B只能理解B语言，model f是将A语言翻译成B语言的翻译模型，model f是将B语言翻译成A语言的翻译模<br>型。上图的执行过程可以按照下面的解释进行：<br>1、Agent_A 发送一句A语言的自然语言的话X1<br>2、model f将X转换成为B语言的自然语言Y<br>3、Agent_B收到Y，并将Y 传送给model g<br>4、model g将Y转换成源语言A的自然语言X2<br>5、比较X1和X2的差异性，并给出反馈.并进行1到4的反复训练</p>
<p>模型的算法过程：<br><img src="media/Dual_Learning_Algorithm.png" alt="Dual_Learning_Algorith"></p>
<p>在step8的时候对翻译模型翻译的结果使用语言模型做了一个判定，判定一个句子在多大程度上是自然语言。step9是给communication一个reward，step10将step8和step9加权共同作为样例的reward.然后使用policy gradient进行优化。<br>需要说明的model f和model g是已有的模型或者说在刚开始的时候使用少量的双语语料进行训练得到吗，然后逐渐加大单语语料的比例。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>NMT code:<a href="https://github.com/nyu-dl" target="_blank" rel="external">https://github.com/nyu-dl</a><br>compute BLEU score by the multi-bleu.perl:<a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl" target="_blank" rel="external">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl</a></p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、the standard NMT, Neural machine translation by jointly learning to align<br>and translate. ICLR, 2015.<br>2、pseudo-NMT, Improving neural machine translation models with monolingual data. In ACL, 2016.</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文的思想很创新，利用了机器翻译中的dual mechinism，仅仅利用少部分双语语料和大部分单语语料就可以达到之前NMT的效果，甚至还高了2到3个百分点。<br>dual的思想不仅可以用于机器翻译中，还可以用于图片、语音、文字等多种语言的共同学习，这样的相互作用共同学习更接近于人类对周围世界认识的方式，接受来自各个方面的信心，综合进行学习。</p>
<h1 id="Two-are-Better-than-One-An-Ensemble-of-Retrieval-and-Generation-Based-Dialog"><a href="#Two-are-Better-than-One-An-Ensemble-of-Retrieval-and-Generation-Based-Dialog" class="headerlink" title="Two are Better than One: An Ensemble of Retrieval and Generation-Based Dialog"></a><a href="https://arxiv.org/pdf/1610.07149v1.pdf" target="_blank" rel="external">Two are Better than One: An Ensemble of Retrieval and Generation-Based Dialog</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, Ming Zhang</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>北京大学</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>对话系统、open domain、chatbot</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>对话系统中可将问题和检索的结果同时作为输入Encoder之后进行解码Decoder，再将生成的结果和原检索结果重排序</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p><img src="media/14788352072241.jpg" alt=""><br><img src="media/14788352189655.jpg" alt=""></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p><img src="media/14788352485111.jpg" alt=""><br><img src="media/14788352559281.jpg" alt=""></p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>作者的思路非常简单，原来的回复生成模型容易发生回复内容短或者回复信息无意义的问题，在此作者将候选结果和原来的问句同时作为RNN生成器的输入，生成结果后再将本次生成的结果加入原检索候选集中，进行重新排序，实验结果证明此种方法比单独使用检索或单独使用生成效果有大幅提升。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>新的研究成果不见得可以直接应用于工程中，但新的paper，尤其是高质量paper中，一定会有很多的创新点，每一个创新点都可能会为后续的研究、工程实现等带来启发，甚至是一些技术上的突破。从本期开始，PaperWeekly会不定期地分享类似的内容，以方便大家了解最新的研究成果。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;本期的PaperWeekly一共分享四篇最近arXiv上放出的高质量paper，包括：机器翻译、表示学习、推荐系统和聊天机器人。人工智能及其
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.10.31-2016.11.04</title>
    <link href="http://rsarxiv.github.io/2016/11/05/cs-CL-weekly-2016-10-31-2016-11-04/"/>
    <id>http://rsarxiv.github.io/2016/11/05/cs-CL-weekly-2016-10-31-2016-11-04/</id>
    <published>2016-11-05T16:40:05.000Z</published>
    <updated>2016-11-05T16:52:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Neural-Machine-Translation-in-Linear-Time"><a href="#Neural-Machine-Translation-in-Linear-Time" class="headerlink" title="Neural Machine Translation in Linear Time"></a><a href="https://arxiv.org/pdf/1610.10099v1.pdf" target="_blank" rel="external">Neural Machine Translation in Linear Time</a></h2><p>【机器翻译】本文提出了一种新的encoder-decoder模型ByteNet。它是由两个扩张（dilated）卷积神经网络堆叠起来的。ByteNet的优势在于时间复杂度是线性的。工作来自deepmind。建议研究机器翻译以及使用MT模型做其他任务的童鞋精读。</p>
<h2 id="Dual-Learning-for-Machine-Translation"><a href="#Dual-Learning-for-Machine-Translation" class="headerlink" title="Dual Learning for Machine Translation"></a><a href="https://arxiv.org/pdf/1611.00179v1.pdf" target="_blank" rel="external">Dual Learning for Machine Translation</a></h2><p>【机器翻译】【增强学习】本文解决的问题是机器翻译中双语训练语料需求过多的问题，旨在通过一种手段来减少数据标注工作。作者采用的方法是现在重新流行的增强学习方法，翻译通常是一个对偶过程，比如：英翻法和法翻英。整个学习过程可以简单的描述如下：对偶的两个翻译任务可以当做是两个agent A和B，通过少量的双语标注数据可以学习出一个初级的翻译模型，同时通过大量的单语数据（无需标注）来学习出相应的语言模型；A将单语数据翻译成B，B通过自身的语言模型对A的翻译结果进行误差反馈，A进行学习；同理，B也可以向A学习，直到收敛。整个学习过程中，训练了A-&gt;B和B-&gt;A两个翻译模型，但是用到的双语标注数据就会比较少。</p>
<h2 id="End-to-End-Reading-Comprehension-with-Dynamic-Answer-Chunk-Ranking"><a href="#End-to-End-Reading-Comprehension-with-Dynamic-Answer-Chunk-Ranking" class="headerlink" title="End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking"></a><a href="https://arxiv.org/pdf/1610.09996v2.pdf" target="_blank" rel="external">End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking</a></h2><p>【机器阅读】本文研究的问题是最近一年非常流行的机器阅读理解问题，给定一段文本和一个问题，输出一个答案（选择、生成）。本文提出了一种新的模型，相比之前模型来说，改进的地方是可以给出变长度的答案。在之前模型的基础上，添加了一个entity表示模块，并且对候选的entity进行排序，得到正确答案。本文在SQuAD上进行了测试，拿到了最好的结果。建议研究QA和机器阅读的童鞋来精读这篇文章，并且开始新一轮SQuAD刷榜。</p>
<h2 id="Knowledge-Questions-from-Knowledge-Graphs"><a href="#Knowledge-Questions-from-Knowledge-Graphs" class="headerlink" title="Knowledge Questions from Knowledge Graphs"></a><a href="https://arxiv.org/pdf/1610.09935v2.pdf" target="_blank" rel="external">Knowledge Questions from Knowledge Graphs</a></h2><p>【问题生成】【知识图谱】本文研究的内容是从知识图谱中自动生成一些具有一定难度且答案唯一的问题，用于教育或评估。问题的第一个难点在于如何确保从图谱中选择的答案具有唯一性，第二个难点是如何评价所生成问题的难度。这个任务非常有趣，也是知识图谱在实际中的一个应用场景。任务本身比文章的模型和方法更值得思考。还是说回chatbot，QA chatbot，都说缺少数据，已构建好的知识图谱本身就是一个很大的数据源，如何利用它，如何将其更好地用于生成有用的训练数据，本文的任务也许会带来一些启发。这个任务也不算首创，之前KBQA的工作都有通过知识库出发来生成问题，且通过平行语料扩展，包括Percy liang等不少大牛的工作都考虑了这点，这里相当于延续，量化了难度确保了答案唯一性等</p>
<h2 id="LightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Networks"><a href="#LightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Networks" class="headerlink" title="LightRNN: Memory and Computation-Efficient Recurrent Neural Networks"></a><a href="https://arxiv.org/pdf/1610.09893v1.pdf" target="_blank" rel="external">LightRNN: Memory and Computation-Efficient Recurrent Neural Networks</a></h2><p>【新RNN】本文提出了一种新的思路来提高RNN的效果，包括时间和空间上的。最核心的点在于构建了一种全新的word embedding表示方式，传统的方法是词表中的每个词都用一个向量表示。将每个词都放入到一张二维表中，表中的每个词都有其所在的行向量和列向量共同表示，如图1所示。从而将词表示的规模从|V|个向量降到了2*sqrt(V)。本文还针对这种表示方法，构建了一种新的RNN模型LightRNN，并在大型数据集上进行了语言模型任务的评测，验证了本文方法在时间和空间上的性能提升。 </p>
<h2 id="Chinese-Poetry-Generation-with-Planning-based-Neural-Network"><a href="#Chinese-Poetry-Generation-with-Planning-based-Neural-Network" class="headerlink" title="Chinese Poetry Generation with Planning based Neural Network"></a><a href="https://arxiv.org/pdf/1610.09889v1.pdf" target="_blank" rel="external">Chinese Poetry Generation with Planning based Neural Network</a></h2><p>【诗词生成】本文研究的任务非常有趣，通过神经网络模型来生成唐诗，类似地可以开展宋词等任务。端到端地训练、学习具有很强的应用性，只要能够给定输入序列和输出序列，打开脑洞，做任何好玩的任务都有可能。</p>
<h2 id="MusicMood-Predicting-the-mood-of-music-from-song-lyrics-using-machine-learning"><a href="#MusicMood-Predicting-the-mood-of-music-from-song-lyrics-using-machine-learning" class="headerlink" title="MusicMood: Predicting the mood of music from song lyrics using machine learning"></a><a href="https://arxiv.org/pdf/1611.00138v1.pdf" target="_blank" rel="external">MusicMood: Predicting the mood of music from song lyrics using machine learning</a></h2><p>【音乐推荐系统】本文研究内容为通过机器学习方法从歌词中来预测音乐的情绪，算是自然语言处理在音乐中的应用。这种简单的应用，可以为音乐推荐系统提供一些特征，现有的音乐推荐系统可以做参考。</p>
<h2 id="Detecting-Context-Dependent-Messages-in-a-Conversational-Environment"><a href="#Detecting-Context-Dependent-Messages-in-a-Conversational-Environment" class="headerlink" title="Detecting Context Dependent Messages in a Conversational Environment"></a><a href="https://arxiv.org/pdf/1611.00483v2.pdf" target="_blank" rel="external">Detecting Context Dependent Messages in a Conversational Environment</a></h2><p>【chatbot】【上下文】chatbot的难点之一在于如何准确理解“人话”，“人话”有个显著的特点是简短而且非正式，常见的NLP分析方法，词性标注、句法分析等都不好用。理解“人话”需要结合上下文。那么，第一个问题来了，理解某句话应该取哪几句history作为上下文，第二个问题是如何理解上下文？本文旨在解决第一个问题，这个问题研究空间比较大，本文做了初步尝试。对chatbot感兴趣的童鞋，不管是学术界还是工业界的童鞋都可以读一下本文，或许会带来一些启发和思考。</p>
<h2 id="Natural-Parameter-Networks-A-Class-of-Probabilistic-Neural-Networks"><a href="#Natural-Parameter-Networks-A-Class-of-Probabilistic-Neural-Networks" class="headerlink" title="Natural-Parameter Networks: A Class of Probabilistic Neural Networks"></a><a href="https://arxiv.org/pdf/1611.00448v1.pdf" target="_blank" rel="external">Natural-Parameter Networks: A Class of Probabilistic Neural Networks</a></h2><p>【NIPS2016】本文提出了一类概率神经网络（贝叶斯），旨在解决现有神经网络在数据规模小的时候容易过拟合的问题。</p>
<h2 id="Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks"><a href="#Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks" class="headerlink" title="Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks"></a><a href="https://arxiv.org/pdf/1611.00454v1.pdf" target="_blank" rel="external">Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks</a></h2><p>【推荐系统】本文的亮点在于将RNN和协同过滤无缝结合起来。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="聊天机器人资料汇总"><a href="#聊天机器人资料汇总" class="headerlink" title="聊天机器人资料汇总"></a><a href="https://www.52ml.net/20510.html" target="_blank" rel="external">聊天机器人资料汇总</a></h2><p>来自52ml汇总的聊天机器人资料</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Neural-Machine-Translation-in-Linear-Time&quot;&gt;&lt;a href=&quot;#
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十二期</title>
    <link href="http://rsarxiv.github.io/2016/11/03/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%BA%8C%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/03/PaperWeekly-第十二期/</id>
    <published>2016-11-04T02:47:13.000Z</published>
    <updated>2016-11-04T03:19:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>文本摘要是自然语言处理的一大经典任务，研究的历史比较长。随着目前互联网生产出的文本数据越来越多，文本信息过载问题越来越严重，对各类文本进行一个“降维”处理显得非常必要，文本摘要便是其中一个重要的手段。传统的文本摘要方法，不管是句子级别、单文档还是多文档摘要，都严重依赖特征工程，随着深度学习的流行以及seq2seq+attention模型在机器翻译领域中的突破，文本摘要任务也迎来了一种全新的思路。本期PaperWeekly将会分享4篇在这方面做得非常出色的paper：</p>
<p>1、A Neural Attention Model for Abstractive Sentence Summarization, 2015<br>2、Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond, 2016<br>3、Neural Summarization by Extracting Sentences and Words, 2016<br>4、AttSum: Joint Learning of Focusing and Summarization with Neural Attention, 2016</p>
<h1 id="A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization"><a href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization" class="headerlink" title="A Neural Attention Model for Abstractive Sentence Summarization"></a><a href="https://aclweb.org/anthology/D/D15/D15-1044.pdf" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Rush, A. M., Chopra, S., &amp; Weston, J.</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Facebook AI Research / Harvard SEAS</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>Neural Attention, Abstractive Sentence Summarization</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>EMNLP 2015</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这篇来自Facebook的paper的主题是基于attention based NN的生成式句子摘要/压缩。</p>
<p><img src="media/emnlp.JPG" alt="1"></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>该工作使用提出了一种encoder-decoder框架下的句子摘要模型。</p>
<p><img src="media/emnlp2.JPG" alt="encoder"></p>
<p>作者在文章中介绍了三种不同的encoding方法，分别为：</p>
<ol>
<li>Bag-of-Words Encoder。词袋模型即将输入句子中词的词向量进行平均。</li>
<li>CNN encoder</li>
<li>Attention-Based Encoder。该encoder使用CNN对已生成的最近c（c为窗口大小）个词进行编码,再用编码出来的context向量对输入句子做attention，从而实现对输入的加权平均。</li>
</ol>
<p>模型中的decoder为修改过的NNLM，具体地：</p>
<p><img src="media/emnlp3.JPG" alt="1"></p>
<p>式中$$y_c$$为已生成的词中大小为c的窗口，与encoder中的Attention-Based Encoder同义。</p>
<p>与目前主流的基于seq2seq的模型不同，该模型中encoder并未采用流行的RNN。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>该文章使用了English Gigaword作为语料，选择新闻中的首句作为输入，新闻标题作为输出，以此构建平行语料。<br>具体的数据构建方法参见文章。</p>
<p>此外，该文章还使用了DUC2004作为测试集。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>在调研范围内，该文章是使用attention机制进行摘要的第一篇。且作者提出了利用Gigaword构建大量平行句对的方法，使得利用神经网络训练成为可能，之后多篇工作都使用了该方法构建训练数据。</p>
<h1 id="Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond"><a href="#Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond" class="headerlink" title="Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"></a><a href="https://aclweb.org/anthology/K/K16/K16-1028.pdf" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Nallapati, Ramesh, et al.</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>IBM Watson</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>seq2seq, Summarization</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>In CoNLL 2016</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>该工作主要研究了基于seq2seq模型的生成式文本摘要。<br>该文章不仅包括了句子压缩方面的工作，还给出了一个新的文档到多句子的数据集。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p><img src="media/conll.JPG" alt=""></p>
<p>该文章使用了常用的seq2seq作为基本模型，并在其基础上添加了很多feature：</p>
<ol>
<li>Large Vocabulary Trick。<br>参见Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2014. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007.</li>
<li><p>添加feature。例如POS tag， TF、IDF， NER tag等。这些feature会被embed之后与输入句子的词向量拼接起来作为encoder的输入。</p>
</li>
<li><p>pointing / copy 机制。使用一个gate来判断是否要从输入句子中拷贝词或者使用decoder生成词。参见ACL 2016的两篇相关paper。</p>
</li>
<li><p>Hierarchical Attention。这是用于文章摘要中多句子的attention，思路借鉴了Jiwei Li的一篇auto encoder的工作。大致思路为使用句子级别的weight对句子中的词进行re-scale。</p>
</li>
</ol>
<h2 id="数据-1"><a href="#数据-1" class="headerlink" title="数据"></a>数据</h2><ol>
<li>English Gigaword</li>
<li>DUC 2004</li>
<li>提出了CNN/Daily Mail Corpus</li>
</ol>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>该工作为在第一篇文章基础上的改进工作，做了大量的实验，非常扎实。文章提出的feature-rich encoder对其他工作也有参考意义，即将传统方法中的特征显示地作为神经网络的输入，提高了效果。</p>
<h1 id="Neural-Summarization-by-Extracting-Sentences-and-Words"><a href="#Neural-Summarization-by-Extracting-Sentences-and-Words" class="headerlink" title="Neural Summarization by Extracting Sentences and Words"></a><a href="https://aclweb.org/anthology/P/P16/P16-1046.pdf" target="_blank" rel="external">Neural Summarization by Extracting Sentences and Words</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Cheng, Jianpeng, and Mirella Lapata.</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>University of Edinburgh</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Extractive Summarization, Neural Attention</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL 2016</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>使用神经网络进行抽取式摘要，分别为句子抽取和单词抽取。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p><img src="media/extractModel1.JPG" alt=""></p>
<h3 id="句子抽取"><a href="#句子抽取" class="headerlink" title="句子抽取"></a>句子抽取</h3><p>由于该工作为文档的摘要，故其使用了两层encoder，分别为：</p>
<ol>
<li>词级别的encoder，基于CNN。即对句子做卷积再做max pooling从而获得句子的表示。</li>
<li>句子级别的encoder，基于RNN。将句子的表示作为输入，即获得文档的表示。</li>
</ol>
<p>由于是抽取式摘要，其使用了一个RNN decoder，但其作用并非生成，而是用作sequence labeling，对输入的句子判断是否进行抽取，类似于pointer network。</p>
<h3 id="词的抽取"><a href="#词的抽取" class="headerlink" title="词的抽取"></a>词的抽取</h3><p>对于词的抽取，该模型同样适用了hierarchical attention。与句子抽取不同，词的抽取更类似于生成，只是将输入文档的单词作为decoder的词表。</p>
<h2 id="数据-2"><a href="#数据-2" class="headerlink" title="数据"></a>数据</h2><p>从DailyMail news中根据其highlight构建抽取式摘要数据集。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>该工作的特别之处在于对attention机制的使用。该paper之前的许多工作中的attention机制都与Bahdanau的工作相同，即用attention对某些向量求weighted sum。该工作则直接使用attention的分数进行对文档中句子进行选择，实际上与pointer networks意思相近。</p>
<h1 id="AttSum-Joint-Learning-of-Focusing-and-Summarization-with-Neural-Attention"><a href="#AttSum-Joint-Learning-of-Focusing-and-Summarization-with-Neural-Attention" class="headerlink" title="AttSum: Joint Learning of Focusing and Summarization with Neural Attention"></a><a href="http://www4.comp.polyu.edu.hk/~cszqcao/data/attsum.pdf" target="_blank" rel="external">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Cao, Ziqiang, et al.</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p> The Hong Kong Polytechnic University, Peking University, Microsoft Research</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Query-focused Summarization</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>COLING 2016</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>Query-focused多文档抽取式摘要</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p><img src="media/coling.JPG" alt=""></p>
<p>由于该任务为针对某个query抽取出可以回答该query的摘要，模型使用了attention机制对句子进行加权，加权的依据为文档句子对query的相关性（基于attention），从而对句子ranking，进而抽取出摘要。具体地：</p>
<ol>
<li>使用CNN对句子进行encoding</li>
<li>利用query，对句子表示进行weighted sum pooling。</li>
<li>使用cosine similarity对句子排序。</li>
</ol>
<h2 id="数据-3"><a href="#数据-3" class="headerlink" title="数据"></a>数据</h2><p>DUC 2005 ∼ 2007 query-focused summarization benchmark datasets</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>该文章的亮点之处在于使用attention机制对文档中句子进行weighted-sum pooling，以此完成query-focused的句子表示和ranking。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本次主要介绍了四篇文本摘要的工作，前两篇为生成式（abstractive）摘要，后两篇为抽取式（extractive）摘要。对于生成式摘要，目前主要是基于encoder-decoder模式的生成，但这种方法受限于语料的获得，而Rush等提出了利用English Gigaword（即新闻数据）构建平行句对语料库的方法。IBM在Facebook工作启发下，直接使用了seq2seq with attention模型进行摘要的生成，获得了更好的效果。对于抽取式摘要，神经网络模型的作用多用来学习句子表示进而用于后续的句子ranking。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引&quot;&gt;&lt;a href=&quot;#引&quot; class=&quot;headerlink&quot; title=&quot;引&quot;&gt;&lt;/a&gt;引&lt;/h1&gt;&lt;p&gt;文本摘要是自然语言处理的一大经典任务，研究的历史比较长。随着目前互联网生产出的文本数据越来越多，文本信息过载问题越来越严重，对各类文本进行一个“降维
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.10.24-2016.10.28</title>
    <link href="http://rsarxiv.github.io/2016/10/28/cs-CL-weekly-2016-10-24-2016-10-28/"/>
    <id>http://rsarxiv.github.io/2016/10/28/cs-CL-weekly-2016-10-24-2016-10-28/</id>
    <published>2016-10-28T23:35:04.000Z</published>
    <updated>2016-10-28T23:46:21.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Iterative-Refinement-for-Machine-Translation"><a href="#Iterative-Refinement-for-Machine-Translation" class="headerlink" title="Iterative Refinement for Machine Translation"></a><a href="https://arxiv.org/pdf/1610.06602v2.pdf" target="_blank" rel="external">Iterative Refinement for Machine Translation</a></h2><p>【机器翻译】本文的研究内容针对机器翻译在解码阶段单调总是单调、不回溯地生成翻译结果的问题，作者提出了一种翻译方案，在解码时不断地回溯并且修正先前生成的结果。模型是CNN+attention。本文工作来自Facebook AI Research。</p>
<h2 id="Bridging-Neural-Machine-Translation-and-Bilingual-Dictionaries"><a href="#Bridging-Neural-Machine-Translation-and-Bilingual-Dictionaries" class="headerlink" title="Bridging Neural Machine Translation and Bilingual Dictionaries"></a><a href="https://arxiv.org/pdf/1610.07272v1.pdf" target="_blank" rel="external">Bridging Neural Machine Translation and Bilingual Dictionaries</a></h2><p>【机器翻译】本文研究的内容是机器翻译中如何应用两门语言中罕见词和未登录词词典的问题，本文针对这一问题提出了两种方法。本文工作来自中科院@张家俊MT 老师。</p>
<h2 id="Two-are-Better-than-One-An-Ensemble-of-Retrieval-and-Generation-Based-Dialog-Systems"><a href="#Two-are-Better-than-One-An-Ensemble-of-Retrieval-and-Generation-Based-Dialog-Systems" class="headerlink" title="Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems"></a><a href="https://arxiv.org/pdf/1610.07149v1.pdf" target="_blank" rel="external">Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems</a></h2><p>【chatbot】开放域聊天机器人在回复时有两种常见的思路：1、基于检索 2、基于生成。第一种方法比较简单但答案相对死板，第二种方法灵活但常常出现“呵呵”式的回答。本文提出了一种集成方法，检索到的答案和用户的query一起作为输入，喂入模型（RNN）中，然后生成回答，新生成的回答作为candidate一起重排序来决定最终的回复。实验结果证明了本文的方法要比单种方法更有效。每个模型都有其优点和缺点，这种集成式的方法将各个模型的优点集成起来，效果会很不错。</p>
<h2 id="EmojiNet-Building-a-Machine-Readable-Sense-Inventory-for-Emoji"><a href="#EmojiNet-Building-a-Machine-Readable-Sense-Inventory-for-Emoji" class="headerlink" title="EmojiNet: Building a Machine Readable Sense Inventory for Emoji"></a><a href="https://arxiv.org/pdf/1610.07710v1.pdf" target="_blank" rel="external">EmojiNet: Building a Machine Readable Sense Inventory for Emoji</a></h2><p>【语义网】本文构建的EmojiNet类似于词的WordNet，同一个Emoji表情在不同的上下文中有着不同的意思，而社交网络非常流行Emoji表情，构建这么一个语义网非常有意义。项目地址：<a href="http://emojinet.knoesis.org./" target="_blank" rel="external">http://emojinet.knoesis.org./</a></p>
<h2 id="Can-Active-Memory-Replace-Attention"><a href="#Can-Active-Memory-Replace-Attention" class="headerlink" title="Can Active Memory Replace Attention?"></a><a href="https://arxiv.org/pdf/1610.08613v1.pdf" target="_blank" rel="external">Can Active Memory Replace Attention?</a></h2><p>【Memory Networks】Memory Networks和Attention是解决长距离依赖问题的两大方法，Attention模型在NLP的很多任务中都有更好的表现，本文对Memory Networks类模型的缺点进行了分析，并且提出了一种改进模型。改进版的memory模型有不错的表现，并且在长句子机器翻译任务中得到了验证。本文作者来自Google Brain。建议关注自然语言处理的童鞋，不管是关注什么任务，都应该精读一下本文。</p>
<h2 id="CoType-Joint-Extraction-of-Typed-Entities-and-Relations-with-Knowledge-Bases"><a href="#CoType-Joint-Extraction-of-Typed-Entities-and-Relations-with-Knowledge-Bases" class="headerlink" title="CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases"></a><a href="https://arxiv.org/pdf/1610.08763v1.pdf" target="_blank" rel="external">CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases</a></h2><p>【信息抽取】本文提出了一种基于Distant Supervision对文本中命名实体和关系联合抽取的框架，克服了之前将两个任务分开做带来的误差影响，框架具有较强的扩展性和迁移性，建议精读。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="Reddit大型评论数据集"><a href="#Reddit大型评论数据集" class="headerlink" title="Reddit大型评论数据集"></a><a href="https://www.reddit.com/r/datasets/comments/59039y/updated_reddit_comment_dataset_up_to_201608/" target="_blank" rel="external">Reddit大型评论数据集</a></h2><p>【数据集】Reddit大型评论数据集，用于情感分析研究。</p>
<h2 id="微软lightGBM框架的python接口"><a href="#微软lightGBM框架的python接口" class="headerlink" title="微软lightGBM框架的python接口"></a><a href="https://github.com/ArdalanM/pyLightGBM" target="_blank" rel="external">微软lightGBM框架的python接口</a></h2><p>【开源框架】ArdalanM/pyLightGBM: Python binding for Microsoft LightGBM </p>
<h2 id="中文对白语料：可用作聊天机器人训练语料"><a href="#中文对白语料：可用作聊天机器人训练语料" class="headerlink" title="中文对白语料：可用作聊天机器人训练语料"></a><a href="https://github.com/rustch3n/dgk_lost_conv" target="_blank" rel="external">中文对白语料：可用作聊天机器人训练语料</a></h2><p>【数据集】chinese conversation corpus by BiNgFeng GitHub</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Iterative-Refinement-for-Machine-Translation&quot;&gt;&lt;a href
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十一期</title>
    <link href="http://rsarxiv.github.io/2016/10/27/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B8%80%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/10/27/PaperWeekly-第十一期/</id>
    <published>2016-10-28T04:54:20.000Z</published>
    <updated>2016-10-28T04:57:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>Memory Networks是由Facebook的Jason Weston等人提出的一个神经网络框架，通过引入长期记忆组件(long-term memory component)来解决神经网络长程记忆困难的问题。在此框架基础上，发展出许多Memory Networks的变体模型，本期精选了5篇Memory Networks相关的论文，分别如下：</p>
<p>1、Memory Networks<br>2、End-To-End Memory Networks<br>3、Ask Me Anything: Dynamic Memory Networks for Natural Language Processing<br>4、THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS<br>5、Key-Value Memory Networks for Directly Reading Documents</p>
<h1 id="Memory-Networks"><a href="#Memory-Networks" class="headerlink" title="Memory Networks"></a><a href="https://arxiv.org/abs/1410.3916" target="_blank" rel="external">Memory Networks</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Jason Weston, Sumit Chopra, Antoine Bordes</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Facebook AI Research</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>Question Answering, Memory Network</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2015</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>为解决长期记忆问题, 提出一类称为Memory Networks的模型框架, 基于该框架构造的模型可以拥有长期(大量)和易于读写的记忆。</p>
<h2 id="模型和思路"><a href="#模型和思路" class="headerlink" title="模型和思路"></a>模型和思路</h2><p>Memory Networks可以理解为一种构造模型的框架, 该类模型由如下五部分组成:</p>
<p>1、记忆m: 模型记忆的表示,由一个记忆槽列表[m<sub>1</sub>-m<sub>i</sub>]组成,可被G,O组件读写<br>2、组件I (input feature map): 将模型输入转化模型内部特征空间中特征表示<br>3、组件G (generalization): 在模型获取新输入时更新记忆m，可以理解为记忆存储<br>4、组件O (output feature map): 根据模型输入和记忆m输出对应于模型内部特征空间中特征表示，可以理解为读取记忆<br>5、组件R(response): 将O组件输出的内部特征空间的表示转化为特定格式，比如文本。可以理解为把读取到抽象的记忆转化为具象的表示。</p>
<p>假设模型输入为x:</p>
<ul>
<li>记忆的更新过程表示为 m<sub>H(x)</sub> = G(m<sub>i</sub>, I(X), m), ∀i, H(x)为选择记忆和遗忘机制</li>
<li>记忆的读取过程表示为 r = R(O(I(x), m))</li>
</ul>
<p>再次强调Memory Networks是一类模型框架, 组件I,G,R,O可以使用不同的实现</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><ul>
<li><a href="https://github.com/facebook/MemNN" target="_blank" rel="external">facebook MemNN实现</a></li>
</ul>
<h2 id="相关工作及引用"><a href="#相关工作及引用" class="headerlink" title="相关工作及引用"></a>相关工作及引用</h2><ul>
<li>Facebook AI的进一步工作, 基于Memory Networks框架和神经网络实现了End-To-End的训练学习<br>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus. End-To-End Memory Networks. arXiv:1503.08895  </li>
</ul>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>文章提出了一个通用的解决长期记忆问题的算法框架, 框架中的每一个模块都可以变更成新的实现, 可以根据不同的应用场景进行适配。 </p>
<h1 id="End-To-End-Memory-Networks"><a href="#End-To-End-Memory-Networks" class="headerlink" title="End-To-End Memory Networks"></a><a href="https://arxiv.org/pdf/1503.08895v5.pdf" target="_blank" rel="external">End-To-End Memory Networks</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,  Rob Fergus</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Dept. of Computer Science Courant Institute, New York University<br>Facebook AI Research</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Memory Networks, End-to-end, Question Answer</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>NIPS 2015</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>本文提出了一个可以端到端训练的Memory Networks，并且在训练阶段比原始的Memory Networks需要更少的监督信息。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>本文提出的模型包括单层和多层两种情况。下面先介绍单层情况，<br>1、单层<br>如图(a)所示，输入的序列可以通过不同的Embedding矩阵A和C分别被表示成Input和Output向量的集合。同样的，通过Embedding矩阵B，我们将Question表示成一个向量u，向量u和Input向量集合中的每个向量计算内积，然后通过softmax得到一个概率向量p（attention过程），概率向量p中的每一个概率值表示每个Output向量对应输出的权重大小。通过p和Output向量集合，对Output中的向量进行加权求和得到输出向量o，将输出向量o和问题向量u相加，再最后通过一个权值矩阵W和softmax来预测最终的label。</p>
<p><img src="media/end_to_end.png" alt="end_to_end"></p>
<p>2、 多层<br>多层的情况如图(b)所示，每层的输出向量o<sup>i</sup>和问题向量u<sup>i</sup>相加获得新的问题表示u<sup>i+1</sup>，然后重复上述单层的过程,直到最后一层通过softmax来预测label。</p>
<p>本文在bAbi数据集、Penn Treebank以及Text8三个数据集上进行实验，均取得了较好的实验效果。</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><ul>
<li>[bAbi]<br>(<a href="https://research.facebook.com/research/babi/" target="_blank" rel="external">https://research.facebook.com/research/babi/</a>)</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>Memory Networks<br>Neural Turing Machines</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>本篇论文提出的模型是在Facebook提出的原始Memory networks基础上进行的改进。在Memory networks的框架下，将原来依赖于中间监督信息的非端到端Memory networks改进为端到端的Memory networks。基础模型之外，本文针对时序编码提出了一些有趣的trick，可作参考。</p>
<h1 id="Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing"><a href="#Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing" class="headerlink" title="Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"></a><a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="external">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>MetaMind</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Memory Networks, Neural Networks, Question Answering</p>
<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>arXiv</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>Question Answering: 给定一段Context，一个与此Context相关的Question，利用模型生成一个单词的Answer。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>下图给出了dynamic memory networks的框架。</p>
<p><img src="media/model-image.png" alt="model-image"></p>
<p>首先Context和Question都经过Gated Recurrent Unit(GRU)转换成成vector形式，分别作为episodic memories e和m储存下来。e代表的是一连串vectors，Context中每句话都会被转换成一个e vector，然而Question只会被转换成一个m vector。</p>
<p>下一步是episodic memory updates，在每一个episode, 每一个e vector会和m计算一个attention，本文中使用一个two layer feed forward neural network计算attention score。然后利用attention scores来update episodic memories。</p>
<p><img src="media/gate.png" alt="gate"></p>
<p><img src="media/episodic-memory.png" alt="episodic-memory"></p>
<p>输出答案也采用了一个GRU decoder</p>
<p><img src="media/output.png" alt="output"></p>
<p>这里的a0是最后一个memory state m。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p><a href="https://research.facebook.com/research/babi/" target="_blank" rel="external">Facebook bAbI dataset</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p><a href="https://arxiv.org/abs/1410.3916" target="_blank" rel="external">Memory Networks</a><br><a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="external">Eng-to-End Memory Networks</a><br><a href="https://arxiv.org/abs/1511.02301" target="_blank" rel="external">The Goldilocks Principle: Reading Children’s Books with Explicit Memory Representations</a></p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>总体来说这是一篇很有趣的文章。其中应用了episodically update memory的想法，期望模型能够借此学到一些logical reasoning的能力。并且模型中多次用的GRU，每一层都使用GRU的encoding或者decoding，比较有趣。</p>
<p>然后我认为本文的写作有一些问题，比如我自始至终也没有找到e的下标究竟代表什么，我的理解是每一句话都被encode成一个e作为episodic memory，那么每次Update 其中一个e都要经过所有其他的e是为了更好的融合所有context sentences的信息吗？looks reasonable to me. </p>
<p>那么每一层的hidden states h究竟又是什么？上一层的hidden state如何更新到下一层？文章中似乎没有给出明确的公式，也没有在model figure中展示出来，似乎写作不够明确。既然e是有h穿过层层GRU得到，我会揣测下一层的h是上一层e的一个function。总之感觉model这一块的解释不够清晰到位，变量太多有些混乱。</p>
<p>然而总体来说，我觉得本文还是非常值得一读的。</p>
<h1 id="THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS"><a href="#THE-GOLDILOCKS-PRINCIPLE-READING-CHILDREN’S-BOOKS-WITH-EXPLICIT-MEMORY-REPRESENTATIONS" class="headerlink" title="THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS"></a><a href="https://arxiv.org/pdf/1511.02301v4.pdf" target="_blank" rel="external">THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Felix Hill, Antoine Bordes, Sumit Chopra &amp; JasonWeston</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>Facebook AI Research</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Memory Networks,self-supervised training,window-based memories,The Children’s Book Test(CBT)</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR2016</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>本文对于语言模型（RNN/LSTM/Memory Network生成）到底能够多好或者在多大程度上表示The Children’s Book做了一项测试。测试结果表面Memor　Network上的效果最好。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>文中主要对比了一系列state-of-the-art的模型，每个用不同的方式对之前已经读过的文本进行编码，然后进行CBT评比。<br>实验中使用的模型以及结果如下：</p>
<p><img src="media/CBT.png" alt="CBT"></p>
<p>CBT简介：数据来自Project Gutenburg所创建的数据集，里面的内容都选自儿童书籍。每20句话产生一个问题，让不同的语言模型去进行预测，看谁预测的效果更好。<br>问题产生于20句话中的某一句话抠掉一个词A。候选集产生分为如下两步:<br>(1)从构成20句话的词表中随机选出和抠掉词A具有相同词性的词集合C 。<br>(2)从C中随机抽选10个词作为答案的备选集。<br>实验最后在CNN QA的语料上进行测试，在新闻文章中识别命名实体，得到的准确率能到<br>69.4%</p>
<h2 id="资源-3"><a href="#资源-3" class="headerlink" title="资源"></a>资源</h2><p>n-gram language model:the KenLM toolkit (Scalable modified Kneser-Ney language<br>model estimation.)</p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>(1) MN:arXiv2015,Bordes,Large-scale simple question answering with memory networks.文中关于end to end的训练方法以及memory network的模型主要来自本篇<br>(2) NIPS2015,Sukhbaatar,End-to-end memory networks.<br>(3)EMNLP2015,Rush,A neural attention model for abstractive sentence summarization. Contextual LSTM模型的参考文章</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>本文提供了一种测试语言模型效果的测试方法，这对于语言模型的评判做出了贡献。<br>在做实验过程中，作者还发现在单层记忆表示中文本被编码的数量对结果有很大的影响：存在一个范围，使得单个词信息和整个句子的信息都得以较好的保留。</p>
<h1 id="Key-Value-Memory-Networks-for-Directly-Reading-Documents"><a href="#Key-Value-Memory-Networks-for-Directly-Reading-Documents" class="headerlink" title="Key-Value Memory Networks for Directly Reading Documents"></a><a href="https://arxiv.org/pdf/1606.03126v2.pdf" target="_blank" rel="external">Key-Value Memory Networks for Directly Reading Documents</a></h1><h2 id="作者-4"><a href="#作者-4" class="headerlink" title="作者"></a>作者</h2><p>Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, Jason Weston</p>
<h2 id="单位-4"><a href="#单位-4" class="headerlink" title="单位"></a>单位</h2><p>Facebook AI Research<br>Language Technologies Institute, Carnegie Mellon University</p>
<h2 id="关键词-4"><a href="#关键词-4" class="headerlink" title="关键词"></a>关键词</h2><p>Memory Networks, Key-Value, Question Answering, Knowledge Bases</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv 2016</p>
<h2 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h2><p>鉴于知识库有知识稀疏、形式受限等问题，本文提出了一种可以通过直接读取文档来解决QA问题的新方法Key-Value Memory Networks。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>如下图所示，Key-Value Memory Networks(KV-MemNNs)模型结构与End-to-end Memory Networks(MemN2N)基本相同，区别之处在于KV-MemNNs的寻址（addressing）阶段和输出阶段采用不同的编码（key和value）。<br><img src="media/key_value.png" alt="key_value"></p>
<p>本文主要提出了以下几种Key-value方法：</p>
<ol>
<li>KB Triple<br>针对知识库中的三元组(subject, relation, object),将subject和relation作为Key，object作为Value。</li>
<li>Sentence Level<br>将文档分割成多个句子，每个句子即作为Key也作为Value，该方法与MemN2N相同。</li>
<li>Window Level<br>以文档中每个实体词为中心开一个窗口，将整个窗口作为Key，中间的实体词作为Value。</li>
<li>Window + Center Encoding<br>该方法与Window Level基本相同，区别之处在于中心实体词与窗口中的其他词采用不同的Embedding。</li>
<li>Window + Titile<br>很多情况下文章的题目可能包含答案，因此在上述提出的Window方法基础上，再添加如下Key-value对：Key为窗口，Value为文档对应的title。</li>
</ol>
<p>本文为了比较使用知识库、信息抽取和直接采用维基百科文档方法之间的效果，构建了新的语料WIKIMOVIES。实验结果表明，KV-MemNNs直接从文档读取信息比信息抽取方法的效果好，却仍比直接利用知识库的方法差不少。其中几种Key-Value方法中，“Window + Center Encoding”方法效果最好。此外，本文还在WikiQA上进行实验，验证了KV-MemNNs的效果。</p>
<h2 id="资源-4"><a href="#资源-4" class="headerlink" title="资源"></a>资源</h2><ul>
<li>[WikiQA]<br>(<a href="https://www.microsoft.com/en-us/download/details.aspx?id=52419" target="_blank" rel="external">https://www.microsoft.com/en-us/download/details.aspx?id=52419</a>)</li>
<li>[WikiMovies]<br>(<a href="https://research.facebook.com/research/babi/" target="_blank" rel="external">https://research.facebook.com/research/babi/</a>)</li>
</ul>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p>Memory Networks<br>End-TO-End Memory Networks<br>The Goldilocks Principle: Reading Children’s Books with Explicit Memory Representations</p>
<h2 id="简评-4"><a href="#简评-4" class="headerlink" title="简评"></a>简评</h2><p>本篇论文提出了一个在新的Memory Networks变体Key-Value Memory Networks，旨在探索在QA过程中，如何消除采用知识库和自由文本（维基百科）之间的效果差距（gap），并为此构建了一个新的数据集WikiMovies。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>长程记忆（long-term memory）问题一直是深度学习中的一个难点，Attention机制就是解决这一问题的经典方法。本文介绍的几篇Memory Networks试图通过构建长期存储记忆组件来解决过去神经网络无法存储过长内容的问题。如何存储大量的外部信息以及如何利用这些外部信息推断是Memory Networks乃至很多NLP任务的难点。本期引入的这几篇论文中，Memory Networks提出了一个整体的框架，End-To-End Memory Networks使memory networks可以端到端的训练学习。Key-Value Memory Networks主要解决外部信息如何存储表示，而THE GOLDILOCKS PRINCIPLE这篇论文则在推理方面有所创新，直接利用attention的打分来预测答案。目前深度学习方法中，无论是存储更新长期记忆的方法还是结合长期记忆进行推理的方法都还很初级，仍需诸君努力前行。</p>
<p>以上为本期Paperweekly的主要内容，感谢cain、destinwang、zeweichu、chunhualiu等四位同学的整理。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;Memory Networks是由Facebook的Jason Weston等人提出的一个神经网络框架，通过引入长期记忆组件(long-te
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.10.17-2016.10.21</title>
    <link href="http://rsarxiv.github.io/2016/10/22/cs-CL-weekly-2016-10-17-2016-10-21/"/>
    <id>http://rsarxiv.github.io/2016/10/22/cs-CL-weekly-2016-10-17-2016-10-21/</id>
    <published>2016-10-22T20:11:20.000Z</published>
    <updated>2016-10-22T20:49:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Cached-Long-Short-Term-Memory-Neural-Networks-for-Document-Level-Sentiment-Classification"><a href="#Cached-Long-Short-Term-Memory-Neural-Networks-for-Document-Level-Sentiment-Classification" class="headerlink" title="Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification"></a><a href="https://arxiv.org/pdf/1610.04989v1.pdf" target="_blank" rel="external">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</a></h2><p>【情感分析】RNN处理文本这样的序列数据有天然优势，但对于长文本效果却不尽人意。本文针对这个问题，提出了一种新的LSTM结构Cached LSTM。通过cache机制，来捕捉整体语义信息，将memory分成几组，对应不同的forget门，在文档集情感分析任务中取得了不错的结果。其实，RNN处理长文本信息都面临这个问题，chatbot中对context信息的处理也可以考虑借鉴这个思路。本文是FudanvNLP的工作。</p>
<h2 id="Lexicon-Integrated-CNN-Models-with-Attention-for-Sentiment-Analysis"><a href="#Lexicon-Integrated-CNN-Models-with-Attention-for-Sentiment-Analysis" class="headerlink" title="Lexicon Integrated CNN Models with Attention for Sentiment Analysis"></a><a href="https://arxiv.org/pdf/1610.06272v1.pdf" target="_blank" rel="external">Lexicon Integrated CNN Models with Attention for Sentiment Analysis</a></h2><p>【情感分析】本文研究的内容是情感分析，论文的亮点在于提出了一种新的CNN+attention模型。本文适合在情感分析模型上有所突破的童鞋来读，从事相关工作的工程师或数据科学家也适合粗读一下。</p>
<h2 id="A-Language-independent-and-Compositional-Model-for-Personality-Trait-Recognition-from-Short-Texts"><a href="#A-Language-independent-and-Compositional-Model-for-Personality-Trait-Recognition-from-Short-Texts" class="headerlink" title="A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts"></a><a href="https://arxiv.org/pdf/1610.04345v1.pdf" target="_blank" rel="external">A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts</a></h2><p>【用户画像】本文研究的问题是从短文本中学习用户画像，提出了一种深度学习模型，模型上从学术上讲没有太多亮点，适合工业界从事相关工作的童鞋阅读。</p>
<h2 id="Neural-Machine-Translation-Advised-by-Statistical-Machine-Translation"><a href="#Neural-Machine-Translation-Advised-by-Statistical-Machine-Translation" class="headerlink" title="Neural Machine Translation Advised by Statistical Machine Translation"></a><a href="https://arxiv.org/pdf/1610.05150v1.pdf" target="_blank" rel="external">Neural Machine Translation Advised by Statistical Machine Translation</a></h2><p>【机器翻译】NMT翻译流利但有时翻译不准，SMT翻译准确但不够流利，两者各有优劣。本文结合了两种方法的优点，提出了在NMT解码阶段，用SMT来做辅助，通过一种门机制来选择用SMT还是NMT生成。</p>
<h2 id="Interactive-Attention-for-Neural-Machine-Translation"><a href="#Interactive-Attention-for-Neural-Machine-Translation" class="headerlink" title="Interactive Attention for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1610.05011v1.pdf" target="_blank" rel="external">Interactive Attention for Neural Machine Translation</a></h2><p>【机器翻译】【注意力模型】注意力模型证明了其强大威力，本文提出了一种新的注意力模型，INTERACTIVE ATTENTION，在encoder和decoder之间通过读和写操作进行交互，实验中对比了其他注意力模型，结果不错。</p>
<h2 id="A-General-Framework-for-Content-enhanced-Network-Representation-Learning"><a href="#A-General-Framework-for-Content-enhanced-Network-Representation-Learning" class="headerlink" title="A General Framework for Content-enhanced Network Representation Learning"></a><a href="https://arxiv.org/pdf/1610.02906v3.pdf" target="_blank" rel="external">A General Framework for Content-enhanced Network Representation Learning</a></h2><p>【社交网络】本文研究的是社交网络中各个节点的表示问题，亮点在于考虑了node（比如：用户）的相关文本信息，从文本中挖掘出node的一些特性（比如：性别、职业、爱好等），对node的刻画更精准。利用富文本信息来刻画社交网络中的各个node，在广告、推荐系统等应用方面都会带来很大的价值，这也正是nlp的价值所在，从杂乱无章的非结构文本中挖掘出大量的有用信息，本文适合研究社交网络价值、推荐系统的童鞋深入阅读。本文工作来自哈工大刘挺老师组。</p>
<h2 id="Reasoning-with-Memory-Augmented-Neural-Networks-for-Language-Comprehension"><a href="#Reasoning-with-Memory-Augmented-Neural-Networks-for-Language-Comprehension" class="headerlink" title="Reasoning with Memory Augmented Neural Networks for Language Comprehension"></a><a href="https://arxiv.org/pdf/1610.06454v1.pdf" target="_blank" rel="external">Reasoning with Memory Augmented Neural Networks for Language Comprehension</a></h2><p>【机器阅读】本文提出了一种做假设检验的神经网络方法（Neural Semantic Encoders），并且应用在机器阅读任务上，取得了不错的效果，涉及的数据集是CBT和WDW。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="STC短文本对话数据"><a href="#STC短文本对话数据" class="headerlink" title="STC短文本对话数据"></a><a href="http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm" target="_blank" rel="external">STC短文本对话数据</a></h2><p>【中文对话数据】对话很热，但依然很难！华为诺亚方舟实验室在NTCIR-13组织的关于短文本对话(Short-Text Conversation)的比赛已经开始注册了，让我们一起从大数据中探求人类对话的本质！比赛有两个任务，一个是基于检索给出response，一个是直接生成response。数据来自微博，输入是微博内容，输出是评论内容。13年华为的文章提到的短文本对话数据集可能就是指该数据集，一直愁对话数据的各位可以看过来，你们等的数据来了！</p>
<h2 id="DataHub"><a href="#DataHub" class="headerlink" title="DataHub"></a><a href="https://datahub.io/dataset" target="_blank" rel="external">DataHub</a></h2><p>一个收集各种数据集的网站。</p>
<h2 id="t-SNE可视化工具的python和torch封装"><a href="#t-SNE可视化工具的python和torch封装" class="headerlink" title="t-SNE可视化工具的python和torch封装"></a><a href="https://github.com/DmitryUlyanov/Multicore-TSNE" target="_blank" rel="external">t-SNE可视化工具的python和torch封装</a></h2><p>DmitryUlyanov/Multicore-TSNE: Parallel t-SNE implementation with Python and Torch wrappers</p>
<h2 id="Jiwei-Li关于聊天机器人NLG问题的slide"><a href="#Jiwei-Li关于聊天机器人NLG问题的slide" class="headerlink" title="Jiwei Li关于聊天机器人NLG问题的slide"></a><a href="http://web.stanford.edu/class/cs224u/materials/cs224u-2016-li-chatbots.pdf" target="_blank" rel="external">Jiwei Li关于聊天机器人NLG问题的slide</a></h2><p>分享一个斯坦福大学Jiwei Li关于聊天机器人NLG问题的slide，Jiwei Li是一个非常高产的作者，这个slide包括了非常多精彩的内容。 </p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Cached-Long-Short-Term-Memory-Neural-Networks-for-Doc
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly十期总结</title>
    <link href="http://rsarxiv.github.io/2016/10/21/PaperWeekly%E5%8D%81%E6%9C%9F%E6%80%BB%E7%BB%93/"/>
    <id>http://rsarxiv.github.io/2016/10/21/PaperWeekly十期总结/</id>
    <published>2016-10-22T00:42:53.000Z</published>
    <updated>2016-10-23T02:38:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>首先，感谢大家关注PaperWeekly和阅读本文，本文的阅读大概花费您10分钟时间，来看一下PaperWeekly这十期（两个多月）内容走过来所经历的一些。</p>
<h1 id="一开始"><a href="#一开始" class="headerlink" title="一开始"></a>一开始</h1><p>PaperWeekly的第一步是从一篇对Andrew Ng的采访开始的，大概的意思是经常读论文是一种非常好的长期投资，回报率也会非常高。虽然之前也在博客上写过一系列《文本文摘》的文章，并有幸得到爱可可老师的转发，但并没有系统地将自己所读的paper进行整理，并写成清晰、简短的文章分享出来。</p>
<p>PaperWeekly最开始的文章都是单篇的文章，源自之前所读的文本摘要的博客，当时取weekly这个名字是因为想给自己留下一个偷懒的借口，毕竟一周写一篇压力不会太多，如果心血来潮或者闲暇时间多的话可以写几篇。</p>
<p>慢慢地养成了刷arxiv的习惯，刷的方向主要包括：cs.CL、cs.AI、cs.LG和cs.NE这四个，最多的是cs.CL。习惯是个可怕的东西，养成了之后是很难改，每天不到arxiv上看看，就会感觉生活缺了点什么。</p>
<p>闻道有先后，术业有专攻。我个人的眼界和所关注的东西是有限的，精力也是有限的，所以在挣扎了一段时间之后，终于决定打开大门，欢迎同样对自然语言处理和分享知识感兴趣和有热情的同学一起来做PaperWeekly，让更多对其他领域更加专业的同学加入进来，来丰富内容，同时也会保证更高的质量，目前PaperWeekly有30名左右的童鞋一起来写文章，根据应用领域分了四个组，小组只是为了方便组织一期一期的topic，这里欢迎有更多感兴趣的同学可以加入，来增加更多的组，来写更多不同形式、不同领域的文章。</p>
<p>算上开始我个人写的两期，到昨天发布的最新一期，一共是十期内容。十期，是我给自己定的一个小目标。当我决定让更多的人参与进来时，我给自己设定了一个小目标，就是成功运营到第十期。想一件事情很简单，说一件事情也不难，难的是做出来，并且可以坚持一直做下去。从第三期的ACL值得读，新团队小试牛刀，再到后面的一个小组一期内容，每一期内容都围绕一个topic展开，从最开始的缺乏各种规范，到现在有了一个稳定的、但不是那么健全的制度来确保运营和沟通的高效，我感觉的到PW每天都在成长，每天都在朝着一个更好的方向走着，虽然仍存在在各种各样的不足，但它在进步，并且在不断努力变得更好。</p>
<h1 id="公众号-微信群"><a href="#公众号-微信群" class="headerlink" title="公众号+微信群"></a>公众号+微信群</h1><p>公众号+微信群的模式带来了很多的方便，最初的想法是让对PW感兴趣的童鞋聚在一起，可以对某些感兴趣的topic进行讨论。微信群有个天然的优势在于用户粘性高，不管什么样的问题，大家都喜欢丢在群里讨论，但也有天然的劣势，讨论过程容易混乱，尝试过用slack来解决这个问题，slack的分组讨论功能非常适合我们的场景，但并没有培养出来这个使用习惯，就是因为微信的粘性太高了，大家就是喜欢在这里交流。</p>
<h2 id="Issue-1"><a href="#Issue-1" class="headerlink" title="Issue 1"></a>Issue 1</h2><p>一个群很快就到了500人，出现了一个棘手的问题，第二个群的人如果太少，几乎没有讨论意义，所以就想用什么办法可以打通两个群，让两个群的童鞋在同一时空内进行交流。抛出这个问题之后，大家给出了很多的建议，最后群里优秀的工程师@碱馒头童鞋做了一个消息转发机器人，并且牺牲了自己的微信号，每天给大家转发来自两个群里的每一条消息。</p>
<h2 id="Issue-2"><a href="#Issue-2" class="headerlink" title="Issue 2"></a>Issue 2</h2><p>群里常常会有很多精彩的讨论和高质量的问答内容，是一笔不小的资源财富，如何让这些资源保存并且整理下来是一个很有挑战性的问题。最开始的想法是，可不可以做一个文本摘要工具，每天从群里摘要出有意义的东西，如果有QA对，整理出QA对。将问题抛到群里之后，大家也是各种讨论，但最后还是拿不出一个靠谱的方案来。我们整天都在用机器学习，也都想通过人工智能来改变这个世界，来改变我们的生活，很多时候模型和工具都有，但缺少数据和需求，这次有了数据和需求，我们却无能为力了，感觉有一点点小讽刺。</p>
<p>后来换了个思路，可不可以通过一些特殊标记，将大家的QA对转发到另一个地方，并且组织起来内容。我想到了bbs，想法很简单，就是大家把Q和A都通过一些标记起来，通过一个小bot将信息转发到bbs的数据库中，通过bbs来保存这些讨论信息。在群里抛出这个问题后，有童鞋响应，并且想做一些尝试，他就是现在群里的转发机器人@种瓜 童鞋，一个非常喜欢钻研问题的童鞋，他是一个blogger，这里是他的博客地址<a href="http://blog.just4fun.site/" target="_blank" rel="external">http://blog.just4fun.site/</a> 。通过他的努力，群里添加了一个看起来很酷的bbs bot，很酷，但最终仍然没有改变大家的习惯，毕竟提问的童鞋并没有太高的期待，因为这个群没有人回答，他转身就会将问题扔到另一个群，总会有人回答他的，所以强行推广使用bbs bot很难，而且bbs bot会自动产生一些状态信息，会显得群里有一些杂乱。所以，现在bbs bot成了群里的一个彩蛋，一个好玩的东西，虽然没有被广泛应用，但我仍觉得这是一件很酷的事情。（现在bot火，很多平台上驻扎了大大小小的bot上万只，但有几只bot可以产生用户粘性呢？大多数都是现象级，从这个角度来看，改变一个用户的习惯是多么困难的一件事情！）</p>
<p>说到彩蛋，群里还有一个彩蛋，就是一个基于StackOverFlow的QA bot，通过特定的表情符号来提问，系统会返回一个相关的答案，实现的大概思路是用google在stack上找答案，然后取排名最高的答案返回给用户，为了让群里的童鞋可以用中文来提问，特意加了一层翻译功能。</p>
<p>好玩的事情一、两个人在没意思，要是有更多感兴趣的童鞋可以加入，功能将会更加丰富和实用。（有感兴趣的童鞋可以私信我）</p>
<h1 id="一些时间点"><a href="#一些时间点" class="headerlink" title="一些时间点"></a>一些时间点</h1><p>2016.05.08 PW发布第一篇文章，《Generating News Headlines with Recurrent Neural Networks》</p>
<p>2016.08.05 PW发布第一期文章，包括三篇文章：《DeepIntent: Learning Attentions for Online Advertising with Recurrent Neural Networks》、《A Neural Knowledge Language Model》、《Neural Sentence Ordering》</p>
<p>2016.09.01 PW发布组建团队后的第一期文章，包括十篇ACL 2016的paper</p>
<p>2016.09.17 PW在群里正式上线了一个同步消息的bot，感谢@碱馒头 童鞋</p>
<p>2016.09.29 PW在群里正式上线了一个bbs bot，感谢@种瓜 童鞋</p>
<p>2016.10.07 PW在群里正式上线了一个QA bot，感谢@种瓜 童鞋</p>
<h1 id="一些数字"><a href="#一些数字" class="headerlink" title="一些数字"></a>一些数字</h1><p>PW在上线运营的这小半年以来，一共：</p>
<p>发布了113篇文章</p>
<p>完成了101篇paper的解读</p>
<p>推荐了80篇高质量paper</p>
<p>分享了20个高质量资源</p>
<p>吸引了30位学生和工程师参与写文章</p>
<h1 id="接下来"><a href="#接下来" class="headerlink" title="接下来"></a>接下来</h1><p>PW永远都处在beta状态，可能变化地很慢，但一定在努力朝着一个正确的方向改变。于是，PW在原有基础上有了一些新的思路：</p>
<p>定位：<br>1、对于学术界，推荐最新的高质量paper，起到一个导读作用；同时以topic为牵引，归纳和总结相似topic的paper。<br>2、对于工业界，推荐实用的或者新颖的paper，起到一个介绍作用；同时不定期的约稿写文章，系统地讲某一个领域、剖析某一个框架、精讲某一篇文章等等等等。</p>
<p>模式：<br>1、小组（不定期）：同之前一样，发起一个topic，做几篇相关的文章，形式变化不大。<br>2、arXiv（定期）：写作形式与之前一样，每周从arXiv上过滤出几篇高质量文章（PaperWeekly官方微博上每天过滤出的好paper作为候选），以周为单位解读最新的paper给大家。<br>3、约稿（不定期）：写作形式不限，可详细解读一篇文章，可写一个方向（比如：文本摘要），也可以与代码、框架有关的内容，也期待大家的投稿。</p>
<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>十期内容，经历了两个多月的时间，60多天是一段漫长的时间，感谢大家的一路相伴和支持。</p>
<p>感谢踊跃加入PW写作团队的你们：magic282sub、陈哲乾、destinwang、yangzhiye、david、brantyz、AllenCai、anngloves、cheezer94、tonya、gcyydxf、guoxh、EdwardHux、hxw2303632、jaylee1992、jian.zhou.cool、lshowway、memray、mygod9、美好时光海苔、cain、王迁、xy504、Susie-nmt、褚则伟、zhang1028kun、zhaosanqiang、zhaoyue、zhoussneu、zeng<br>，也期待更多的童鞋可以加入写作团队。</p>
<p>感谢加入PW讨论群的童鞋，感谢你们贡献了很多精彩的讨论。</p>
<p>感谢机器之心的支持和宣传，看着你们一路走来，逐渐地成长和壮大，有一种榜样的力量！</p>
<p>感谢帮忙分享和推广的各种大牛们，谢谢你们让更多的人知道了PW。</p>
<p>感谢留言提意见的童鞋们，有时时间紧张，不能一一回复，有时精力有限，无法满足每一位的需求，但还是感谢你们的期待和支持！</p>
<p>第一个十期结束了，我不知道后面会有多少个十期，希望可以一直坚持做下去。放弃可以找到很多种借口，但坚持下来只需要一个理由，因为热爱！</p>
<h1 id="PW-Ebook"><a href="#PW-Ebook" class="headerlink" title="PW Ebook"></a>PW Ebook</h1><p>我将PW的十期内容汇总成一本电子书，大家可以从<a href="http://www.kancloud.cn/mcgrady164/paperweekly" target="_blank" rel="external">http://www.kancloud.cn/mcgrady164/paperweekly</a> 下载阅读，里面的文章会随着PW的更新不断地更新。</p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引&quot;&gt;&lt;a href=&quot;#引&quot; class=&quot;headerlink&quot; title=&quot;引&quot;&gt;&lt;/a&gt;引&lt;/h1&gt;&lt;p&gt;首先，感谢大家关注PaperWeekly和阅读本文，本文的阅读大概花费您10分钟时间，来看一下PaperWeekly这十期（两个多月）内容走过来所经
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十期</title>
    <link href="http://rsarxiv.github.io/2016/10/20/PaperWeekly-%E7%AC%AC%E5%8D%81%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/10/20/PaperWeekly-第十期/</id>
    <published>2016-10-21T06:30:26.000Z</published>
    <updated>2016-10-21T06:39:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引"><a href="#引" class="headerlink" title="引"></a>引</h2><p>本期PaperWeekly的主题是基于翻译模型(Trans系列)的知识表示学习，主要用来解决知识表示和推理的问题。表示学习旨在将研究对象的语义信息表示为稠密低维实值向量，知识表示学习主要是面向知识图谱中的实体和关系进行表示学习。使用建模方法将实体和向量表示在低维稠密向量空间中，然后进行计算和推理。一般而言的应用任务为triplet classification 和link prediction.自从2013年TransE模型提出后，产生了一系列模型对TransE模型进行改进和补充,比如TransH、TransG等等。本期PaperWeekly主要提供了Trans系列的7篇文章供大家赏读。</p>
<p>paper目录：<br>（1）TransE，NIPS2013，Translating embeddings for modeling multi-relational data。<br>（2）TransH，AAAI2014，Knowledge graph embedding by translating on hyperplanes。<br>（3）TransD，ACL2015，Knowledge graph embedding via dynamic mapping matrix。<br>（4）TransA，arXiv2015，An adaptive approach for knowledge graph embedding。<br>（5）TransG，arxiv2015，A Generative Mixture Model for Knowledge Graph Embedding)<br>（6）KG2E，CIKM2015，Learning to represent knowledge graphs with gaussian embedding。<br>（7）TranSparse，AAAI2016，Knowledge graph completion with adaptive sparse transfer matrix。 </p>
<h1 id="TransE-Translating-Embeddings-for-Modeling-Multi-relational-Data"><a href="#TransE-Translating-Embeddings-for-Modeling-Multi-relational-Data" class="headerlink" title="TransE:Translating Embeddings for Modeling Multi-relational Data"></a><a href="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="external">TransE:Translating Embeddings for Modeling Multi-relational Data</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>CNRS, Google inc.</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>Embedding entities and relationships, Multi-relational data, link prediction</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>NIPS 2013/12</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>如何建立简单且易拓展的模型把知识库中的实体和关系映射到低维向量空间中，从而计算出隐含的关系？</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>传统训练知识库中三元组(head,relation,tail)建模的方法参数特别多，导致模型太复杂难以解释，并且需要很大的计算代价，很容易出现过拟合或欠拟合问题。而简单的模型在表现上与复杂的模型几乎一样，但更易拓展。TransE的训练过程如下图：</p>
<p><img src="media/TransE_1.png" alt=""></p>
<p>TransE模型的训练中，第12步是损失函数，对E和L做uniform初始化之后，让正确的h+l-t结果趋近于0，让错误的h‘+l-t’的结果变大，损失函数结果大于0取原值，小于0则取0，这种hinge loss function可以尽可能的将对和错分开，模型使用SGD训练，每次更新可以只更新这个batch里的三元组的向量，因为参数之间并没有冲突。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>数据集 WordNet    <a href="http://wordnet.princeton.edu/wordnet/download/" target="_blank" rel="external">http://wordnet.princeton.edu/wordnet/download/</a><br>数据集 Freebase   <a href="http://developers.google.com/freebase/" target="_blank" rel="external">http://developers.google.com/freebase/</a><br>Code: <a href="https://github.com/thunlp/KB2E" target="_blank" rel="external">https://github.com/thunlp/KB2E</a></p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本文提出了一种将实体与关系嵌入到低维向量空间中的简单模型，弥补了传统方法训练复杂、不易拓展的缺点。尽管现在还不清楚是否所有的关系种类都可以被本方法建模，但目前这种方法相对于其他方法表现不错。TransE更是作为知识库vector化的基础，衍生出来了很多变体。</p>
<h1 id="TransH-Knowledge-Graph-Embedding-by-Translating-on-Hyperplanes"><a href="#TransH-Knowledge-Graph-Embedding-by-Translating-on-Hyperplanes" class="headerlink" title="TransH:Knowledge Graph Embedding by Translating on Hyperplanes"></a><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531" target="_blank" rel="external">TransH:Knowledge Graph Embedding by Translating on Hyperplanes</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Zhen Wang1, Jianwen Zhang2, Jianlin Feng1, Zheng Chen2</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Sun Yat-sen University<br>microsoft</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>knowledge graph embedding, Multi-relational data</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>AAAI 2014</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>对知识库中的实体关系建模,特别是一对多,多对一,多对多的关系。设计更好的建立负类的办法用于训练。 </p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>过去指示图库建模的方法参数过多, TransE在一定程度上解决了这个问题, 但是TransE过于简单，很难对一对多,多对一和多对多关系建模。所以为了平衡模型复杂度和建模效果，TransH将把关系映射到另一个空间（如下图 ）。 注意: 这种想法和Distant Model (Bordes et al. 2011)很相似，但是TransH用了更少的参数， 因为TransH假设关系是向量而不是距离。<br><img src="media/TransH_1.png" alt="TransH_1"></p>
<p>这个模型的一个亮点就是用尽量少的参数对复杂的关系建模。 下图罗列了相关工作的模型以及复杂度。图中可以看到从TransE到TransH并没有添加太多的参数（Unstructured只是TransE简化版）。Bilinear，Single Layer， NTN对关系或者实体进行了非线性的转换，作者认为是没有必要的（增加了模型复杂度）。</p>
<p><img src="media/TransH_2.png" alt="TransH_2"></p>
<p>TransH模型的训练和TransE类似 （SGD优化） ，下面是损失函数（因为一些限制，后面加入了拉格朗日乘数）。论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。<br><img src="media/TransH_3.png" alt="TransH_3"></p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>数据集 WordNet:<a href="http://wordnet.princeton.edu/wordnet/download/" target="_blank" rel="external">http://wordnet.princeton.edu/wordnet/download/</a><br>数据集 Freebase:  <a href="http://developers.google.com/freebase/" target="_blank" rel="external">http://developers.google.com/freebase/</a><br>Code:<a href="https://github.com/thunlp/KB2E" target="_blank" rel="external">https://github.com/thunlp/KB2E</a></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>（1）TransE (Bordes et al. 2013b): 和TransH相比，它没有将关系映射到另一个空间，关系由一个向量r表示。<br>（2）Unstructured Model：简化版的TransE，假设r = 0。<br>（3）Structured Embedding：  使用了两个关系相关的矩阵，分别用于头h和尾t，评估函数为:<br><img src="media/TransH_4.PNG" alt=""><br>该方法并没有抓住实体和关系之间的关系。<br>（4）Single Layer Model(SLM)：使用了神经网络，评估函数为:<br><img src="media/TransH_5.PNG" alt=""><br>（5）Distant Model (Bordes et al. 2011)：它将实体映射到另一个空间，然后假定关系是距离而不是向量（因为用了2个不同矩阵映射实体，所以对实体关系建模并不是很好）。<br>（6）Bilinear Model (Jenatton et al. 2012; Sutskever, Tenen- baum, and Salakhutdinov 2009)，Single Layer Model (Socher et al. 2013)，NTN (Socher et al. 2013)：他们都是使用非线性函数映射实体，这样模型表达能力虽然好但是太多参数也太复杂了（容易过拟合）。</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>论文提出的TransH模型，为了解决TransE对一对多，多对一，多对多关系建模的难题。它权衡模型复杂度和模型表达能力。而且还设计了复杂取样的办法用于训练。</p>
<h1 id="TransD-knowledge-graph-embedding-via-dynamic-mapping-matrix"><a href="#TransD-knowledge-graph-embedding-via-dynamic-mapping-matrix" class="headerlink" title="TransD: knowledge graph embedding via dynamic mapping matrix"></a><a href="http://www.aclweb.org/anthology/P15-1067.pdf" target="_blank" rel="external">TransD: knowledge graph embedding via dynamic mapping matrix</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu and Jun Zhao</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>中国科学院自动化研究所  National Laboratory of Pattern Recognition (NLPR)</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>knowledge graph embedding, link prediction.</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL2015</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>知识图谱中的link prediction。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>在link prediction上的TransE扩展模型，函数仍然为:   </p>
<p> <img src="media/TransD_1.PNG" alt=""> </p>
<p>但h丄和t丄为entity向量h和entity向量t在该relation r上的投影表示。投影定义为：</p>
<p> <img src="media/TransD_2.PNG" alt=""></p>
<p>其中(h_p)^T为某entity的投影向量，h为该entity的表示向量。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>数据集 WordNet <a href="http://wordnet.princeton.edu/" target="_blank" rel="external">http://wordnet.princeton.edu/</a><br>数据集 FreeBase <a href="https://developers.google.com/freebase/" target="_blank" rel="external">https://developers.google.com/freebase/</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>如果TransD的所有投影向量为0，TransD就是TransE。类似的还有TransR/CTransR，他们对每个relation定义了一个mapping矩阵，参数更多计算复杂度更大。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>模型只涉及vector的相乘，因此计算复杂度较小，效果也取得了state-of-the-art，适合用于规模很大的知识图谱。</p>
<h1 id="TransA-An-Adaptive-Approach-for-Knowledge-Graph-Embedding"><a href="#TransA-An-Adaptive-Approach-for-Knowledge-Graph-Embedding" class="headerlink" title="TransA:An Adaptive Approach for Knowledge Graph Embedding"></a><a href="https://arxiv.org/pdf/1509.05490v2.pdf" target="_blank" rel="external">TransA:An Adaptive Approach for Knowledge Graph Embedding</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Hao Xian, Minlin  Huang,  Hao Yu,  Xiaoyan  Zhu</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位:"></a>单位:</h2><p> 清华大学  State Key Lab on Intelligent Technology and Systems</p>
<h2 id="关键词："><a href="#关键词：" class="headerlink" title="关键词："></a>关键词：</h2><p>knowledge graph embedding,  elliptical equipotential hypersurfaces,  metric learning.</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>如何解决了translation-based 知识表示方法存在的过于简化损失度量，没有足够竞争力去度量知识库中实体/关系的多样性和复杂性问题。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>知识图谱在AI搜索和应用中扮演着越来越重要的角色，但是它是符号表示，有一定的逻辑性的，因此如何表示这些关系就成了一个很大的挑战，为了解决这个挑战，很多模型如TransE, TransH, TransR纷纷被提出来，在这些模型中，基于几何关系的方法是很重要的一个分支，而基于几何关系的方法是使用K维的向量表示实体或者关系，然后利用一个函数f_r(h,t)来度量三元组(h, r, t)，而他们都是基于一个准则h+r=t。<br>因此就使用了同一个损失度量h+r=t，这种损失度量其实是利用了在一个球形等价超平面，越接近中心，三元组的可信度越高，因此从未匹配的t中寻找合适的t就变得很苦难，同时这种方法也很难处理一对多，多对一，多对多的关系。因此这些方法不够灵活。<br>具体可以从图1(a)看出。同时这种方法将等价对待向量中的每一维，但实际上各个维度的重要性是不同的，只有一些维度是有效的，其他维度可以认为是噪音，会降低效果，具体见图2(a).</p>
<p>因此作者提出了另一种损失度量函数</p>
<p><img src="media/TransA-2.PNG" alt=""></p>
<p>通过增加一个矩阵Wr​，首先利用了一个椭圆等价超平面，解决了上述问题1，具体见图1(b)；同时利用LDL分解，公式变为:</p>
<p><img src="media/TransA-3.PNG" alt=""></p>
<p>其中D_r就是一个对角阵，而对角阵中的每个值的大小，正好说明了每一维的不同重要程度，也就解决了上述问题2，具体减图2(b)。</p>
<p><img src="media/TransA-4.JPG" alt="figure 1"><br>图1<br><img src="media/TransA-5.JPG" alt="figure 2"><br>图2</p>
<h2 id="资源-3"><a href="#资源-3" class="headerlink" title="资源"></a>资源</h2><p>数据集 Wordnet <a href="http://wordnet.princeton.edu/" target="_blank" rel="external">http://wordnet.princeton.edu/</a><br>数据集 FreeBase <a href="https://developers.google.com/freebase/" target="_blank" rel="external">https://developers.google.com/freebase/</a></p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>如模型部分介绍的，当前的一些现有模型都是基于一个准则h+r=t，因此就使用了同一个损失度量h_r+r=t_r，只是在h_r和t_r的表示上有不同：</p>
<p>（1）TransE  h_r = h, t_r = t<br>（2）TransH  h_r = h - (w_r)^T.h.w_r,  t_r = t - (w_r)^T.t.w_r<br>（3）TransR  h_r = M_r.h,  t_r = M_r.t<br>（4）TransM则是预先计算了出每一个训练三元组的直接权重</p>
<p>还有很多类似的模型，这里就不再介绍了。</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>感觉这篇文章的思路比较简单，就是针对当前模型的一些不足，更换了一个损失度量函数。但是几点还是值得学习的，首先通过图像来描述不同的损失度量函数，给人一个更直观的感觉；其次针对向量表示中的区别对待，感觉很有attention mechanism的感觉，对不同的triple关注向量表示的不同维度，以取得最好的效果，这点是非常值得借鉴参考的。</p>
<h1 id="TransG-A-Generative-Mixture-Model-for-Knowledge-Graph-Embedding"><a href="#TransG-A-Generative-Mixture-Model-for-Knowledge-Graph-Embedding" class="headerlink" title="TransG : A Generative Mixture Model for Knowledge Graph Embedding"></a><a href="https://arxiv.org/abs/1509.05488" target="_blank" rel="external">TransG : A Generative Mixture Model for Knowledge Graph Embedding</a></h1><h2 id="作者-4"><a href="#作者-4" class="headerlink" title="作者"></a>作者</h2><p> Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu</p>
<h2 id="单位-4"><a href="#单位-4" class="headerlink" title="单位"></a>单位</h2><p>清华大学  State Key Lab on Intelligent Technology and Systems</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>knowledge graph embedding, generative mixture model, multiple relration semantics.</p>
<h2 id="文章来源-4"><a href="#文章来源-4" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv2015</p>
<h2 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h2><p>解决多关系语义(multiple relation semantics)的问题。</p>
<h2 id="模型-4"><a href="#模型-4" class="headerlink" title="模型"></a>模型</h2><p>传统的基于翻译的模型采用h_r+r= t_r(其中，h_r为头部实体，t_r为尾部实体，r为头部<br>实体跟尾部实体的关系)，仅仅对一个关系赋予一种翻译向量。<br>它们不能细分多关系语义，比如，(Atlantics, HasPart, NewYorkBay)和(Table, HasPart, Leg)两个的关系都是HasPart，但是这两个的关系在语义上不同，第一个是“部件”的关系，第二个是“位置”的关系。TransG能够解决关系的多语义问题。如图所示，多关系语义分析可以提高三元组的分类准确度。</p>
<p><img src="media/TransG.png" alt="figure 1"></p>
<p>TransG利用贝叶斯非参数无限混合模型对一个关系生成多个翻译部分，根据三元组的特定语义得到当中的最佳部分。最大数据相似度原理用来训练，优化采用SGD。实验结果在link prediction和triple classification这两种任务上都优于目前最好的结果，运行速度与TransE(最快的方法)成正相关，系数为关系语义部分的数目。</p>
<h2 id="资源-4"><a href="#资源-4" class="headerlink" title="资源"></a>资源</h2><p>数据集 WordNet    <a href="http://wordnet.princeton.edu/wordnet/download/" target="_blank" rel="external">http://wordnet.princeton.edu/wordnet/download/</a><br>数据集 Freebase   <a href="http://developers.google.com/freebase/" target="_blank" rel="external">http://developers.google.com/freebase/</a></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p>大多数都已介绍，这里就只说明CTransR，其中关系的实体对被分类到不同的组，同一组的实体对共享一个关系向量。相比较而言，TransG不需要对聚类的预处理。</p>
<h2 id="简评-4"><a href="#简评-4" class="headerlink" title="简评"></a>简评</h2><p>这篇文章的idea比较重要，考虑到一种关系存在的多语义问题，相当于对关系进行了细化，就是找到关系的隐形含义，最终从细化的结果中选出一个最佳的关系语义。这个在应用中很有意义，不同的语义可能需要不同的应对方法，可以借鉴。</p>
<h1 id="KG2E-KG2E-learning-to-represent-knowledge-graphs-with-gaussian-embedding"><a href="#KG2E-KG2E-learning-to-represent-knowledge-graphs-with-gaussian-embedding" class="headerlink" title="KG2E:KG2E_learning to represent knowledge graphs with gaussian embedding"></a><a href="http://dl.acm.org/citation.cfm?id=2806502" target="_blank" rel="external">KG2E:KG2E_learning to represent knowledge graphs with gaussian embedding</a></h1><h2 id="作者-5"><a href="#作者-5" class="headerlink" title="作者"></a>作者</h2><p>Shizhu He, Kang Liu, Guoliang Ji and Jun Zhao</p>
<h2 id="单位-5"><a href="#单位-5" class="headerlink" title="单位"></a>单位</h2><p>National Laboratory of Pattern Recognition<br>Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China</p>
<h2 id="关键词-4"><a href="#关键词-4" class="headerlink" title="关键词"></a>关键词</h2><p>Distributed Representation, Gaussian Embedding, Knowledge Graph</p>
<h2 id="文章来源-5"><a href="#文章来源-5" class="headerlink" title="文章来源"></a>文章来源</h2><p>CIKM 2015</p>
<h2 id="问题-5"><a href="#问题-5" class="headerlink" title="问题"></a>问题</h2><p>本文所解决的问题是知识图谱的表示问题（即将知识图谱表示为低维连续向量空间），本文使用Gaussian Distribution 来表示实体和关系，提出了用Gaussian Distribution的协方差来表示实体和关系的不确定度的新思想，提升了已有模型在link prediction和triplet classification问题上的准确率。</p>
<h2 id="模型-5"><a href="#模型-5" class="headerlink" title="模型"></a>模型</h2><p>传统的表示学习的表示学习的方法和计算比较复杂，自TransE模型诞生后，很多模型都是在TransE的基本思想上加以改进，KG2E模型也是一样。<br>KG2E模型使用高斯分布来表示实体和关系。<br>模型实例见下图：<br><img src="media/KG2E_example.png" alt="model example"></p>
<p>每个圆圈代表不同实体与关系的表示，它们分别于“Bill Clinton”构成三元组关系，圆圈大小表示的是不同实体或关系的不确定度。</p>
<p>模型算法流程图如下：<br><img src="media/KG2E_Algorithm.png" alt="model algorithm"></p>
<p>算法解读：<br>输入：训练集三元组，KG中所有的实体和关系，以及其它的一些参数。<br>输出：KG中所有实体和关系建模后生成的Gaussian Embeddings.（主要包含两个部分，均值（向量）和协方差（矩阵））<br>line 1到line 4主要是数据的归一化<br>line 5到line 15是算法实现部分：模型采用的是minibatch的训练方法，每一个minibatch的训练中都会进行负采样，并将负采样的样例和正例样例混合在一起学习，然后使用评分函数进行评估，要达到的目的是正例三元组的得分比负例三元组高或者低（高低取决于具体的评分而函数的设定）。在一次一次的迭代中不断更新结果，最后将得到的means和covariance进行正则化。</p>
<p>文章核心公式：<br>（1）评分函数<br><img src="media/KG2E_Score_function.png" alt="score function"></p>
<p>（2）KL散度的能量函数</p>
<p><img src="media/KG2E_KL_function.png" alt="KL energy function"></p>
<p>（3）期望概率能量函数<br><img src="media/KG2E_EL_function.png" alt="EL energy function"></p>
<h2 id="资源-5"><a href="#资源-5" class="headerlink" title="资源"></a>资源</h2><p>数据集：<br>    <a href="https://github.com/Mrlyk423/Relation_Extraction/blob/master/data.zip" target="_blank" rel="external">WN18</a><br>    <a href="https://github.com/dddoss/tensorflow-socher-ntn/tree/master/data/Wordnet" target="_blank" rel="external">WN11</a><br>    <a href="https://github.com/dddoss/tensorflow-socher-ntn/tree/master/data/Freebase" target="_blank" rel="external">FB13K</a><br>    <a href="https://github.com/Mrlyk423/Relation_Extraction/blob/master/data.zip" target="_blank" rel="external">FB15K</a></p>
<h2 id="相关工作-4"><a href="#相关工作-4" class="headerlink" title="相关工作"></a>相关工作</h2><p>（1）TransR，2015年AAAI，Learning entity and relation embeddings for knowledgh completition。</p>
<h2 id="简评-5"><a href="#简评-5" class="headerlink" title="简评"></a>简评</h2><p>创新点：<br>    （1）以前的文章是属于point-based，KG2E是属于density-based的。<br>    （2）提出了(un)certainty的概念，在建模过程中融入了关系和实体语义本身的不确定性的知识，使用高斯分布的协方差表示该实体或关系的不确定度，高斯分布的均值表示实体或关系在语义空间中的中心值。<br>    （3）使用了新的score funciton：KL-divergence和expected likelihood<br>应用场景：link prediction，triplet classification,knowledge reasoning<br>不足之处：本文提出的方法在link prediction的many-to-many relations上的预测性能不是很好，主要原因是KG2E模型没有考虑实体的类型和粒度。</p>
<p>7.TranSparse</p>
<h1 id="Knowledge-Graph-Completion-with-Adaptive-Sparse-Transfer-Matrix"><a href="#Knowledge-Graph-Completion-with-Adaptive-Sparse-Transfer-Matrix" class="headerlink" title="Knowledge Graph Completion with Adaptive Sparse Transfer Matrix"></a><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11982/11693" target="_blank" rel="external">Knowledge Graph Completion with Adaptive Sparse Transfer Matrix</a></h1><h2 id="作者-6"><a href="#作者-6" class="headerlink" title="作者"></a>作者</h2><p>Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao</p>
<h2 id="单位-6"><a href="#单位-6" class="headerlink" title="单位"></a>单位</h2><p>中科院模式识别国家重点实验室</p>
<h2 id="关键词-5"><a href="#关键词-5" class="headerlink" title="关键词"></a>关键词</h2><p>Knowledge Graph Embedding,Sparse Matrix</p>
<h2 id="文章来源-6"><a href="#文章来源-6" class="headerlink" title="文章来源"></a>文章来源</h2><p>AAAI 2016</p>
<h2 id="问题-6"><a href="#问题-6" class="headerlink" title="问题"></a>问题</h2><p>针对不同难度的实体间关系，使用不同稀疏程度的矩阵（不同数量的参数）来进行表征，从而防止对复杂关系欠拟合或者对简单关系过拟合。</p>
<h2 id="模型-6"><a href="#模型-6" class="headerlink" title="模型"></a>模型</h2><p>本文的模型与TransR类似，即对每一个关系r学习一个转换矩阵M_r,将h和t的向量映射到关系向量所在的空间。</p>
<p>不过本文注意到knowledge graph中面临两个问题，分别是heterogeneous（有的实体关系十分复杂，连接许多不同的实体）和unbalanced（很多关系连接的head和tail数目很不对等）。如果只使用一个模型应对所有情况的话可能会导致对复杂关系underfit，对简单关系overfit。因此本文认为需要对症下药，复杂的关系就需要下猛药（用有更多的参数的复杂模型），简单关系就简单处理（较少的参数）。</p>
<p>但是怎么实现这样灵活的建模？在方法上本文借用了SparseMatrix，如果关系比较复杂就用比较稠密的矩阵，如果关系简单则用稀疏矩阵进行表达。文章假设关系的复杂程度正比于包含该关系的triplet数目，并根据两类问题提出了对应的稀疏矩阵初始化方法。不过并没有提出同时解决两类问题的统一方案。</p>
<ul>
<li>针对heterogeneity问题的模型叫做TranSparse(share)，模型参数sparse degree，theta_r，是由下列公式确定:</li>
</ul>
<p><img src="media/TranSparse_equation1.png" alt="alt text"><br>其中N_r是该关系r所连接的triplet数目，N_r*是数据集中最大的关系triplet数目。通过这个sparse degree我们就可以确定参数矩阵的稀疏程度了。entity的向量通过下式进行转换：<br><img src="media/TranSparse_equation2.png" alt="alt text"></p>
<ul>
<li>针对imbalance问题提出的TranSparse(separate)方法也十分类似，即在关系的head和tail两端使用不同复杂度的matrix。sparse degree的公式与上面TranSparse(share)的几乎一样，只不过N_r和N_r*替换成了entity的个数。如果某一端要连接更多不同的entity，那么这一端就需要更复杂的模型来表征（matrix有更多非零参数）。</li>
</ul>
<p>确定这个sparse degree之后，我们就可以初始化对应的稀疏参数矩阵了（原文中提到了Structured与Unstructured两种矩阵形式）。目标函数以及训练过程与其他工作一致，只不过在进行训练时我们只对矩阵中的非零部分进行更新。</p>
<p>最后模型在triplet分类和链接预测任务上进行实验，相比于先前模型取得了更好的成绩，不过相比于TranD优势并不十分明显。提出的两个模型中TranSparse(separate)的表现更好。</p>
<h2 id="资源-6"><a href="#资源-6" class="headerlink" title="资源"></a>资源</h2><p>数据集 WordNet    <a href="http://wordnet.princeton.edu/wordnet/download/" target="_blank" rel="external">http://wordnet.princeton.edu/wordnet/download/</a><br>数据集 Freebase   <a href="http://developers.google.com/freebase/" target="_blank" rel="external">http://developers.google.com/freebase/</a></p>
<h2 id="相关工作-5"><a href="#相关工作-5" class="headerlink" title="相关工作"></a>相关工作</h2><p>上面的相关工作已经介绍差不多了，这里不再赘述。</p>
<h2 id="简评-6"><a href="#简评-6" class="headerlink" title="简评"></a>简评</h2><p>TranSparse模型主要是为了解决关系和实体的异质性和不平衡性而提出，问题针对性强。</p>
<h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>最近几年人们对知识表示方法的探究一直都在进行，知识表示学习对于计算机如何理解和计算知识的意义是重大的。在2013年embedding的思想出现之前，人们基本采用one-hot的表示方法来表示实体，近几年知识表示的核心思想就是如何找到合适的方法来将知识图谱emmbedding到向量空间，从而在向量空间中进行计算，并且也在这方面取得了不错的进展。</p>
<p>但知识表示学习仍然面临着挑战，主要包括以下几个方面：（1）对于多源知识融合的表示学习，如何将知识库中的文本等信息加入到学习中。（2）如何进行更加复杂的知识推理。（3）对于知识图谱无法表达的信息，应该进行如何表示和推理。（4）如何在知识库中融入常识信息。<br>参考文献说明：本文主要参考清华大学刘知远老师的《知识表示学习研究进展》这篇综述。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引&quot;&gt;&lt;a href=&quot;#引&quot; class=&quot;headerlink&quot; title=&quot;引&quot;&gt;&lt;/a&gt;引&lt;/h2&gt;&lt;p&gt;本期PaperWeekly的主题是基于翻译模型(Trans系列)的知识表示学习，主要用来解决知识表示和推理的问题。表示学习旨在将研究对象的语义信息表
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.10.10-2016.10.14</title>
    <link href="http://rsarxiv.github.io/2016/10/15/cs-CL-weekly-2016-10-10-2016-10-14/"/>
    <id>http://rsarxiv.github.io/2016/10/15/cs-CL-weekly-2016-10-10-2016-10-14/</id>
    <published>2016-10-15T17:39:35.000Z</published>
    <updated>2016-10-15T17:57:59.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Personalizing-a-Dialogue-System-with-Transfer-Learning"><a href="#Personalizing-a-Dialogue-System-with-Transfer-Learning" class="headerlink" title="Personalizing a Dialogue System with Transfer Learning"></a><a href="https://arxiv.org/pdf/1610.02891v1.pdf" target="_blank" rel="external">Personalizing a Dialogue System with Transfer Learning</a></h2><p>【对话系统】【迁移学习】面向具体任务的对话系统由于数据不充分，面临难以训练的尴尬境地。解决这一问题的方法之一是用迁移学习来做，本文提出了一种基于POMDP的迁移学习框架。并且在购买咖啡的实际场景中得到了应用，取得了不错的效果。港科大杨强老师是迁移学习领域的专家，而迁移学习是解决机器学习中领域数据过小问题的一种有效方法，现有的特有对话系统面临着这个问题，尤其是要求对话系统具有个性化的特点时。本文对于研究语音对话系统和聊天机器人都有一定的启发性。</p>
<h2 id="Dialogue-Session-Segmentation-by-Embedding-Enhanced-TextTiling"><a href="#Dialogue-Session-Segmentation-by-Embedding-Enhanced-TextTiling" class="headerlink" title="Dialogue Session Segmentation by Embedding-Enhanced TextTiling"></a><a href="https://arxiv.org/pdf/1610.03955v1.pdf" target="_blank" rel="external">Dialogue Session Segmentation by Embedding-Enhanced TextTiling</a></h2><p>【chatbot】【上下文处理】本文研究的内容是开放域聊天机器人context处理的问题，当前聊天的内容很大程度上都会与之前的聊天内容有相关，但并不是每一句都相关，因此算好相关度很有必要。</p>
<h2 id="Exploiting-Sentence-and-Context-Representations-in-Deep-Neural-Models-for-Spoken-Language-Understanding"><a href="#Exploiting-Sentence-and-Context-Representations-in-Deep-Neural-Models-for-Spoken-Language-Understanding" class="headerlink" title="Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding"></a><a href="https://arxiv.org/pdf/1610.04120v1.pdf" target="_blank" rel="external">Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding</a></h2><p>【对话系统】【深度学习】本文是steve young组的一篇新文，旨在探索CNN表示对话句子和LSTM表示上下文信息在对话理解问题上的效果，相比于传统方法，DNN方法鲁棒性更强。</p>
<h2 id="Latent-Sequence-Decompositions"><a href="#Latent-Sequence-Decompositions" class="headerlink" title="Latent Sequence Decompositions"></a><a href="https://arxiv.org/pdf/1610.03035v1.pdf" target="_blank" rel="external">Latent Sequence Decompositions</a></h2><p>【seq2seq】本文研究的内容是对seq2seq框架中输入和输出序列进行有意义分解的问题，而不是简单地分解为char，提出了一种Latent Sequence Decompositions框架，在语音识别问题上取得了不错的效果。其实不仅仅是语音识别问题，在用seq2seq框架时总会遇到OOV的问题，char是一种方法，但信息量太少，如果能够将word sequence分解为更加有意义的子序列，既兼顾了信息量，又降低了词表维度。对英文系的语言效果好一些，中文效果应该不会那么明显。</p>
<h2 id="Diverse-Beam-Search-Decoding-Diverse-Solutions-from-Neural-Sequence-Models"><a href="#Diverse-Beam-Search-Decoding-Diverse-Solutions-from-Neural-Sequence-Models" class="headerlink" title="Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models"></a><a href="https://arxiv.org/pdf/1610.02424v1.pdf" target="_blank" rel="external">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</a></h2><p>【seq2seq】seq2seq框架中在解码阶段，常常会用beam search来从左至右、贪心地生成N个最好的输出，不仅仅效率低下，而且在很多复杂任务中效果不好。本文提出了一种新的搜索算法。算法会在解空间内exploration和exploitation，通过设定diverse的目标进行训练，得到结果。相比之下，本文的方法更加高效。并且在image  caption、VQA和MT等任务中进行了验证。</p>
<h2 id="Gated-End-to-End-Memory-Networks"><a href="#Gated-End-to-End-Memory-Networks" class="headerlink" title="Gated End-to-End Memory Networks"></a><a href="https://arxiv.org/pdf/1610.04211v1.pdf" target="_blank" rel="external">Gated End-to-End Memory Networks</a></h2><p>【seq2seq】【memory networks】端到端的记忆网络在简单的机器阅读理解任务上取得了不错的效果，但复杂的事实问答和对话理解相关的任务处理的并不好，原因在于记忆单元与模型之间交互复杂。本文针对该问题，提出了一种Gated记忆网络，取得了不错的效果。本文模型在机器阅读理解bAbI dataset和task-oriented 对话系统任务DSTC2中均取得了非常好的结果。</p>
<h2 id="Neural-Paraphrase-Generation-with-Stacked-Residual-LSTM-Networks"><a href="#Neural-Paraphrase-Generation-with-Stacked-Residual-LSTM-Networks" class="headerlink" title="Neural Paraphrase Generation with Stacked Residual LSTM Networks"></a><a href="https://arxiv.org/pdf/1610.03098v3.pdf" target="_blank" rel="external">Neural Paraphrase Generation with Stacked Residual LSTM Networks</a></h2><p>【paraphrase】本文提出用多层残差LSTM网络来做paraphrase的任务，得到了比之前seq2seq以及seq2seq+attention更好的效果。转述在某个角度上和标题生成（句子level摘要）类似，方法可借鉴。</p>
<h2 id="SentiHood-Targeted-Aspect-Based-Sentiment-Analysis-Dataset-for-Urban-Neighbourhoods"><a href="#SentiHood-Targeted-Aspect-Based-Sentiment-Analysis-Dataset-for-Urban-Neighbourhoods" class="headerlink" title="SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods"></a><a href="https://arxiv.org/pdf/1610.03771v1.pdf" target="_blank" rel="external">SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods</a></h2><p>【观点挖掘】【数据集】本文给出了一个观点挖掘的数据集，数据源来自Yahoo问答中与London相关的提问。这个数据集适合这样的场景，一段评论中包含了多个entity的多个aspect的观点，相互之间有一些比较。</p>
<h2 id="Domain-specific-Question-Generation-from-a-Knowledge-Base"><a href="#Domain-specific-Question-Generation-from-a-Knowledge-Base" class="headerlink" title="Domain-specific Question Generation from a Knowledge Base"></a><a href="https://arxiv.org/pdf/1610.03807v1.pdf" target="_blank" rel="external">Domain-specific Question Generation from a Knowledge Base</a></h2><p>【问题生成】问答系统是一个热门研究领域，其关注点在于如何理解问题然后选择或者生成相应的答案。而本文研究的问题是如何根据知识图谱生成高质量的问题。提出高质量的问题难度很大，且看本文内容。</p>
<h2 id="Compressing-Neural-Language-Models-by-Sparse-Word-Representations"><a href="#Compressing-Neural-Language-Models-by-Sparse-Word-Representations" class="headerlink" title="Compressing Neural Language Models by Sparse Word Representations"></a><a href="https://arxiv.org/pdf/1610.03950v1.pdf" target="_blank" rel="external">Compressing Neural Language Models by Sparse Word Representations</a></h2><p>【语言模型】【提升效率】本文解决的是在学习语言模型时输出层词表过大的问题，词表过大导致效率过低，本文针对这一问题，提出了一种压缩方法，常见词用dense向量来表示，而罕见词用常见词的线性组合来表示。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="CCL-amp-NLP-NABD-2016论文集"><a href="#CCL-amp-NLP-NABD-2016论文集" class="headerlink" title="CCL &amp; NLP-NABD 2016论文集"></a><a href="http://www.cips-cl.org/static/CCL2016/index.html" target="_blank" rel="external">CCL &amp; NLP-NABD 2016论文集</a></h2><p>由Springer出版的CCL &amp; NLP-NABD 2016论文集已经公布，并在10月10日-11月10日期间可以免费下载。免费下载方式如下：（1）访问会议首页；（2）点击该页面最新“会议论文下载”中的链接；（3）点击新页面的“Download Book (PDF, 35547KB)”按钮。</p>
<h2 id="北京大学万小军老师组开源自动摘要小工具PKUSUMSUM"><a href="#北京大学万小军老师组开源自动摘要小工具PKUSUMSUM" class="headerlink" title="北京大学万小军老师组开源自动摘要小工具PKUSUMSUM"></a><a href="http://www.icst.pku.edu.cn/lcwm/wanxj/pkusumsum.htm" target="_blank" rel="external">北京大学万小军老师组开源自动摘要小工具PKUSUMSUM</a></h2><p>本组推出文档自动摘要小工具PKUSUMSUM，集成多种无监督摘要提取算法，支持多种摘要任务与多种语言，采用Java编写，代码完全开源，欢迎批评指正，也欢迎同行一起完善该工具。</p>
<h2 id="斯坦福大学NLP组2016年秋季paper阅读周计划"><a href="#斯坦福大学NLP组2016年秋季paper阅读周计划" class="headerlink" title="斯坦福大学NLP组2016年秋季paper阅读周计划"></a><a href="http://nlp.stanford.edu/read/" target="_blank" rel="external">斯坦福大学NLP组2016年秋季paper阅读周计划</a></h2><p>斯坦福大学NLP组2016年秋季paper阅读周计划，挺多篇都是对话系统相关的。 </p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Personalizing-a-Dialogue-System-with-Transfer-Learnin
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第九期</title>
    <link href="http://rsarxiv.github.io/2016/10/13/PaperWeekly-%E7%AC%AC%E4%B9%9D%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/10/13/PaperWeekly-第九期/</id>
    <published>2016-10-14T04:09:21.000Z</published>
    <updated>2016-10-14T16:54:16.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>深度生成模型基本都是以某种方式寻找并表达（多变量）数据的概率分布。有基于无向图模型（马尔可夫模型）的联合概率分布模型，另外就是基于有向图模型（贝叶斯模型）的条件概率分布。前者的模型是构建隐含层(latent)和显示层（visible)的联合概率，然后去采样。基于有向图的则是寻找latent和visible之间的条件概率分布，也就是给定一个随机采样的隐含层，模型可以生成数据。</p>
<p>生成模型的训练是一个非监督过程，输入只需要无标签的数据。除了可以生成数据，还可以用于半监督的学习。比如，先利用大量无标签数据训练好模型，然后利用模型去提取数据特征（即从数据层到隐含层的编码过程），之后用数据特征结合标签去训练最终的网络模型。另一种方法是利用生成模型网络中的参数去初始化监督训练中的网络模型，当然，两个模型需要结构一致。</p>
<p>由于实际中，更多的数据是无标签的，因此非监督和半监督学习非常重要，因此生成模型也非常重要。本篇主要介绍一种基于对抗模式的生成模型，GAN － 从第一篇提出此模型的论文开始，之后紧接着两篇基于它的实现以及改进。三篇文章一脉相承，可以看到结合这种模型的研究进展及方向。</p>
<h1 id="Generative-Adversarial-Nets"><a href="#Generative-Adversarial-Nets" class="headerlink" title="Generative Adversarial Nets"></a><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Generative Adversarial Nets</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Universite of Montreal</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>生成模型 （Generative model）</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>NIPS 2014</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>通过模拟对抗过程，提出一种新的生成模型框架</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ul>
<li>建模</li>
</ul>
<p>在对抗生成模型中，同时训练两个网络，第一个网络是生成网络，G(z)，输入z一般是来自常见概率分布函数的样本向量，维度一般比较低，比如100。生成网络输入向量z，输出图片样例，如果使用卷机网实现的话，整个网络可以看过一个反向的CNN，其中的卷积层替换成 transposed convolution layer。第二个网络是识别网络discriminator net - D(x)，输入为一张图片x，而输出为一个标量，用来代表x来自真实图片的概率。</p>
<ul>
<li>训练</li>
</ul>
<p>整个网络的loss定义为</p>
<p>V = E’[log D(x)] + E’’[log (1 - D(G(z)) )]<br>E’ - 当x来自真实数据的期望<br>E’’ - 当x来自生成网络的期望</p>
<p>很显然，在对抗网络中，生成模型希望能够增大D(G(z))，即，希望生成的图片越真实而让识别模型“误以为”是来自真实的图片集。</p>
<p>如果生成网络G的参数用theta表示，识别模型的参数用theta_d表示，在使用SGD训练的时候，两组参数分别进行训练，对于D来说，需要对上面的公式求Gradient，但是只更新自己的参数。对G来说，只有第二项是相关的，而且可以等效的转换为maximize log D(G(z))。两个网络的参数更新交替进行。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>网上有很多实现，比如:</p>
<p><a href="https://github.com/goodfeli/adversarial" target="_blank" rel="external">goodfeli/adversarial</a>: Theano GAN implementation released by the authors of the GAN paper.<br><a href="https://github.com/Newmu/dcgan_code" target="_blank" rel="external">Newmu/dcgan_code</a>: Theano DCGAN implementation released by the authors of the DCGAN paper.<br><a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="external">carpedm20/DCGAN-tensorflow</a>: Unofficial TensorFlow DCGAN implementation.</p>
<p>这些实现一般都会包含MNIST测试集。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>其他的生成模型包括restricted Boltzmann machine (RBM), deep Boltzmann machine (DBM) 以及 variational autoencoder</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>其他生成模型中训练过程涉及intractable的计算，在实际实现时往往采取马尔可夫链模特卡洛采样(MCMC)。对抗生成模型(GAN)则不需要，整个网络的训练可以使用backpropagation来实现。</p>
<p>缺点包括训练不稳定，生成网络会塌陷到某些数据点（比如这些数据点目前看最像真实数据，生成网络会不停生成这些数据点），接下来的几篇中将提及如何改进。</p>
<h1 id="Unsupervised-Representation-Learning-with-Deep-Convolutional-Generative-Adversarial-Networks-https-arxiv-org-abs-1511-06434"><a href="#Unsupervised-Representation-Learning-with-Deep-Convolutional-Generative-Adversarial-Networks-https-arxiv-org-abs-1511-06434" class="headerlink" title="[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (https://arxiv.org/abs/1511.06434)"></a>[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (<a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="external">https://arxiv.org/abs/1511.06434</a>)</h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Alec Radford, Luke Metz, Soumith Chintala</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>facebook</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>DCGAN, Representation Learning</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2016</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>基于深度卷积网络的生成对抗模型(DCGAN)实现</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>在GAN的论文中提出的对抗模型的原型，但是对抗模型是一个大的框架，并不局限于某种网络实现。本文给出了基于卷机网的实现。</p>
<p>生成网络<br><img src="media/gen-architecture-1.png" alt="gen-architecture"></p>
<p>其中反卷积的过程是</p>
<p><img src="media/padding_strides_transposed-1.gif" alt="padding_strides_transposed"></p>
<p>识别网络是传统的CNN</p>
<p><img src="media/discrim-architecture.png" alt="discrim-architecture"></p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>本文紧密承接上篇论文，描述了实现过程中的细节，比如参数设置。也提到了解决GAN中训练不稳定的措施，但是并非完全解决。文中还提到利用对抗生成网络来做半监督学习。在训练结束后，识别网络可以用来提取图片特征，输入有标签的训练图片，可以将卷基层的输出特征作为X，标签作为y做训练。</p>
<h1 id="Improved-Techniques-for-Training-GANs"><a href="#Improved-Techniques-for-Training-GANs" class="headerlink" title="Improved Techniques for Training GANs"></a><a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">Improved Techniques for Training GANs</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>OpenAI</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>DCGAN</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2016</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>提出改进DCGAN的措施</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>这篇论文同样跟前文非常紧密，具体针对DCGAN中的问题，提出了改进方法。具体有</p>
<ul>
<li>feature matching 解决训练不稳定instability的问题</li>
<li>minibatch discrimination 解决生成网络生成图片集中的问题，原理是让识别网络一次看一组图片，而不是一张图片</li>
<li>如果对实现感兴趣，其他改进细节可以参见论文</li>
</ul>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>对抗生成网络的模型很有意思，Bengio, Hinton等都表达了很高的评价。相对其他生成模式而言，对抗生成模式模型清晰简单，目前来看效果也比较不错。但是目前对抗生成网络也有很多问题，比如生成模型是通过来自概率分布的向量生成样本，而不是直接表示输入的概率分布，因此，生成的图片可能不稳定之类。此外，希望能看到GAN在语言模型中的应用。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>GAN这种模型非常新颖，从论文中的结果来看，在图像生成上取得了不错的效果，对于MNIST这种简单的图形数据集，生成的图片已经可以“以假乱真”。对于另外的图片，比如在第二篇论文中的LSUN bedroom图片集以及人脸图片集上，生成的图片效果也不错（分辨率64×64）。<br>GAN目前来看已经卷积网络图像生成中取得了不错的效果，但是还有很多问题需要继续研究改进， 比如<br>如何生成高像素高质量的图片。目前一般像素不超过64。<br>如何提高复杂图片的质量。目前在CIFAR，ILSVRC等图片集上训练生成的图片还是很糟糕。<br>如何提高整个模型的稳定性。在实际中，尤其对于复杂图形，生成器经常很快收敛到某些单个数据集，使得整个模型的训练陷入僵局。<br>如何在其他领域，比如NLP使用GAN，如何将GAN和LSTM结合的。目前来看，还没有成功的应用。原文作者在reddit上回答内容来看，由于GAN的输入是采样自连续分布，而NLP中，每个单词的表达往往是离散的，作者提到NLP可以用增强训练的方法替代。但是也不排除可以有其他方法将GAN和LSTM结合起来的，这也是以后的一个研究点。</p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br><img src="media/qrcode_for_gh_5138cebd4585_430%20-2-.jpg" alt="qrcode_for_gh_5138cebd4585_430 -2-"></p>
<p>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;深度生成模型基本都是以某种方式寻找并表达（多变量）数据的概率分布。有基于无向图模型（马尔可夫模型）的联合概率分布模型，另外就是基于有向图模型
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.10.03-2016.10.07</title>
    <link href="http://rsarxiv.github.io/2016/10/07/cs-CL-weekly-2016-10-03-2016-10-07/"/>
    <id>http://rsarxiv.github.io/2016/10/07/cs-CL-weekly-2016-10-03-2016-10-07/</id>
    <published>2016-10-08T02:06:28.000Z</published>
    <updated>2016-10-08T02:18:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读（偏学术）"><a href="#一周值得读（偏学术）" class="headerlink" title="一周值得读（偏学术）"></a>一周值得读（偏学术）</h1><h2 id="Controlling-Output-Length-in-Neural-Encoder-Decoders"><a href="#Controlling-Output-Length-in-Neural-Encoder-Decoders" class="headerlink" title="Controlling Output Length in Neural Encoder-Decoders"></a><a href="https://arxiv.org/pdf/1609.09552v1.pdf" target="_blank" rel="external">Controlling Output Length in Neural Encoder-Decoders</a></h2><p>本文针对encoder-decoder框架在应用时无法控制生成序列长度（比如文本摘要）的问题，作者提出了一种基于学习的模型来解决这个问题。encoder-decoder框架已经被成功应用于各大任务中，加上attention，不同变种的attention，研究的人很多。本文也是属于变种之一，考虑了在实际应用中文本摘要长度需要被控制的问题，提出了本文的模型。</p>
<h2 id="Embracing-data-abundance-BookTest-Dataset-for-Reading-Comprehension"><a href="#Embracing-data-abundance-BookTest-Dataset-for-Reading-Comprehension" class="headerlink" title="Embracing data abundance: BookTest Dataset for Reading Comprehension"></a><a href="https://arxiv.org/pdf/1610.00956v1.pdf" target="_blank" rel="external">Embracing data abundance: BookTest Dataset for Reading Comprehension</a></h2><p>【数据福利】本文发布了一个新的机器阅读理解数据集BookTest，该数据集最大的亮点是规模大，是Facebook发布的Children’s Book Test的60倍之大。</p>
<h2 id="Visual-Question-Answering-Datasets-Algorithms-and-Future-Challenges"><a href="#Visual-Question-Answering-Datasets-Algorithms-and-Future-Challenges" class="headerlink" title="Visual Question Answering: Datasets, Algorithms, and Future Challenges"></a><a href="https://arxiv.org/pdf/1610.01465v1.pdf" target="_blank" rel="external">Visual Question Answering: Datasets, Algorithms, and Future Challenges</a></h2><p>【综述】这是一篇Visual Question Answer任务的综述性文章，系统地总结、讨论和对比了近几年该领域的数据集和算法，并给出了一些该领域未来的研究方向。</p>
<h2 id="Multi-View-Representation-Learning-A-Survey-from-Shallow-Methods-to-Deep-Methods"><a href="#Multi-View-Representation-Learning-A-Survey-from-Shallow-Methods-to-Deep-Methods" class="headerlink" title="Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods"></a><a href="https://arxiv.org/pdf/1610.01206v1.pdf" target="_blank" rel="external">Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods</a></h2><p>【综述】本文是一篇2015年出版的多模态表示学习的综述文章，非常适合刚刚了解或者准备进入这个领域的童鞋来读。 </p>
<h2 id="Neural-based-Noise-Filtering-from-Word-Embeddings"><a href="#Neural-based-Noise-Filtering-from-Word-Embeddings" class="headerlink" title="Neural-based Noise Filtering from Word Embeddings"></a><a href="https://arxiv.org/pdf/1610.01874v1.pdf" target="_blank" rel="external">Neural-based Noise Filtering from Word Embeddings</a></h2><p>词向量已经是NLP中各任务的基础部件，对词向量的研究工作也非常多。本文研究的切入点是从语料中的噪声入手，提出了两种无监督去噪模型，取得了不错的效果。</p>
<h1 id="一周值得读（偏应用）"><a href="#一周值得读（偏应用）" class="headerlink" title="一周值得读（偏应用）"></a>一周值得读（偏应用）</h1><h2 id="Learning-to-Translate-in-Real-time-with-Neural-Machine-Translation"><a href="#Learning-to-Translate-in-Real-time-with-Neural-Machine-Translation" class="headerlink" title="Learning to Translate in Real-time with Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1610.00388v2.pdf" target="_blank" rel="external">Learning to Translate in Real-time with Neural Machine Translation</a></h2><p>本文研究的内容实时机器翻译，与传统的翻译问题不同，该任务需要在翻译质量和速度两个方面寻找一个平衡点，NMT已经证明了其强大的实<br>力，在此基础上用增强学习做训练，以满足两个方面的需求。</p>
<h2 id="A-Tour-of-TensorFlow"><a href="#A-Tour-of-TensorFlow" class="headerlink" title="A Tour of TensorFlow"></a><a href="https://arxiv.org/pdf/1610.01178v1.pdf" target="_blank" rel="external">A Tour of TensorFlow</a></h2><p>本文系统的剖析了TensorFlow的计算图架构和分布式执行模型，并且系统地对比了TF和其他框架的性能。本文的结论对于框架选择困难的童鞋有一定参考意义，内容对于有志于深挖TF原理和想开发框架的童鞋具有较强的指导意义。对于立志于成为一名TFBoys（TensorFlow）的童鞋，本文是一篇不错的文章。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="Chatbots-–-Conversational-UI-and-the-Future-of-Online-Interaction-Swat-io-Blog"><a href="#Chatbots-–-Conversational-UI-and-the-Future-of-Online-Interaction-Swat-io-Blog" class="headerlink" title="Chatbots – Conversational UI and the Future of Online Interaction | Swat.io Blog"></a><a href="https://pan.baidu.com/s/1nuT9qnZ" target="_blank" rel="external">Chatbots – Conversational UI and the Future of Online Interaction | Swat.io Blog</a></h2><p>研究chatbot的童鞋，这本电子书值得一看，或许会有一些思考和启发！ </p>
<h2 id="王威廉老师关于如何做科研的微博"><a href="#王威廉老师关于如何做科研的微博" class="headerlink" title="王威廉老师关于如何做科研的微博"></a><a href="http://weibo.com/1657470871/EbJnqBBJ5?type=comment#_rnd1475892970397" target="_blank" rel="external">王威廉老师关于如何做科研的微博</a></h2><p>“什么是研究？本科生如何做好研究？我今天在组会上简单地给组里的本科生介绍了一点个人做研究的经验，与大家分享一下。”</p>
<h2 id="Configuring-Eclipse-with-Torch-–-Lighting-Torch"><a href="#Configuring-Eclipse-with-Torch-–-Lighting-Torch" class="headerlink" title="Configuring Eclipse with Torch – Lighting Torch"></a><a href="http://www.lighting-torch.com/2015/07/27/configuring-eclipse-with-torch/" target="_blank" rel="external">Configuring Eclipse with Torch – Lighting Torch</a></h2><p>将Torch配置到Eclipse中进行开发和调试。</p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br><img src="media/qrcode_for_gh_5138cebd4585_430%20-2-.jpg" alt="qrcode_for_gh_5138cebd4585_430 -2-"></p>
<p>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读（偏学术）&quot;&gt;&lt;a href=&quot;#一周值得读（偏学术）&quot; class=&quot;headerlink&quot; title=&quot;一周值得读（偏学术）&quot;&gt;&lt;/a&gt;一周值得读（偏学术）&lt;/h1&gt;&lt;h2 id=&quot;Controlling-Output-Length-in-Neur
    
    </summary>
    
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
</feed>
