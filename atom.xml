<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PaperWeekly</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://rsarxiv.github.io/"/>
  <updated>2016-12-30T04:10:25.000Z</updated>
  <id>http://rsarxiv.github.io/</id>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2016年自然语言处理领域10篇值得读的Paper</title>
    <link href="http://rsarxiv.github.io/2016/12/29/2016%E5%B9%B4%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E9%A2%86%E5%9F%9F10%E7%AF%87%E5%80%BC%E5%BE%97%E8%AF%BB%E7%9A%84Paper/"/>
    <id>http://rsarxiv.github.io/2016/12/29/2016年自然语言处理领域10篇值得读的Paper/</id>
    <published>2016-12-30T04:10:12.000Z</published>
    <updated>2016-12-30T04:10:25.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>告别2016，迎接2017</title>
    <link href="http://rsarxiv.github.io/2016/12/29/%E5%91%8A%E5%88%AB2016%EF%BC%8C%E8%BF%8E%E6%8E%A52017/"/>
    <id>http://rsarxiv.github.io/2016/12/29/告别2016，迎接2017/</id>
    <published>2016-12-29T19:20:43.000Z</published>
    <updated>2016-12-30T18:02:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>2016年即将结束，首先对支持PaperWeekly的各位童鞋表示衷心的感谢，感谢你们让我有动力将这个用来督促我自己多读paper的side project坚持一直做下来，感谢各位对自然语言处理感兴趣并且愿意牺牲一些个人时间来写paper note的小伙伴，也感谢每天坚持一起刷arXiv来保证周末推荐“每周值得读”质量的几位童鞋，感谢加入PaperWeekly交流群每天都贡献很多高质量讨论内容的各位朋友，感谢为三个交流群做消息同步机器人的种瓜同学。</p>
<p>PaperWeekly从刚开始只有我一个人，到现在一共有50多位一起愿意分享内容的小伙伴，并且这个数字随着大家的热情参与会逐渐增加，正是有了这么多积极参与的小伙伴，才有了PaperWeekly一周敢做一个topic的底气。从9月1号重新组织PaperWeekly的内容形式，到今天为止，一共发布了17期内容，加上之前我自己写的2期内容，一共是19期内容，19期意味着19周的时间，19周的时间我们可以走过很多地方，吃过很多美食，看过很多美景，而我们选择了读19周的paper，选择了写19周的paper note，选择了推荐这19周中高质量的paper，选择了分享这19周以来大家的成长、积累与思考。</p>
<p>19周的时间，PaperWeekly一共完成了83篇paper notes，而这83篇paper可以用19个独立的topic组织起来，比如：</p>
<p>1、提高seq2seq生成对话的流畅性和多样性；<br>2、通过无监督/半监督的方法来做命名实体识别（NER）；<br>3、哪些ICLR2017的paper值得关注；<br>4、Attention模型在NMT任务中的应用和进展；<br>5、文本摘要技术的进展情况；<br>6、增强学习在对话生成中的应用；<br>7、GAN的研究进展；</p>
<p>每个topic都涉及到了一个研究方向，有的内容非常热门，比如GAN，有的内容非常经典，比如NER，每个topic都会抓住一些特点来归纳几篇paper，为准备入门、正在入门、已经入门的同学提供了服务和方便。</p>
<p>从8.25开始，PaperWeekly推出了“每周值得读”栏目，旨在充当arXiv上自然语言处理方面的人工过滤器，旨在解决信息过载问题，旨在帮助大家更快地了解到哪些paper更值得关注。</p>
<p>从8.25开始，PaperWeekly一共推荐了153篇高质量的paper，当然每个人对于质量的理解都会有所偏差，有的paper给整个研究带来了巨大的影响，有的paper可能对某个领域有所提高，有的paper所蕴含的思想会带来很多的启发，这是一件仁者见仁智者见智的事情。</p>
<p>在做PaperWeekly的时候，我观察到大家有一定的招聘需求，可能是公司，也可能是院校或者科研机构，但是在交流群中发的效果又不是很好，于是做了个决定，在11月中旬开始推出了一项新的服务—公益广告服务，从第一个帮助清华大学刘知远老师招博士后开始至今已经发了一些广告了，虽然还没有做效果反馈工作，但我们确实尽力在帮这些需要帮助的企业或者院校，如果您有这样的招聘需求，可以私信来联系我，如果可以与PaperWeekly合作写一期文章会更好！</p>
<p>在做PaperWeekly的时候，我也有过一阵迷茫，就是关于PaperWeekly到底是什么的思考，记得是一个周日晚上，我到了夜里3点仍没有睡着，就爬起来写了一篇《PaperWeekly到底是什么》的文章，来好好地定义了一下我们所做的事情以及所想追求的东西。最后，我是这么定义PaperWeekly的，“PaperWeekly是一个由50多名喜欢分享知识的童鞋利用宝贵的业余时间来一起，以一周为单位、对一个topic进行多篇paper解读和对比总结的、不追求热点、不搞些噱头的爱心公益组织，旨在分享知识。”</p>
<p>对一个东西的定位很重要，直接决定了对这个东西的态度和所应采取的方式、方法。我做不到拿一些哗众取宠的名字来命名文章标题，也做不到过分地夸大或者贬低某一个东西，我只想纯粹地做这么一件事情。各种指标对我们来说没有意义，哪怕没有人来读文章，而我们每天所读的这些paper，所学到的知识都不会减少，当然我希望大家写的东西可以分享给更多的人，让更多的人一起来感受科技的进步和学术的前沿，但我们不会刻意地去追求什么。我一直认为人能够坚持并且努力做好一件事情的最大动力是热爱，是那种没有半点虚伪、没有半点功利的热爱。因为热爱，所以纯粹。</p>
<p>PaperWeekly不是一个完美的东西，但是一个成长的东西，是一个一直在努力变好的东西。2016快要结束了，在2017年里，我们将不断地完善文章质量，丰富文章的形式，增加一些群内的直播交流活动，比如针对某一篇、某几篇paper的讨论，不定期地邀请更多的业界大牛来讲一讲理论和技术如何在工业界落地等等。</p>
<p>PaperWeekly是一个非常开放的组织，随时欢迎想一起写paper notes或者写分享的童鞋加入，让我们不断地努力，不断地壮大力量，在2017年书写出更多值得读的文章，产生更多高质量的讨论内容，一起为国内自然语言处理的发展贡献一点点力量。</p>
<p>最后，感谢各位合作伙伴对PaperWeekly的大力支持，感谢机器之心、科研圈、IEEE计算科学评论、ChatbotChina、将门创投等媒体和机构的支持。</p>
<p>2016年是一个开始，也仅仅是一个开始，2017年即将到来，PaperWeekly将与深度学习社区AI100进行深度合作，为大家提供更好的服务！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2016年即将结束，首先对支持PaperWeekly的各位童鞋表示衷心的感谢，感谢你们让我有动力将这个用来督促我自己多读paper的side project坚持一直做下来，感谢各位对自然语言处理感兴趣并且愿意牺牲一些个人时间来写paper note的小伙伴，也感谢每天坚持一
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>本周值得读(2016.12.19-2016.12.23)</title>
    <link href="http://rsarxiv.github.io/2016/12/25/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-19-2016-12-23/"/>
    <id>http://rsarxiv.github.io/2016/12/25/本周值得读-2016-12-19-2016-12-23/</id>
    <published>2016-12-25T18:31:57.000Z</published>
    <updated>2016-12-25T18:39:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Machine-Reading-with-Background-Knowledge"><a href="#Machine-Reading-with-Background-Knowledge" class="headerlink" title="Machine Reading with Background Knowledge"></a><a href="http://t.cn/RIx3EjP" target="_blank" rel="external">Machine Reading with Background Knowledge</a></h2><p>【语义理解】在理解一句话的时候通常是直接分析该句话，而没有借助其他外部的知识，所以常常会产生一些歧义或者错误。本文的思路是在分析一句话时，借助一些背景知识来进行辅助，文中给出了两个任务，一个是句法分析中的介词短语消歧，一个是名词短语的关系抽取，都取得了明显的效果。本文作者包括了《机器学习》的作者Tom M. Mitchell教授。单纯地基于统计方法来做句法分析或者语义角色标注确实会遇到一些瓶颈，借助外部的背景知识是一个不错的思路。随着本文一起还开放了一个数据集 Prepositional Phrase Attachment Ambiguity (PPA) dataset <a href="http://t.cn/RIx1lEm" target="_blank" rel="external">http://t.cn/RIx1lEm</a></p>
<h2 id="A-User-Simulator-for-Task-Completion-Dialogues"><a href="#A-User-Simulator-for-Task-Completion-Dialogues" class="headerlink" title="A User Simulator for Task-Completion Dialogues"></a><a href="http://t.cn/RI9czrW" target="_blank" rel="external">A User Simulator for Task-Completion Dialogues</a></h2><p>【对话系统】本文研究的问题非常有用，人人都在做chatbot，却苦于没有训练数据，用户模拟是一个不错的思路。本文探索了一种模拟真实用户来训练chatbot的方法，文中给出了模拟器的设计和部分代码，涉及到的领域包括找电影和订电影票。虽然效果有很大提升空间，但是个不错的尝试。推荐给研究和开发chatbot的童鞋。源代码也同时开放了，地址 <a href="http://t.cn/RICfMSB" target="_blank" rel="external">http://t.cn/RICfMSB</a> 感兴趣的童鞋可以研究下。</p>
<h2 id="Reducing-Redundant-Computations-with-Flexible-Attention"><a href="#Reducing-Redundant-Computations-with-Flexible-Attention" class="headerlink" title="Reducing Redundant Computations with Flexible Attention"></a><a href="http://t.cn/RI9f3Bx" target="_blank" rel="external">Reducing Redundant Computations with Flexible Attention</a></h2><p>【注意力模型优化】注意力已经是一个应用比较广泛的深度学习模型，本文对decoding过程中的计算效率进行了优化，提出了一种Flexible注意力模型，在每一步解码时都会通过一个惩罚函数来过滤掉一些不重要的encoder unit，从而降低计算量。 </p>
<h2 id="Improving-Tweet-Representations-using-Temporal-and-User-Context"><a href="#Improving-Tweet-Representations-using-Temporal-and-User-Context" class="headerlink" title="Improving Tweet Representations using Temporal and User Context"></a><a href="http://t.cn/RI9I8LS" target="_blank" rel="external">Improving Tweet Representations using Temporal and User Context</a></h2><p>【用户画像】本文在对tweet进行表示学习时，通过引入用户timeline上相邻的tweets来提高准确度。</p>
<h2 id="Automatic-Generation-of-Grounded-Visual-Questions"><a href="#Automatic-Generation-of-Grounded-Visual-Questions" class="headerlink" title="Automatic Generation of Grounded Visual Questions"></a><a href="http://t.cn/RIKmwoo" target="_blank" rel="external">Automatic Generation of Grounded Visual Questions</a></h2><p>【VQA】【问题生成】可视化问答是个很有意思的东西，本文提出了一种新的任务，自动生成与图片内容相关的问题，有一点image caption的意思，只是说这里用来提问。感兴趣的童鞋可以关注一下。 </p>
<h2 id="CLEVR-A-Diagnostic-Dataset-for-Compositional-Language-and-Elementary-Visual-Reasoning"><a href="#CLEVR-A-Diagnostic-Dataset-for-Compositional-Language-and-Elementary-Visual-Reasoning" class="headerlink" title="CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"></a><a href="http://t.cn/RICIat8" target="_blank" rel="external">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</a></h2><p>【VQA】【数据福利】Li Feifei组发布的一组VQA数据集，100k规模的图片集，值得关注！文中提到数据和相关的处理代码近期会公开。</p>
<h2 id="Fast-Domain-Adaptation-for-Neural-Machine-Translation"><a href="#Fast-Domain-Adaptation-for-Neural-Machine-Translation" class="headerlink" title="Fast Domain Adaptation for Neural Machine Translation"></a><a href="http://t.cn/RICJro8" target="_blank" rel="external">Fast Domain Adaptation for Neural Machine Translation</a></h2><p>【机器翻译】【迁移学习】本文的工作是将某一个领域中训练好的模型以最低的代价迁移到领域外，同时保证领域内和领域外都有不错的效果。具体的思路是：先训练出一个不错的baseline model，然后在baseline的基础上使用领域外的少量数据进行几个回合的训练，得到一个continue model，然后将baseline和continue进行mix，得到最终的model。</p>
<h2 id="A-Context-aware-Attention-Network-for-Interactive-Question-Answering"><a href="#A-Context-aware-Attention-Network-for-Interactive-Question-Answering" class="headerlink" title="A Context-aware Attention Network for Interactive Question Answering"></a><a href="http://t.cn/RINpcT9" target="_blank" rel="external">A Context-aware Attention Network for Interactive Question Answering</a></h2><p>【交互式QA】本文的工作亮点在于做问答时提供了一种交互机制，当answer模块觉得现有的信息无法回答question的话，会生成一个更加深入的问题给用户，通过学习用户的反馈来生成答案。</p>
<h2 id="中文信息处理发展报告"><a href="#中文信息处理发展报告" class="headerlink" title="中文信息处理发展报告"></a><a href="http://t.cn/RINHLN8" target="_blank" rel="external">中文信息处理发展报告</a></h2><p>中国中文信息学会发布2016年《中文信息处理发展报告》，值得一读！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Machine-Reading-with-Background-Knowledge&quot;&gt;&lt;a href=&quot;#Machine-Reading-with-Background-Knowledge&quot; class=&quot;headerlink&quot; title=&quot;Machine Re
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十九期</title>
    <link href="http://rsarxiv.github.io/2016/12/23/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B9%9D%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/23/PaperWeekly-第十九期/</id>
    <published>2016-12-23T18:40:03.000Z</published>
    <updated>2016-12-23T18:56:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>本期的PaperWeekly一共分享四篇最近arXiv上发布的高质量paper，包括：情感分析、机器阅读理解、知识图谱、文本分类。人工智能及其相关研究日新月异，本文将带着大家了解一下以上四个研究方向都有哪些最新进展。四篇paper分别是：</p>
<p>1、Linguistically Regularized LSTMs for Sentiment Classification, 2016.11<br>2、End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension, 2016.10<br>3、Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples, 2016.10<br>4、AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification, 2016.11</p>
<h1 id="Linguistically-Regularized-LSTMs-for-Sentiment-Classification"><a href="#Linguistically-Regularized-LSTMs-for-Sentiment-Classification" class="headerlink" title="Linguistically Regularized LSTMs for Sentiment Classification"></a><a href="https://arxiv.org/pdf/1611.03949v1.pdf" target="_blank" rel="external">Linguistically Regularized LSTMs for Sentiment Classification</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Qiao Qian, Minlie Huang, Xiaoyan Zhu</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>sentiment classification, neural network models, linguistically coherent representations,</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016.11</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>利用语言资源和神经网络相结合来提升情感分类问题的精度</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>在LSTM和Bi-LSTM模型的基础上加入四种规则约束，这四种规则分别是: Non-Sentiment Regularizer,Sentiment Regularizer, Negation Regularizer, Intensity Regularizer.因此，新的loss function变为:</p>
<p><img src="media/eqn.png" alt="eqn"></p>
<p>不同的规则约束对应不同的L函数</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>1、Movie Review (MR) <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><br>2、Stanford Sentiment Tree- bank (SST) <a href="http://nlp.stanford.edu/sentiment/treebank.html" target="_blank" rel="external">http://nlp.stanford.edu/sentiment/treebank.html</a></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、Neural Networks for Sentiment Classification<br><a href="https://arxiv.org/abs/1412.3555" target="_blank" rel="external">Empirical evaluation of gated recurrent neural networks on sequence modeling</a><br><a href="https://pdfs.semanticscholar.org/5807/664af8e63d5207f59fb263c9e7bd3673be79.pdf" target="_blank" rel="external">Hybrid speech recognition with deep bidirectional lstm</a><br>2、Applying Linguistic Knowledge for Sentiment Classification<br><a href="http://www.site.uottawa.ca/~diana/publications/ci.pdf" target="_blank" rel="external">Sentiment classification of movie reviews using contextual valence shifters</a></p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本文提出了一种新的基于语言资源约束和LSTM/Bi-LSTM的模型用于情感分类，并通过在MR和SST数据集上的实验和对RNN/RNTN,LSTM,Tree-LSTM,CNN的效果对比证明了这一模型的有效性。除此之外，本文还基于不同的约束进行了实验，证明的不同的约束在提高分类精度上的作用。本文实验丰富，效果的提升虽不显著，但新的模型确实在不同程度上克服了旧模型的一些不足。</p>
<h1 id="End-to-End-Answer-Chunk-Extraction-and-Ranking-for-Reading-Comprehension"><a href="#End-to-End-Answer-Chunk-Extraction-and-Ranking-for-Reading-Comprehension" class="headerlink" title="End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension"></a><a href="https://arxiv.org/pdf/1610.09996v2.pdf" target="_blank" rel="external">End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, Bowen Zhou</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>IBM Watson</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Reading Comprehension, Chunk extraction, Ranking</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016.10</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>针对答案非定长的阅读理解任务，本文提出了DCR（dynamic chunk reader）模型<br>来从给定的文档中抽取可能的候选答案并进行排序。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>本文提出的模型结构共分为四部分，<br>1、Encoder Layer<br>如图所示，这部分是用双向GRU分别对文档（Passage）和问题（Question）进行编码。<br>2、Attention Layer<br>该层采用的方法与相关工作中的mLSTM类似，文档每个时刻的状态h<sub>j</sub><sup>p</sup>都与问题中的每个状态h<sub>k</sub><sup>q</sup>进行匹配得到一个权重向量α<sub>k</sub>，然后再根据该权重向量对问题的GRU隐层输出h<sup>p</sup>进行加权求和，得到文档中该时刻状态h<sub>j</sub><sup>p</sup>对应的上下文向量β<sub>j</sub>，两个向量h<sub>j</sub><sup>p</sup>和β<sub>j</sub>拼接在一起作为该时刻新的表示v<sub>j</sub>。最后再将上述与问题相关的新文档表示v通过双向GRU，得到文档最终的表示γ。<br><img src="media/DCR.png" alt="DC"></p>
<p>3、Chunk-Representation Layer<br>上一部分获得了与问题相关的文档表示γ，那么这部分则是考虑如何抽取候选答案，并获得候选答案的表示向量。本文提出了两种候选答案抽取方法，第一种方法是抽取所有满足训练数据中答案对应词性标注模式的候选项，第二种方法则是简单粗暴地确定一个候选项最大长度，然后遍历所有可能的候选项。至于候选答案的表示方式，本文将候选答案前向GRU的最后一个时刻状态和反向GRU第一个时刻状态拼接在一起作为最终候选项的表示。<br>4、Ranker Layer<br>已经获得了所有候选项的表示，那么接着就是对所有候选项进行打分排序。本文中打分是采用问题的表示和候选项的表示计算内积的方式得到的，本文训练过程中没有采用常见于排序任务的Margin ranking loss，而是先用softmax对所有候选项计算一个概率值，然后采用交叉熵损失函数进行训练。</p>
<p>本文在SQuAD数据集上进行实验，提出的方法效果比之前两篇SQuAD相关paper的方法有较大的提升。</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、SQuAD <a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="external">https://rajpurkar.github.io/SQuAD-explorer/</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、数据集相关论文<br>SQuAD: 100,000+ Questions for Machine Comprehension of Text<br>2、模型相关论文<br>MACHINE COMPREHENSION USING MATCH-LSTM</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>在对文档和问题编码阶段，本篇论文提出的模型与之前mLSTM那篇paper有些相似。两篇论文中模型的主要区别在于：mLSTM那篇论文采用预测起始、终止位置的方法来确定答案，而本文则是先采用一些规则或Pattern的方法来抽取一些候选答案，然后再对候选答案进行排序。</p>
<h2 id="联系方式"><a href="#联系方式" class="headerlink" title="联系方式"></a>联系方式</h2><p>有DL或者NLP相关话题，欢迎讨论。destin.bxwang@gmail.com </p>
<h1 id="Knowledge-will-Propel-Machine-Understanding-of-Content-Extrapolating-from-Current-Examples"><a href="#Knowledge-will-Propel-Machine-Understanding-of-Content-Extrapolating-from-Current-Examples" class="headerlink" title="Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples"></a><a href="https://arxiv.org/abs/1610.07708?from=groupmessage&amp;isappinstalled=0" target="_blank" rel="external">Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Amit Sheth, Sujan Perera, and Sanjaya Wijeratne</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>Kno.e.sis Center, Wright State University Dayton, Ohio, USA</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Semantic analysis of multimodal data，Machine intelligence,Understanding complex text，EmojiNet</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016.10</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>利用知识和多模态数据来解决特定情况下的复杂文本的深层理解问题</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>1、现知识库在处理特定领域问题中的局限性及解决方法<br>（1）知识库的杂乱<br>解决方法：采用自动判别技术，领域知识库索引技术，利用实体和关系的语义去判别所给定知识库领域中的相关部分。<br>（2）知识库数据的不完备和不充足<br>解决方法：使用 human-in-the-loop模型在真实的临床数据和已有的知识库中去发现更多的实体与实体之间的关系。<br>（3）知识表示技术和推理技术的局限性<br>解决方法：在单个属性的表示中加入了三元组和软逻辑的解释能力及其相关概率值和理由。</p>
<p>2、新的研究应用<br>（1）隐实体链接<br>（2）表情符号语义消歧<br>（3）理解和分析web论坛中关于药物滥用的相关讨论<br>利用相关背景知识加强不同种类信息的信息抽取模型<br><img src="media/img1.png" alt="img1"></p>
<p>3、在健康领域中的文本理解模型<br><img src="media/img2.png" alt="img2"></p>
<p>4、使用感知器和文本资料了解城市交通情况<br>(1)交通领域的概念关系网模型<br>(2)概率图模型<br><img src="media/img3.png" alt="img3"></p>
<p>使用领域知识关联不同模态下的上下文相关数据<br><img src="media/img4.png" alt="img4"></p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文主要举例说明了知识将推动机器对内容的理解。总体来看本文像一篇综述性的文章，给出了在知识库创建过程中所遇到的问题的解决方案，同时以实际案例来阐述知识在我们实际问题中应用。</p>
<h1 id="AC-BLSTM-Asymmetric-Convolutional-Bidirectional-LSTM-Networks-for-Text-Classification"><a href="#AC-BLSTM-Asymmetric-Convolutional-Bidirectional-LSTM-Networks-for-Text-Classification" class="headerlink" title="AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification"></a><a href="https://arxiv.org/pdf/1611.01884v2.pdf" target="_blank" rel="external">AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Depeng Liang and Yongdong Zhang</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>Guangdong Province Key Laboratory of Computational Science, School of Data and<br>Computer Science, Sun Yat-sen University, Guang Zhou, China</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>ACNN; BLSTM; Text Classification</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016.11</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>本文提出了一个新的深度学习的模型–AC-BLSTM的模型（即：将ACNN和BLSTM组合在一起），用于句子和文章层面的分类。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>AC-BLSTM模型可以分成四个部分,如Figure 1所示：<br>1.输入: 输入是一个sentence，使用 ( L <em> d )的矩阵表示，其中L表示句子中的L个词，d表示每个词的词向量的维度<br>2.ACNN(Asymmetric CNN): 传统的CNN采用的是 ( k </em> d ) 大小的filter，ACNN则把filter的过程分成 ( 1 <em> d ) 和 ( k </em> 1 ) 的两个过程，相当于是把 ( k <em> d ) 的filter做因式分解。<br>这一层的输入是一个 ( L </em> d ) 的矩阵，对于n个尺度为( 1 <em> d ) 和( ki </em> 1 )的卷积层的输出是一个 [ (L - ki + 1) <em> n ]的矩阵，如下图所示，本文采用了3种不同的卷积核，所以输出是3种不同的[ (L - ki + 1) </em> n ]的矩阵（图中一个彩色的小方块表示 (1 * n)的向量）<br>3.连接层: 为了给BLSTM构造输入，连接层将3种不同卷积层的输出，以Ct^i表示第1种卷积层为LSTM第t个time step贡献的输入，则LSTM网络的第t步输入Ct = [Ct^1, Ct^2, Ct^3]，其中t属于{1,2,…,L-K+1}, K = max{ki}<br>4.BLSTM: LSTM能够很好的解决long time delay 和long range context的问题，但其处理是单向的，而BLSTM能够解决given point的双边的依赖关系，因此，本文选择了BLSTM网络层来学习ACNN输入的特征的dependencies<br>5.Softmax层: 为了应用于分类问题，本文在最后使用全连接层和softmax函数来实现分类。<br><img src="media/Figure1.jpg" alt="Figure1"></p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>文章中使用的数据集<br>1、SST-1 <a href="http://nlp.stanford.edu/sentiment/index.html" target="_blank" rel="external">http://nlp.stanford.edu/sentiment/index.html</a><br>2、SST-2 <a href="http://nlp.stanford.edu/sentiment/index.html" target="_blank" rel="external">http://nlp.stanford.edu/sentiment/index.html</a><br>3、Movie Review(MR) <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><br>4、SUBJ <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><br>5、TREC <a href="http://cogcomp.cs.illinois.edu/Data/QA/QC/" target="_blank" rel="external">http://cogcomp.cs.illinois.edu/Data/QA/QC/</a><br>6、YELP13 <a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="external">https://www.yelp.com/dataset_challenge</a></p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、Yoon Kim于2014年在<a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" rel="external"><strong>Convolutional neural networks for sentence classification</strong></a>一文中提出将词向量和CNN结合，用于句子分类的模型。在该文中，Kim将不同长度的filter的组合在一起，且提出了static或者可以fine-tuning的word embedding模型<br>2、Zhou et al.则于2015年在<a href="https://arxiv.org/abs/1511.08630" target="_blank" rel="external"><strong>A C-LSTM neural network for text classification</strong></a>一文中提出将CNN和LSTM叠加的模型，且使用固定的word embedding<br>3、Szegedy et al.于2015年在<a href="https://arxiv.org/pdf/1512.00567v3.pdf" target="_blank" rel="external"><strong>Rethinking the Inception Architecture for Computer Vision</strong></a>中提出了ACNN模型，这减少了参数的个数且提高了模型的表征</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>这篇文章主要贡献就是提出了一个AC-BSLTM的模型用于文本分类，亮点就在于：ACNN可以在减少参数的个数的同时通过增加更多的非线性性来提高表达能力，而BLSTM能够捕捉输入的两端的信息。两者的结合就提高了分类的精度。但事实上，这两个网络模型都是现有的，本文的工作感觉只是两个网络的连接，在本质上没有太大的改进，且在分类精度上的提高也比较有限。</p>
<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>感谢@方嘉倩 @destin wang 和 @min279 三位童鞋的辛勤工作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引&quot;&gt;&lt;a href=&quot;#引&quot; class=&quot;headerlink&quot; title=&quot;引&quot;&gt;&lt;/a&gt;引&lt;/h1&gt;&lt;p&gt;本期的PaperWeekly一共分享四篇最近arXiv上发布的高质量paper，包括：情感分析、机器阅读理解、知识图谱、文本分类。人工智能及其相关研
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>本周值得读(2016.12.12-2016.12.16)</title>
    <link href="http://rsarxiv.github.io/2016/12/18/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-12-2016-12-16/"/>
    <id>http://rsarxiv.github.io/2016/12/18/本周值得读-2016-12-12-2016-12-16/</id>
    <published>2016-12-18T17:16:11.000Z</published>
    <updated>2016-12-18T17:28:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Building-Large-Machine-Reading-Comprehension-Datasets-using-Paragraph-Vectors"><a href="#Building-Large-Machine-Reading-Comprehension-Datasets-using-Paragraph-Vectors" class="headerlink" title="Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors "></a><a href="http://t.cn/RIbpc9X" target="_blank" rel="external">Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors </a></h2><p>【机器阅读理解】【数据福利】<br>本文利用一种无监督的方法构建了一组大型的机器阅读理解数据集。其中机器阅读理解问题是提供一篇新闻，从5个候选标题中选择一个正确的。无监督的方法用了Mikolov提出的Paragraph Vector（Word2Vec的文档版），用来训练和计算各个新闻标题之间的相似度，产生候选答案。本文所生成的数据集地址：<a href="https://github.com/google/mcafp" target="_blank" rel="external">https://github.com/google/mcafp</a></p>
<h2 id="Multi-Perspective-Context-Matching-for-Machine-Comprehension"><a href="#Multi-Perspective-Context-Matching-for-Machine-Comprehension" class="headerlink" title="Multi-Perspective Context Matching for Machine Comprehension "></a><a href="http://t.cn/RIbdvXM" target="_blank" rel="external">Multi-Perspective Context Matching for Machine Comprehension </a></h2><p>【机器阅读理解】本文的研究基于SQuAD数据集，提出了一个端到端训练模型，主要的思路是passage中与问题相似的span更加倾向于是正确答案。SQuAD是这个领域中有名的数据集，相应的模型很多，本文的结果相对一般。</p>
<h2 id="ConceptNet-5-5-An-Open-Multilingual-Graph-of-General-Knowledge"><a href="#ConceptNet-5-5-An-Open-Multilingual-Graph-of-General-Knowledge" class="headerlink" title="ConceptNet 5.5: An Open Multilingual Graph of General Knowledge "></a><a href="http://t.cn/RIbgeA5" target="_blank" rel="external">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge </a></h2><p>【知识图谱】【资源推荐】本文介绍了一个通用知识图谱ConceptNet 5.5，图谱主页的地址：<a href="http://conceptnet.io/" target="_blank" rel="external">http://conceptnet.io/</a>  相关的code和文档地址： <a href="https://github.com/commonsense/conceptnet5" target="_blank" rel="external">https://github.com/commonsense/conceptnet5</a></p>
<h2 id="Tracking-the-World-State-with-Recurrent-Entity-Networks"><a href="#Tracking-the-World-State-with-Recurrent-Entity-Networks" class="headerlink" title="Tracking the World State with Recurrent Entity Networks "></a><a href="http://t.cn/RIbsLuo" target="_blank" rel="external">Tracking the World State with Recurrent Entity Networks </a></h2><p>【Dynamic Memory】本文介绍了一种新的模型，Recurrent Entity Network (EntNet)，引用外部动态长程记忆来做推理，并在 SYNTHETIC WORLD MODEL、bAbI和CBT三个任务上得到了验证，值得关注。本文工作来自FB LeCun组。</p>
<h2 id="Online-Sequence-to-Sequence-Reinforcement-Learning-for-Open-Domain-Conversational-Agents"><a href="#Online-Sequence-to-Sequence-Reinforcement-Learning-for-Open-Domain-Conversational-Agents" class="headerlink" title="Online Sequence-to-Sequence Reinforcement Learning for Open-Domain Conversational Agents "></a><a href="http://t.cn/RIbsrka" target="_blank" rel="external">Online Sequence-to-Sequence Reinforcement Learning for Open-Domain Conversational Agents </a></h2><p>【对话系统】用几个关键词来概括一下本文的工作：1、在线训练；2、seq2seq；3、深度增强学习；4、开放域问题。建议对对话系统感兴趣的童鞋研读。</p>
<h2 id="Neural-Emoji-Recommendation-in-Dialogue-Systems"><a href="#Neural-Emoji-Recommendation-in-Dialogue-Systems" class="headerlink" title="Neural Emoji Recommendation in Dialogue Systems "></a><a href="http://t.cn/RIqZTsq" target="_blank" rel="external">Neural Emoji Recommendation in Dialogue Systems </a></h2><p>【对话系统】【Emoji】Emoji表情是大家在平时聊天时经常会用到的，往往一个表情胜过一句话的表达。本文研究了在多轮对话中如何通过上下文来预测和推荐emoji表情，是个很好玩的工作。如果能够分析和预测更广泛的表情包（不仅限于emoji）的话，可能是件更好玩的事情。</p>
<h2 id="Learning-Through-Dialogue-Interactions"><a href="#Learning-Through-Dialogue-Interactions" class="headerlink" title="Learning Through Dialogue Interactions "></a><a href="http://t.cn/RI5dgWk" target="_blank" rel="external">Learning Through Dialogue Interactions </a></h2><p>【对话系统】Jiwei Li的新文章，通过和Teacher的交互（基于知识库相互问和答）来提高bot的学习能力，整体框架仍是增强学习，值得精读。代码和数据都已开放，地址：<a href="https://github.com/facebook/MemNN/tree/master/AskingQuestions" target="_blank" rel="external">https://github.com/facebook/MemNN/tree/master/AskingQuestions</a> torch实现。</p>
<h2 id="Diverse-Beam-Search-Decoding-Diverse-Solutions-from-Neural-Sequence-Models"><a href="#Diverse-Beam-Search-Decoding-Diverse-Solutions-from-Neural-Sequence-Models" class="headerlink" title="Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models "></a><a href="http://t.cn/RVbp10D" target="_blank" rel="external">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models </a></h2><p>【seq2seq多样性】【柱搜索】一篇考虑了生成内容多样性的beam search改进算法，可以应用在chatbot、nmt、image caption、vqa等各种场景中。开源代码用torch实现的，基于neuraltalk2代码。地址：<a href="https://github.com/ashwinkalyan/dbs" target="_blank" rel="external">https://github.com/ashwinkalyan/dbs</a>  在线demo地址：<a href="http://dbs.cloudcv.org/captioning" target="_blank" rel="external">http://dbs.cloudcv.org/captioning</a></p>
<h2 id="Multilingual-Word-Embeddings-using-Multigraphs"><a href="#Multilingual-Word-Embeddings-using-Multigraphs" class="headerlink" title="Multilingual Word Embeddings using Multigraphs "></a><a href="http://t.cn/RIqqODu" target="_blank" rel="external">Multilingual Word Embeddings using Multigraphs </a></h2><p>【词向量】本文给了一组单语和多语的词向量学习方法，基于SkipGram模型，skipgram的context考虑比较简单，本文主要是在context上做了一些文章，添加了一些特征，比如syntactic dependencies and word alignments等。</p>
<h2 id="FastText-zip-Compressing-text-classification-models"><a href="#FastText-zip-Compressing-text-classification-models" class="headerlink" title="FastText.zip: Compressing text classification models "></a><a href="http://t.cn/RI4uuHE" target="_blank" rel="external">FastText.zip: Compressing text classification models </a></h2><p>【模型压缩】模型过大是DL的一个问题，尤其是在部署模型时，这个问题尤其明显。本文工作来自FB，是开源分类工具fasttext的一个模型压缩版。FastText的地址：<a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">https://github.com/facebookresearch/fastText</a></p>
<h2 id="Mining-Compatible-Incompatible-Entities-from-Question-and-Answering-via-Yes-No-Answer-Classification-using-Distant-Label-Expansion"><a href="#Mining-Compatible-Incompatible-Entities-from-Question-and-Answering-via-Yes-No-Answer-Classification-using-Distant-Label-Expansion" class="headerlink" title="Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion "></a><a href="http://t.cn/RIqG4QU" target="_blank" rel="external">Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion </a></h2><p>【评论挖掘】本文针对的应用场景是从商品评论中挖掘各种商品的兼容性，比如买了个鼠标，想知道这个鼠标和ipad、pc的兼容性如何。文中的Complementary Entity Recognition 方法来自上周同作者的一篇文章，地址是<a href="https://arxiv.org/abs/1612.01039" target="_blank" rel="external">https://arxiv.org/abs/1612.01039</a> 这个应用场景比较接地气，建议对评论挖掘感兴趣的童鞋阅读。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Building-Large-Machine-Reading-Comprehension-Datasets-using-Paragraph-Vectors&quot;&gt;&lt;a href=&quot;#Building-Large-Machine-Reading-Comprehensio
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十八期</title>
    <link href="http://rsarxiv.github.io/2016/12/17/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%85%AB%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/17/PaperWeekly-第十八期/</id>
    <published>2016-12-17T18:37:27.000Z</published>
    <updated>2016-12-17T19:35:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>对话系统是当前的研究热点，也是风险投资的热点，从2016年初开始，成立了无数家做chatbot、语音助手等类似产品的公司，不管是对用户的，还是对企业的，将对话系统这一应用推到了一个新的高度。seq2seq是当前流行的算法框架，给定一个输入，模型自动给出一个不错的输出，听起来都是一件美好的事情。seq2seq在对话系统中的研究比较多，本期PaperWeekly分享4篇非常新的paper notes，涉及到如何提高所生成对话的流畅度和多样性，使得对话系统能够更加接近人类的对话。4篇paper如下：</p>
<p>1、Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation, 2016<br>2、A Simple, Fast Diverse Decoding Algorithm for Neural Generation, 2016<br>3、DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS, 2016<br>4、A Diversity-Promoting Objective Function for Neural Conversation Models, 2015</p>
<h1 id="Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation"><a href="#Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation" class="headerlink" title="Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation"></a><a href="http://cn.arxiv.org/pdf/1607.00970" target="_blank" rel="external">Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China<br>Institute of Software, Peking University, China<br>Institute of Network Computing and Information Systems, Peking Univerity, China<br>Institute of Computer Science and Technology, Peking University, China</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>content-introducing approach<br>neural network-based<br>generative dialogue systems<br>seq2BF</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>使用引入内容方法，用于处理基于神经网络的生成式对话系统</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="media/model-18.png" alt="mode"></p>
<p>该模型由两部分组成：<br>1、use PMI to predict a keyword for the reply<br>使用逐点互信息(PMI)进行预测，选取PMI值最大的单词作为回答中的关键词，该关键词可以出现在回答语句中的任意位置。<br><img src="media/%E5%85%AC%E5%BC%8F.png" alt="公式"></p>
<p>2、generate a reply conditioned on the keyword as well as the query<br>使用sequence to backward and forward sequences(seq2BF)模型来生成包含关键词的回答。以该关键词为基点，将回答语句划分为两个序列：<br>(1) 反向序列：关键词左侧的所有单词以逆序排列<br>(2) 正向序列：关键词右侧的所有单词以顺序排列</p>
<p>seq2BF模型具体工作如下：<br>(1) 使用seq2seq神经网络将问题编码，仅对关键词左侧的单词进行解码，逆序输出每个单词<br>(2) 使用另一个seq2seq模型将问题再次编码，在给定上步中解码后的逆序单词序列下，对回答中的剩余单词进行顺序解码，输出最终单词序列<br><img src="media/%E5%85%AC%E5%BC%8F3.png" alt="公式3"></p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>Dataset：<a href="http://tieba.baidu.com" target="_blank" rel="external">http://tieba.baidu.com</a></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、 Dialogue Systems<br>(1) (Isbell et al., 2000; Wang et al., 2013) retrieval methods<br>(2)  (Ritter et al., 2011) phrase-based machine translation<br>(3)  (Sordoni et al., 2015; Shang et al., 2015) recurrent neural networks </p>
<p>2、 Neural Networks for Sentence Generation<br>(1)  (Sordoni et al., 2015) bag-of-words features<br>(2)  (Shang et al., 2015) seq2seq-like neural networks<br>(3)  (Yao et al., 2015; Serban et al., 2016a) design hierarchical neural networks<br>(4)  (Li et al., 2016a) mutual information training objective</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本文的创新点在于，不同与目前普遍存在的从句首到句尾顺序生成目标单词的方法，引入逐点互信息方法来预测回答语句中的关键词，使用seq2BF机制确保该关键词可以出现在目标回答语句的任意位置之中并确保输出的流利度，相比于seq2seq的生成方法显著地提升了对话系统的质量。</p>
<h1 id="A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation"><a href="#A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation" class="headerlink" title="A Simple, Fast Diverse Decoding Algorithm for Neural Generation"></a><a href="https://arxiv.org/abs/1611.08562" target="_blank" rel="external">A Simple, Fast Diverse Decoding Algorithm for Neural Generation</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Jiwei Li, Will Monroe and Dan Jurafsky</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Stanford</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>seq2seq, diversity, RL</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>seq2seq模型decoder时改进beam search，引入惩罚因子影响排序结果，并加入强化学习模型来自动学习diversity rate，使得解码出的结果更具多样性</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p><img src="media/18-0.png" alt="18-0"></p>
<p>对比标准beam search，本模型引入惩罚因子，公式如下</p>
<p><img src="media/18-1.png" alt="18-1"></p>
<p>其中$\gamma$称为diversity rate，k’范围为[1,k]，K为beam size<br>强化学习模型中，策略为</p>
<p><img src="media/18-2.png" alt="18-2"></p>
<p>reward为评价指标，例如机器翻译中的BLEU值等</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、回复生成实验数据集：OpenSubtitles <a href="https://github.com/jiweil/mutual-information-for-neural-machine-translation" target="_blank" rel="external">https://github.com/jiweil/mutual-information-for-neural-machine-translation</a><br>（代码模型可从作者另外一篇文章的源码稍加改动）</p>
<p>2、机器翻译数据集：WMT’14 <a href="http://www.statmt.org/wmt13/translation-task.html" target="_blank" rel="external">http://www.statmt.org/wmt13/translation-task.html</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p><img src="media/18-3.png" alt="18-3"></p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>本模型的创新点在于引入惩罚因子，使得decoder时对standard beam search算法进行重排序，并引入强化学习模型，自动学习diversity rate。作者分别在三个实验上进行验证，机器翻译、摘要抽取与对话回复生成，实验表明在不同的实验上有不同的表现，但是总体而言本方法能够在一定程度上解码出更具有多样性的句子。（思路简明清晰，对于传统的beam search稍加改动，原文中作者提到在Matlab代码中只改动一行即可）</p>
<h1 id="DIVERSE-BEAM-SEARCH-DECODING-DIVERSE-SOLUTIONS-FROM-NEURAL-SEQUENCE-MODELS"><a href="#DIVERSE-BEAM-SEARCH-DECODING-DIVERSE-SOLUTIONS-FROM-NEURAL-SEQUENCE-MODELS" class="headerlink" title="DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS"></a><a href="https://arxiv.org/abs/1610.02424" target="_blank" rel="external">DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun1 Stefan Lee, David Crandall &amp; Dhruv Batra</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>Virginia Tech, Blacksburg, VA, USA<br>Indiana University, Bloomington, IN, USA</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Beam Search; Diversity; Image Caption; Machine Translation; Visual Question Answer; Chatbot</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016.10</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>如何改进beam search解码算法，使其在seq2seq模型中可以生成更加丰富的结果？</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>经典的beam search算法以最大后验概率作为优化目标函数，每一个time step只保留B个最优的状态，是一种典型的贪心算法，这个经典算法常常被用于解码可选状态数量多的情形，比如生成对话、生成图片描述、机器翻译等，每一步都有词表大小的可选状态集。seq2seq模型的流行，让这种解码算法的研究变得热门。在生成对话任务时，用经典的beam search会生成类似“我不知道”等这种没有营养的对话，虽然没有语法上的错误，而且可能在一定的评价体系内会得到不错的分数，但实际应用效果太差，因此diversity的研究变得热门。</p>
<p>本文针对diversity的问题，提出了一种改进版的beam search算法，旨在生成更加多样性的话。</p>
<p><img src="media/18-5.png" alt="18-5"></p>
<p>新算法的主要思路是将经典算法中的Beam进行分组，通过引入一个惩罚机制，使得每一组的相似度尽量低，这一项保证了生成的话相互之间差异更大一些，即满足了多样性的需求，在每一组Beam中，用经典的算法进行优化搜索。具体的算法流程如下图：</p>
<p><img src="media/18-6.png" alt="18-6"></p>
<p>实验中，用了Image Caption、Machine Translation和VQA三个任务进行了对比，验证了本文算法的有效性，并且对算法中的几个参数进行了敏感度分析，分析了分组数对多样性的影响。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>1、本文算法torch实现 <a href="https://github.com/ashwinkalyan/dbs" target="_blank" rel="external">https://github.com/ashwinkalyan/dbs</a><br>2、本文在线demo dbs.cloudcv.org<br>3、neuraltalk2实现 <a href="https://github.com/karpathy/neuraltalk2" target="_blank" rel="external">https://github.com/karpathy/neuraltalk2</a><br>4、机器翻译开源实现dl4mt <a href="https://github.com/nyu-dl/dl4mt-tutorial" target="_blank" rel="external">https://github.com/nyu-dl/dl4mt-tutorial</a></p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>相关的工作主要分类两类：<br>1、Diverse M-Best Lists<br>2、Diverse Decoding for RNNs<br>之前Jiwei Li将解码算法的目标函数换成了互信息进行优化解码，对diversity进行了研究。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文研究的问题是一类基础问题，beam search算法作为一种经典的近似解码算法，应用的场景非常多。但在实际应用中，尤其是具体到生成对话、生成答案等任务上，存在一些适应性的问题，比如diversity。只是生成简单而又安全的话对于实际应用没有太多的意义，所以本文的研究非常有意义。本文的实验从三个不同的任务上对改进后的beam search都做了对比验证，非常扎实的结果验证了算法的有效性，并且对几个关键参数进行了敏感度分析，有理有据。同时在github上开源了代码，并且给出了一个在线demo。在评价方面，不仅仅设计了几个自动评价指标，而且用了人工评价的方法对本文算法进行了验证，是一篇非常好的paper，值得学习。</p>
<h1 id="A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models"><a href="#A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models" class="headerlink" title="A Diversity-Promoting Objective Function for Neural Conversation Models"></a><a href="https://arxiv.org/pdf/1510.03055.pdf" target="_blank" rel="external">A Diversity-Promoting Objective Function for Neural Conversation Models</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>Stanford University, Stanford, CA, USA<br>Microsoft Research, Redmond, WA, USA</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Sequence-to-sequence neural network models, conversational responses, Maximum Mutual Information(MMI)</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2015</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>使用MMI训练sequence-to-sequence model for conversational responses generation<br>传统的ML(最大似然估计)在训练sequence-to-sequence model的时候，易产生与输入无关的’safe’ responses(最大似然估计的弊病—-always try to cover all mode of input data)<br>作者通过使用MMI, 最大化输入与输出的互信息，能够有效避免与输入无关的responses，得到更为diverse的responses.</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>MMI最早在speech recognition中提出并应用(discriminative training criteria). 语音识别中，通常先用ML训练声学模型，然后再接MMI和语言模型，对声学模型进一步调优。</p>
<p>在本文中，作者通过提出MMI用于seq-to-seq model的优化。作者提出了MMI-antiLM和MMI-bidi 两个不同的MMI的formulations. MMI在seq-to-seq的应用中存在decoding的问题。</p>
<p>MMI-antiLM中，作者通过使用带有权重的LM以生成更为diverse的responses by penalizing first word。</p>
<p>MMI-bidi中，搜索空间的数目过大，导致expolring所有的可能性在实际中无法实现。作者首先产生N-best list, 然后根据相应的准则函数 re-rank得到的N-best list。</p>
<p>在MMI不同的formulation中，作者通过启发式的设计，使得decoding更为容易且产生的response更为diverse，在相关的数据集上取得了较好的BLEU且产生的response更为diverse。</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>最大后验概率通常作为优化的目标函数，但很多应用场景中得到的结果并不理想。本文采用了一个新的而且也是其他领域中比较常见的目标函数来替换最大后验概率，在生成对话时得到了更加丰富的结果。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>对话系统是一个相对高级的、综合性很强的任务，所依赖的基础任务比较多，比如分词、命名实体识别、句法分析、语义角色标注等等。对于规范的中文表达而言，句法分析仍是一个没有解决好的问题，更何况是不那么规范的人话，句法分析的准确性又要下一个level了，随之语义角色标注也得不到好的效果。经典的、基础的任务还有很长的路要走，对话系统这种更难、更复杂的任务相信不是一年、两年就可以突破的事情，虽然现在大热，做的人很多，但就目前的研究水平来看，应该还有很长的路要走。seq2seq是个逃避这些问题的好方法和好思路，但相对来说更加不成熟，而且存在着很多的问题，想通过大量的数据来覆盖所有的问题，是一种不太科学的思路。我想，seq2seq是个好方法，但传统的NLP方法也是必不可少的，而且两者应该是相互补充的。越多的人关注对话系统，就会越快地推动这个领域的发展，希望早日看到靠谱的、成熟的解决方案。感谢@Penny、@tonya、@zhangjun和@皓天 四位童鞋完成的paper notes。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;对话系统是当前的研究热点，也是风险投资的热点，从2016年初开始，成立了无数家做chatbot、语音助手等类似产品的公司，不管是对用户的，还
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>本周值得读(2016.12.05-2016.12.09)</title>
    <link href="http://rsarxiv.github.io/2016/12/11/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-05-2016-12-09/"/>
    <id>http://rsarxiv.github.io/2016/12/11/本周值得读-2016-12-05-2016-12-09/</id>
    <published>2016-12-11T16:51:29.000Z</published>
    <updated>2016-12-11T17:01:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="End-to-End-Joint-Learning-of-Natural-Language-Understanding-and-Dialogue-Manager"><a href="#End-to-End-Joint-Learning-of-Natural-Language-Understanding-and-Dialogue-Manager" class="headerlink" title="End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager "></a><a href="http://t.cn/RfDCS5X" target="_blank" rel="external">End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager </a></h2><p>【对话系统】自然语言理解和对话管理通常是两个独立的任务，NLU的误差会影响到对话管理的效果。本文将两个任务联合起来进行端到端的训练，得到了不错的效果。建议研究对话系统的童鞋来读。</p>
<h2 id="Sequential-Match-Network-A-New-Architecture-for-Multi-turn-Response-Selection-in-Retrieval-based-Chatbots"><a href="#Sequential-Match-Network-A-New-Architecture-for-Multi-turn-Response-Selection-in-Retrieval-based-Chatbots" class="headerlink" title="Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots "></a><a href="http://t.cn/RIhcTFP" target="_blank" rel="external">Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots </a></h2><p>【对话系统】本文研究的问题是基于检索的多轮对话机器人，单轮对话和多轮对话的一大区别在于后者需要考虑更多的上下文内容，本文在检索答案时除了相关性还考虑了上下文之间的关系，建议研究检索式聊天机器人的童鞋来读本文。本文还给出了一个测试数据集，地址在：<a href="http://t.cn/RIhf4Sh" target="_blank" rel="external">http://t.cn/RIhf4Sh</a></p>
<h2 id="CER-Complementary-Entity-Recognition-via-Knowledge-Expansion-on-Large-Unlabeled-Product-Reviews"><a href="#CER-Complementary-Entity-Recognition-via-Knowledge-Expansion-on-Large-Unlabeled-Product-Reviews" class="headerlink" title="CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews "></a><a href="http://t.cn/RfDpyCm" target="_blank" rel="external">CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews </a></h2><p>【相关实体识别】本文研究的问题是产品评论数据中的相关实体识别问题，评论数据是个很有意思的数据，用户在买东西时希望可以通过对比买到更好的产品。建议做评论挖掘的童鞋读。</p>
<h2 id="The-Evolution-of-Sentiment-Analysis-A-Review-of-Research-Topics-Venues-and-Top-Cited-Papers"><a href="#The-Evolution-of-Sentiment-Analysis-A-Review-of-Research-Topics-Venues-and-Top-Cited-Papers" class="headerlink" title="The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers "></a><a href="http://t.cn/RIvkAop" target="_blank" rel="external">The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers </a></h2><p>【情感分析】【综述】一篇很细的情感分析的综述，刚刚进入这个领域的童鞋可以来读一读。</p>
<h2 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h2><h2 id="文本上的算法"><a href="#文本上的算法" class="headerlink" title="文本上的算法"></a><a href="http://t.cn/RhtyvzE" target="_blank" rel="external">文本上的算法</a></h2><p>《文本上的算法》v4.0：增加自然语言处理和对话系统章节；丰富了其他内容。</p>
<h2 id="NIPS-2016-Spotlight-Videos"><a href="#NIPS-2016-Spotlight-Videos" class="headerlink" title="NIPS 2016 Spotlight Videos"></a><a href="http://t.cn/RfB5cA2" target="_blank" rel="external">NIPS 2016 Spotlight Videos</a></h2><p>【NIPS 2016 Spotlight Videos】NIPS 2016焦点视频集。神经信息处理系统大会(Conference and Workshop on Neural Information Processing Systems)，简称NIPS，是一个关于机器学习和计算神经科学的国际会议。该会议固定在每年的12月举行,由NIPS基金会主办。NIPS是机器学习领域的顶级会议 。在中国计算机学会的国际学术会议排名中，NIPS为人工智能领域的A类会议。(via @网路冷眼)</p>
<h2 id="2016年深度学习的主要进展"><a href="#2016年深度学习的主要进展" class="headerlink" title="2016年深度学习的主要进展"></a><a href="http://t.cn/RIvuIuV" target="_blank" rel="external">2016年深度学习的主要进展</a></h2><p>2016年深度学习的主要进展，The major advancements in Deep Learning in 2016 (via @视觉机器人)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;End-to-End-Joint-Learning-of-Natural-Language-Underst
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十七期</title>
    <link href="http://rsarxiv.github.io/2016/12/10/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B8%83%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/10/PaperWeekly-第十七期/</id>
    <published>2016-12-10T18:16:44.000Z</published>
    <updated>2016-12-10T20:07:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>命名实体识别是自然语言处理中一个非常基础的工作，是自然语言处理中关键的一个环节。监督学习是解决命名实体识别的一个基本手段，但标注数据的获取成本往往会比较高，本期PaperWeekly将带大家来看一下如何通过半监督或者无监督的方法来做命名实体识别任务。本期分享的4篇Paper Notes分别是：</p>
<p>1、Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre), 2016<br>2、ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering, 2015<br>3、Bootstrapped Text-level Named Entity Recognition for Literature, 2016<br>4、Recognizing Named Entities in Tweets, 2011</p>
<h1 id="Building-a-Fine-Grained-Entity-Typing-System-Overnight-for-a-New-X-X-Language-Domain-Genre"><a href="#Building-a-Fine-Grained-Entity-Typing-System-Overnight-for-a-New-X-X-Language-Domain-Genre" class="headerlink" title="Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)"></a><a href="https://arxiv.org/pdf/1603.03112v1.pdf" target="_blank" rel="external">Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Rensselaer Polytechnic Institute,<br>Information Sciences Institute,<br>Rensselaer Polytechnic Institute</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>Entity Recognition and Typing, Unspuversied</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 2016</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>细粒度的实体识别是这几年比较流行的工作。传统的方法是需要先预定义一组实体所属类型，随后使用大量的标注数据来训练多分类器。本文针对需要标注数据的问题，提出了一个使用非监督学习的思路来解决这个问题</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>本文中方法的架构如下图:</p>
<p><img src="media/overview.png" alt="overvie"></p>
<p>1）通过entity mention的语料，构建entity mention的context<br>2）随后构建知识库的表达<br>3）通过知识库和entity mention进行连接<br>4）将连接后的数据学习三种表达</p>
<ul>
<li>a general entity distributed representation</li>
<li>a specific context representation</li>
<li>a knowledge representation</li>
</ul>
<p>其中entity distributed representation主要是通过上下文来表达实体。<br>而 a specific context representation主要是表达一些local feature和一些语言结构的特征。<br>最后a knowledge representation主要是用来模拟领域相关的知识</p>
<p>最后算法通过一个层次聚类算法来获取entity mention可能的分类信息</p>
<p>1、General Entity Representation<br>entity mention的表达作者主要是用了Skip-gram model通过大量的语料来训练，最终可以得到每个entity mention的表达。这个思路的好处是让两个entity mention属于同一类型时，entity mention的上下文会比较相似，进而可以得到相似的分布式表达</p>
<p>2、a specific context representation<br>为了得到a specific context representation，本文使用AMR（(Abstract Meaning Representation）语法或者句法结构的上下文。<br>其生成的结构如下图所示。根据给定的entity mention以及对应关系，首先选择entity mention可能的类型，如关系为ARG0 capital of ARG1则ARG0可能的类型则为国家，同理ARG1可能的类型为城市。随后将所有entity mention可能的候选类型通过一个encoder-decoder模型得到一个单一的表达</p>
<p><img src="media/context%20specific.png" alt="context specifi"></p>
<p>3、Knowledge Representation</p>
<p>由于entity mention的类型在很多情况是非常依赖领域相关的知识库的。因此本文也对知识库进行建模，从而推断出在某个相关领域下更细粒度的实体。为例计算Knowledge Representation，首先对entity mention跟知识库做连接。随后根据链接的实体和实体对应的属性以及类型信息构建一个基于权重的二步图。构建好的二步图根据 Large-scale information network embedding算法来对这个二步图训练并得到其分布式表达。</p>
<p>最后对于一个entity mention，将该entity mention对应的三种表达General Entity Representation，a specific context representation和Knowledge Representation整合，通过一个hierarchical X-means clustering算法得到这个entity mention在一个分类体系下的type信息。最终完成识别实体类型的信息。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>细粒度的实体识别是这几年比较流行的工作。传统的方法是需要先预定义一组实体所属类型，随后使用大量的标注数据来训练多分类器。这篇文章的创新点是提出了一个非监督学习的算法来识别实体所属的type，这种非监督的方法在缺少标注数据的垂直领域具有一定的实用性。本文的思路主要是通过文章中的entity mention跟知识库进行连接，通过文章的上下文学习entity mention的分布式表达，同时通过学习知识库中实体和类型的分布式表达。最后将这些表达送入一个层次聚类算法，entity mention得到的embedding和相似的知识库符号embedding会聚到同一个聚类下。进而通过非监督的方法对entity mention打上type的标签。实验证明本文的方法可以跟监督学习起到类似的效果。</p>
<h1 id="ClusType-Effective-Entity-Recognition-and-Typing-by-Relation-Phrase-Based-Clustering"><a href="#ClusType-Effective-Entity-Recognition-and-Typing-by-Relation-Phrase-Based-Clustering" class="headerlink" title="ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering"></a><a href="http://nlp.cs.rpi.edu/paper/entitytyping.pdf" target="_blank" rel="external">ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Xiang Ren, Ahmed El-Kishky, Chi Wang, Fangbo Tao, Clare R. Voss, Heng Ji, Jiawei Han</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>University of Illinois at Urbana-Champaign,<br>Microsoft Research, Redmond,<br>Rensselaer Polytechnic Institute,<br>Army Research Laboratory, Adelphi</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Entity Recognition and Typing,<br>Relation Phrase Clustering</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>KDD, 2015</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>远程监督方法在特定领域的实体抽取方面存在领域扩展性差、实体歧义问题以及上下文稀缺三大问题，本文主要研究如何改进这三个问题。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>针对上述的三个问题，本文提出了各自对应的解决思路：只使用浅层的分析方法例如POS等解决领域独立性问题；对entity mention(token span in the text document which refers to a real-world entity)应用词形和上下文联合建模来解决歧义问题；挖掘relation phrase和entity mention的共现情况，利用relation phrase前后实体（主语和宾语）的类别来找到相同的关系，进而辅助实体类型的推断。基于上述的思路，本文提出了ClusType的方法。</p>
<p>ClusType的问题定义如下：给定一个特定领域的文档集合，一个实体类型集合以及一个知识库，主要完成三个任务：第一，从文档集合中抽取出候选的entity mention集合；第二，将一部分entity mention链接到知识库，作为种子entity mention集合；第三，对于剩余未完成知识链接的entity mention集合，预测每一个entity mention的对应实体类别。</p>
<p>根据任务的定义，整个框架也分为三个部分，分别解决这三个任务。</p>
<p>本文方案的具体思路如下：</p>
<p>1、构建关系图</p>
<p>关系图的基本样式如下：  </p>
<p><img src="media/graph.png" alt="graph"></p>
<p>图当中的节点主要分为三种：entity mention, surface name, relation phrase.<br>图中的边的类型也有三种：entity mention和surface name的关系、surface name和relation phrase在语料中的共现情况、entity mention和entity mention的关系，表现entity mention之间的相似程度。这三个关系均是通过邻接矩阵的形式表示。<br>关于三种要素的确定，relation phrase的确定主要参考开放域抽取的方法，entity mention的确定方法也比较简单：首先找到固定长度的一个频繁词串集；为集合中每一个词串计算两两之间的得分，得分越高证明越需要合并；在合并的过程中，利用贪心算法，从得分最高开始合并，直到所有得分均低于某一阈值。</p>
<p>2、种子集合的生成</p>
<p>这里利用了dbpedia-spotlight工具进行entity mention到知识库的映射，只选取置信度得分高于0.8的作为有效输出。</p>
<p>3、实体类型推断<br>目标函数如下：<br><img src="media/function.png" alt="function"><br>公式共分为三部分：<br>第一部分遵循实体关系共现假设：如果一个surface name经常在relation phrase前后出现，那么它的类型应该同relation phrase前后实体的类型相关。  </p>
<p>第二部分遵循两个假设。<br>假设一：如果两个relation phrase相似，那么他们前后实体的类型也应该相似；<br>假设二：判断两个relation phrase相似的特征为词形、上下文和其前后实体的类型。<br>因此，第二部分的作用在于根据两个假设建模一个基于joint non-negative matrix factorization的multi-view clustering.</p>
<p>第三部分就是建模entity mention对应实体类别、entity mention之间的关系以及引入种子集合的监督，利用一个entity mention的surface name和relation phrase对应的关系类别推断关系类型，同时考虑到相似entity mention的一致性以及对于种子集合的预测误差函数。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>本文主要借鉴两方面的工作，一部分是远距离监督的方法，另一部分是开放关系抽取。<br>远距离监督的工作主要有：<br>1、N. Nakashole, T. Tylenda, and G. Weikum. Fine-grained semantic typing of emerging entities. In ACL, 2013.<br>2、T. Lin, O. Etzioni, et al. No noun phrase left behind: de- tecting and typing unlinkable entities. In EMNLP, 2012.<br>3、X. Ling and D. S. Weld. Fine-grained entity recognition. In AAAI, 2012.<br>开放关系抽取的工作主要有：<br>1、A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP, 2011.</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>本文通过对于远程监督方法的缺陷分析，提出了一种基于关系短语的实体识别方法。同时，还提出了一个领域无关的生成relation phrase和entity mention。通过将关系短语的聚类和实体类型的识别联合建模，可以在解决实体歧义和上下文问题上发挥很大的作用，而且可以根据entity mention的surface name和relation phrase预测关系类型。同时，我个人认为，将实体识别和关系识别进行联合建模可以起到一个相互促进的作用，而且可以很好的避免在这两个任务当中引入深度语法分析的工具如依存、句法分析等，减少误差积累和领域依赖性。未来两种任务结合依旧是一个很好的研究方向和热点。</p>
<h1 id="Bootstrapped-Text-level-Named-Entity-Recognition-for-Literature"><a href="#Bootstrapped-Text-level-Named-Entity-Recognition-for-Literature" class="headerlink" title="Bootstrapped Text-level Named Entity Recognition for Literature"></a><a href="http://people.eng.unimelb.edu.au/tbaldwin/pubs/acl2016-ner.pdf" target="_blank" rel="external">Bootstrapped Text-level Named Entity Recognition for Literature</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Julian Brooke，Timothy Baldwin，Adam Hammond</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>English and Comparative Literature San Diego State University<br>Computing and Information Systems The University of Melbourne</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>NER，Brown clustering，Text-level context classifier</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL2016</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>在无标注数据的情况下，对Literature做命名实体识别</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>模型主要分为四个部分：<br>1、Corpus preparation and segmentation<br>使用GutenTag tool对语料做基本的名称切分<br>2、Brown clustering<br>在预先切分好的预料上做Brown clustering。根据Brown clustering的聚类中的每个类的rank值，将聚类结果分成三个类别（PERSON，LOCATION，catch- all category）并将其作为Bootstrap的种子进行训练。<br>3、Text-level context classifier<br>为了解决Brown clustering聚类结果可能出现的一些confusion，引入了Text-level context classifier的思想。构建名称特征向量，将种子集数据放到LR模型中进行训练，得到分类模型。<br>4、Improved phrase classification<br>为解决模型对短语名词分类不准确问题，引入了改进的短语名称分类方法，在LR模型得到的p(t|r)值的基础上进一步对其优化得到修正的p’(t|r) ，修正方法如下：<br> <img src="media/imag1.png" alt="imag1"></p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>1、dataset：<a href="https://www.gutenberg.org" target="_blank" rel="external">https://www.gutenberg.org</a><br>2、GutenTag tool：<a href="http://www.projectgutentag.org" target="_blank" rel="external">http://www.projectgutentag.org</a>   </p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>在Literature上做NER任务的工作包括：<br>1、(He et al., 2013)character speech identification<br>2、(Bamman et al., 2014)analysis of characterization<br>3、(Vala et al., 2015)character identification<br>4、(Vala et al. 2015)character identification deal the multiple aliases of the same character problem</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文的创新点在于，使用了无监督学习模型对特定领域(fiction)知识做NER，并取得了很好的效果。但是本文方法主要研究特定领域知识的NER，因此本方法使用在跨领域跨语言的NER识别任务中并不能达到很好的效果，方法具有一定的局限性。</p>
<h1 id="Recognizing-Named-Entities-in-Tweets"><a href="#Recognizing-Named-Entities-in-Tweets" class="headerlink" title="Recognizing Named Entities in Tweets"></a><a href="http://people.dbmi.columbia.edu/~szhang/P11-1037.pdf" target="_blank" rel="external">Recognizing Named Entities in Tweets</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Xiaohua Liu, Shaodian Zhang, Furu Wei, Ming Zhou</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>Harbin Institute of Technology,<br>Shanghai Jiao Tong University,<br>Microsoft Research Asia</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Named Entity Recognition, Semi-Supervised Learning</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL, 2011</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>如何建立一种半监督学习的模型对使用非正式语言的tweet进行命名实体识别？</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>现有的分词、词性标注、NER工具解决非正式语言占主导的tweet时常常会失效，得不到令人满意的结果，而twitter作为一种主流的社交媒体，有着丰富的语料和非常高的研究价值。本文以tweet为研究对象，提出了一种基于bootstrapping的半监督学习方案。</p>
<p>tweet的NER任务包括四类实体：Person、Location、Organization和Product，标注方法用BILOU标注法，而没有用经典的IOB标注法。</p>
<p>本文方案的具体思路如下：</p>
<p><img src="media/knn-crf.png" alt="knn-crf"></p>
<p>1、KNN分类器</p>
<p>将tweet中的每个词用词袋模型表示，输入到KNN中得到一个分类标签，这个标签作为CRF标注时的输入。</p>
<p>2、CRF标注器</p>
<p>NER是一个典型的序列标注任务，CRF是解决序列标注问题的一个典型方法。</p>
<p>3、训练过程：</p>
<p>（1）先根据已有标注数据，训练好初始的KNN和CRF模型。<br>（2）获得未标注的tweet，每条tweet中的每个词都经过KNN分类器，得到一个分类标签和相应的概率，如果这个概率大于预设阈值，则更新这个标签给该词。整个tweet经过KNN之后，作为特征输入到CRF模型中进行预测，如果预测出的结果概率大于预设阈值，则认为该标注结果可靠，加入可靠结果集中。<br>（3）当可靠结果集的数量达到N=1000时，则重新训练KNN和CRF模型，并且清空可靠结果集，继续（2）的过程。</p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>基于bootstrapping做NER任务的工作还包括：</p>
<p>1、Instance weighting for domain adaptation in nlp, 2007<br>2、Domain adaption bootstrapping for named entity recognition, 2009</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>本文是比较早的文章了，算是比较早地探索tweet文本挖掘。bootstrapping是一种经典的半监督学习方法，通过从大量的非标注文本中进行学习和补充，来提高训练数据集的规模。tweet是一种非正式语言的文本，现有的NLP工具基本上都不好用，包括微博、论坛的文本都面临这样的问题，而且这样的文本占据着更大的比重，非常有必要对类似的文本进行NLP工具的研究，大概想了两种思路，要么专门地来研究一套适合这种非正式文本的工具，要么想办法将这样的文本转化为正式的语言，用现有的工具来解决问题。现在很火的chatbot对话理解也面临这样的问题，大家在和bot对话的时候说的话也是类似的非正式语言，如何准确理解和分析这类话，对于chatbot能否真的被应用至关重要。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>NER的应用场景非常广泛，基于监督学习的训练方法是最简单、最有效的方法，但在实际应用中常常会遇到训练数据难以获得的尴尬境地，那么半监督和无监督学习的研究正是为了解决这个问题，值得关注！感谢@高桓 @韩其琛 @min279 @zhangjun 四位童鞋的辛勤工作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;命名实体识别是自然语言处理中一个非常基础的工作，是自然语言处理中关键的一个环节。监督学习是解决命名实体识别的一个基本手段，但标注数据的获取成
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>本周值得读(2016.11.28-2016.12.02)</title>
    <link href="http://rsarxiv.github.io/2016/12/04/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-11-28-2016-12-02/"/>
    <id>http://rsarxiv.github.io/2016/12/04/本周值得读-2016-11-28-2016-12-02/</id>
    <published>2016-12-04T18:06:33.000Z</published>
    <updated>2016-12-04T18:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation"><a href="#A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation" class="headerlink" title="A Simple, Fast Diverse Decoding Algorithm for Neural Generation "></a><a href="http://t.cn/Rfj8F3k" target="_blank" rel="external">A Simple, Fast Diverse Decoding Algorithm for Neural Generation </a></h2><p>【beam search】在用seq2seq做一些nlp任务的时候，解码器负责将结果一个个地解出来。解码出的结果应该具有多样性的特点，尤其是在chatbot应用中体现更为突出。传统的beam search算法在解码时常常会解出一些非常安全但是没有实际意义的response，类似于“呵呵呵”，“我认为是这样的”这里的话。本文的工作针对这一问题，提出了一种改进版的beam search算法，通过引入一个惩罚因子来影响排序结果，从而使得解码出的结果更加多样性。建议研究seq2seq或者尝试用它来解决一些问题的童鞋可以精读此文。本文来自Jiwei Li。</p>
<h2 id="Dialogue-Learning-With-Human-In-The-Loop"><a href="#Dialogue-Learning-With-Human-In-The-Loop" class="headerlink" title="Dialogue Learning With Human-In-The-Loop "></a><a href="http://t.cn/Rf8XOcr" target="_blank" rel="external">Dialogue Learning With Human-In-The-Loop </a></h2><p>【对话系统】【在线学习】在线学习是chatbot在与人交互的过程中自动学习的一种方法，快速而且有效。本文给出了一种基于强化学习的在线交互学习方案，作者是Jiwei Li。建议研究或者做chatbot应用的童鞋可以好好研读此文。</p>
<h2 id="Visual-Dialog"><a href="#Visual-Dialog" class="headerlink" title="Visual Dialog "></a><a href="http://t.cn/RfH7BWW" target="_blank" rel="external">Visual Dialog </a></h2><p>【多模态对话】多模态问答（VQA）是一个比较好玩的任务，本文在此基础上提出了一个更加复杂而且有意思的任务，即给定一张图像，给出若干个问和答的历史对话，提出一个新问题，要求给出正确答案。问题不仅仅需要理解图片，而且需要理解历史对话。新的任务意味着新的坑，文中给出了一些常见NN模型作为baseline，感兴趣的童鞋可以入坑。</p>
<h2 id="Neural-Machine-Translation-with-Latent-Semantic-of-Image-and-Text"><a href="#Neural-Machine-Translation-with-Latent-Semantic-of-Image-and-Text" class="headerlink" title="Neural Machine Translation with Latent Semantic of Image and Text "></a><a href="http://t.cn/RfjRQMP" target="_blank" rel="external">Neural Machine Translation with Latent Semantic of Image and Text </a></h2><p>【Visual NMT】就在NMT被讨论地如火如荼的时候，还有一部分工作是结合多模态（图片）来做机器翻译，因为人类获取信息不仅仅可以通过文字，图片也是一个重要的学习资源。</p>
<h2 id="Scalable-Bayesian-Learning-of-Recurrent-Neural-Networks-for-Language-Modeling"><a href="#Scalable-Bayesian-Learning-of-Recurrent-Neural-Networks-for-Language-Modeling" class="headerlink" title="Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling "></a><a href="http://t.cn/RfjELTY" target="_blank" rel="external">Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling </a></h2><p>【贝叶斯学习】通过BPTT来训练的RNN在解决问题上会存在过拟合的问题，一个主要原因是随机优化训练无法给出模型权重的概率估计，本文通过最近stochastic gradient Markov Chain Monte Carlo的研究来试着学习模型权重的概率。语言模型的实验结果和其他的相关实验结果表明本文所用的方法确实有效。</p>
<h2 id="Learning-Python-Code-Suggestion-with-a-Sparse-Pointer-Network"><a href="#Learning-Python-Code-Suggestion-with-a-Sparse-Pointer-Network" class="headerlink" title="Learning Python Code Suggestion with a Sparse Pointer Network "></a><a href="http://t.cn/RfjEjY5" target="_blank" rel="external">Learning Python Code Suggestion with a Sparse Pointer Network </a></h2><p>【代码补全】本文研究的问题非常有意思，就是大家常见的IDE代码补全功能。现有的IDE对静态编程语言支持的比较好，对于动态编程语言支持的一般，而且一般都是补全某个函数或者方法之类的，而不能给出更复杂的代码。本文针对这个问题，构造了一个大型的python code数据集，并且用了比较流行的Pointer Network模型来做端到端的训练，取得了不错的效果。代码补全在实际应用中非常有用，但想做到很复杂、很智能的补全还有很长的路。不过这个topic还是一个非常有意思的东西。</p>
<h2 id="Joint-Copying-and-Restricted-Generation-for-Paraphrase"><a href="#Joint-Copying-and-Restricted-Generation-for-Paraphrase" class="headerlink" title="Joint Copying and Restricted Generation for Paraphrase "></a><a href="http://t.cn/RfHAsim" target="_blank" rel="external">Joint Copying and Restricted Generation for Paraphrase </a></h2><p>【NLG】本文的思路与Pointer Network或者Copynet类似，在用seq2seq做自然语言生成时，增加一个判断的环节，来决定接下来的这个词是从source来copy还是用decoder来rewrite。</p>
<h2 id="Context-aware-Natural-Language-Generation-with-Recurrent-Neural-Networks"><a href="#Context-aware-Natural-Language-Generation-with-Recurrent-Neural-Networks" class="headerlink" title="Context-aware Natural Language Generation with Recurrent Neural Networks "></a><a href="http://t.cn/RfEClfC" target="_blank" rel="external">Context-aware Natural Language Generation with Recurrent Neural Networks </a></h2><p>【NLG】论文的方法、模型没有太多的值得说的地方，倒是应用的点非常有意思，根据商品的上下文来伪造评论，人工评判时有50%以上的伪造评论都通过了，90%以上骗过了现有的识别算法。有点道高一尺魔高一丈的感觉，如果这篇paper的结果确实这么牛的话，确实很有意思，值得研究一下。</p>
<h2 id="MS-MARCO-A-Human-Generated-MAchine-Reading-COmprehension-Dataset"><a href="#MS-MARCO-A-Human-Generated-MAchine-Reading-COmprehension-Dataset" class="headerlink" title="MS MARCO: A Human Generated MAchine Reading COmprehension Dataset "></a><a href="http://t.cn/RfH2eXu" target="_blank" rel="external">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset </a></h2><p>【机器阅读理解】【数据福利】微软放出了一个100k规模的机器阅读理解数据集，数据来源于真实的Bing搜索query。数据包括：query、10个相关的passage和query对应的answer。</p>
<h2 id="NewsQA-A-Machine-Comprehension-Dataset"><a href="#NewsQA-A-Machine-Comprehension-Dataset" class="headerlink" title="NewsQA: A Machine Comprehension Dataset "></a><a href="http://t.cn/Rf8MPnf" target="_blank" rel="external">NewsQA: A Machine Comprehension Dataset </a></h2><p>【机器阅读理解】【数据福利】Maluuba公司放出一个新的机器阅读理解的数据集，规模在100k左右，数据来源为CNN新闻。通过用多个之前表现比较好的NN模型和人工结果对比，发现F1指标存在25.3%的差距，说明本数据集需要更好的模型来进行研究。数据集已公开，地址是：<a href="http://datasets.maluuba.com/NewsQA" target="_blank" rel="external">http://datasets.maluuba.com/NewsQA</a></p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="中国人工智能学会通讯"><a href="#中国人工智能学会通讯" class="headerlink" title="中国人工智能学会通讯"></a><a href="http://t.cn/Rf8Yvwn" target="_blank" rel="external">中国人工智能学会通讯</a></h2><p>《中国人工智能学会通讯》，本期为学会优秀博士论文专刊</p>
<h2 id="C-wrapper"><a href="#C-wrapper" class="headerlink" title="C++ wrapper"></a><a href="http://t.cn/RfEIAdZ" target="_blank" rel="external">C++ wrapper</a></h2><p>TensorFlow使用swig作为C++ wrapper，最近Google又推出了pyclif，宣称“it’s much cleaner and easier” </p>
<h2 id="智能时代的自然语言处理"><a href="#智能时代的自然语言处理" class="headerlink" title="智能时代的自然语言处理"></a><a href="http://t.cn/Rfm4hJb" target="_blank" rel="external">智能时代的自然语言处理</a></h2><p>今天ADL前沿讲习班《智能时代的自然语言处理》Zhengdong Lu的报告，题目Recent Progress on Deep Learning for NLP。</p>
<h2 id="自然语言处理中深度学习活跃领域的课程讲义"><a href="#自然语言处理中深度学习活跃领域的课程讲义" class="headerlink" title="自然语言处理中深度学习活跃领域的课程讲义"></a><a href="http://www.zishu010.com/z/newdetail/9404521.html" target="_blank" rel="external">自然语言处理中深度学习活跃领域的课程讲义</a></h2><p>本文是纽约大学助理教授 Sam Bowman 关于自然语言处理中深度学习活跃领域的课程讲义PPT。对深度学习NLP领域最近较为活跃的研究进行了综述，其中包括Attention 模型、结构化记忆、词水平以上的无监督学习等等。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-G
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十六期</title>
    <link href="http://rsarxiv.github.io/2016/12/03/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%85%AD%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/03/PaperWeekly-第十六期/</id>
    <published>2016-12-03T18:08:45.000Z</published>
    <updated>2016-12-03T18:36:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本期PaperWeekly将带着大家来看一下ICLR 2017的六篇paper，其中包括当下非常火热的GAN在NLP中的应用，开放域聊天机器人如何生成更长更丰富的回答，如何用强化学习来构建树结构的神经网络和层次化的记忆网络等内容。六篇paper分别是：</p>
<p>1、A SELF-ATTENTIVE SENTENCE EMBEDDING<br>2、Adversarial Training Methods for Semi-Supervised Text Classification<br>3、GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS<br>4、Hierarchical Memory Networks<br>5、Mode Regularized Generative Adversarial Networks<br>6、Learning to compose words into sentences with reinforcement learning</p>
<h1 id="A-SELF-ATTENTIVE-SENTENCE-EMBEDDING"><a href="#A-SELF-ATTENTIVE-SENTENCE-EMBEDDING" class="headerlink" title="A SELF-ATTENTIVE SENTENCE EMBEDDING"></a><a href="http://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="external">A SELF-ATTENTIVE SENTENCE EMBEDDING</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou &amp; Yoshua Bengio</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>IBM Watson<br>Universit´e de Montr´eal</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>self-attention, sentence embedding, author profiling, sentiment classification, textual entailment</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>本文提出一种在没有额外输入的情况下如何利用attention来提高模型表现的句子表示方法。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>本文提出的模型结构分为两部分，</p>
<ol>
<li>BLSTM<br>这部分采用双向LSTM对输入的文本进行处理，最后得到BLSTM的所有隐层状态H。</li>
<li>Self-attention mechanism<br>同attention机制类似，我们需要计算一个权重向量a，然后通过对隐层状态H加权求和得到句子的表示向量。这个过程如下公式所示：<br><img src="media/equation1.png" alt="equation1"><br>但是实际任务中，我们通常可能会对一个句子语义的多个方面感兴趣，因此我们可以通过下面的公式，获得多个权重向量组成的矩阵A。<br><img src="media/equation2.png" alt="equation2"><br>然后每一个权重向量a都可以得到一个句子表示向量v，所有句子表示向量组合在一起就可以获得句子表示矩阵M。<br><img src="media/equation3.png" alt="equation3"><br>本文的模型在author profiling, sentiment classification和textual entailment三个任务上进行验证，都取得了较好的效果。</li>
</ol>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>1、[Yelp]<br>(<a href="https://www.yelp.com/dataset" target="_blank" rel="external">https://www.yelp.com/dataset</a> challenge)<br>2、 [SNLI]<br>(<a href="http://nlp.stanford.edu/projects/snli/" target="_blank" rel="external">http://nlp.stanford.edu/projects/snli/</a>)</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>A large annotated<br>corpus for learning natural language inference</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本文提出的self-attention方法用一个matrix表示一个句子，并且matrix中的每一个vector都是句子语义某一方面的表示，增强了sentence embedding的可解释性。</p>
<h1 id="Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification"><a href="#Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification" class="headerlink" title="Adversarial Training Methods for Semi-Supervised Text Classification"></a><a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="external">Adversarial Training Methods for Semi-Supervised Text Classification</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Takeru Miyato, Andrew M. Dai, Ian Goodfellow</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Google Brain, Kyoto University和OpenAI</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Adversarial training, text classification, semi-supervised learning</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>Adversarial training和virtual adversarial training都需要对输入的数字形式做小的perturbation，不适用于高维稀疏输入，比如one-hot word representations。文章扩展图像领域流行的这两种方法到文本领域，对word embedding进行perturbation来作为LSTM的输入，取代原本的输入向量。可以把这两种方法看做是正则化的方法，为输入加入噪声，可以用来实现semi-supervised的任务。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>以adversarial training为例，文章对word embeddings进行adversarial perturbation，而不是直接应用在输入上。假设normalized之后的输入序列为s，给定s，y的条件概率为p(y|s;theta)，其中theta为模型参数，则s上的adversarial perturbation r_adv为：<br><img src="media/16-1-1.png" alt="16-1"></p>
<p>应用在LSTM上，如下图(b)所示。定义其adversarial loss如下：</p>
<p><img src="media/adversarial1.png" alt="adversaria"></p>
<p><img src="media/16-2.png" alt="16-2"></p>
<p>其中N为labeled的例子的数目。通过随机梯度下降来进行training。</p>
<p>文章也提供了virtual adversarial training的方法。</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="external">IMDB</a><br>2、<a href="http://riejohnson.com/cnn_data.html" target="_blank" rel="external">Elec</a><br>3、<a href="http://snap.stanford.edu/data/web-Amazon.html" target="_blank" rel="external">Rotten Tomatoes</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>主要列三篇work：<br>1、2015年NIPS, SA-LSTM。Semi-supervised sequence learning<br>2、2015年NIPS，One-hot CNN。Semi-supervised convolutional neural networks for text categorization via region<br>embedding<br>3、2016年ICML，One-hot bi-LSTM。Supervised and semi-supervised text categorization using LSTM for region<br>embeddings</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>作者将图像领域的adversarial training应用在了文本领域，改善了word embedding。传统的word embedding被语法结构影响，即使两个完全相反的词（比如”good”和”bad”）在表示形式上也是相近的，没有表示出词本身的意思。Adversarial training使得有相近语法结构但是不同意义的词能够被分开，可以用来做情感分类和sequence model等。</p>
<h1 id="GENERATING-LONG-AND-DIVERSE-RESPONSES-WITH-NEURAL-CONVERSATION-MODELS"><a href="#GENERATING-LONG-AND-DIVERSE-RESPONSES-WITH-NEURAL-CONVERSATION-MODELS" class="headerlink" title="GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS"></a><a href="http://openreview.net/pdf?id=HJDdiT9gl" target="_blank" rel="external">GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray Kurzweil1</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>Google Research, Google Brain</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Long and Diverse Responses</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>开放域聊天机器人如何生成更长且较为丰富的回答？</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>本文模型是基于经典的seq2seq+attention框架，在其基础上进行了若干修改，得到了满意的效果。不同于之前模型的地方有两点：</p>
<p>1、encoder不仅仅包括整个source，还包括一部分target，这样attention不仅仅考虑了source，而且考虑了部分target。</p>
<p><img src="media/16-3.png" alt="16-3"></p>
<p>经典的seq2seq+attention在decoding部分会将source中的每个token都考虑到attention中来，之前有一种做法是将整个target部分也加入到attention中，效果上虽然有一定的提升，但随着数据规模地增加，内存代价太大。本文正是针对这一个问题，提出了所谓的“glimpse”模型，如上图所示，在encoder部分加入了target的前几个token，相当于是上面两种方案的一种折中。</p>
<p>2、提出了一种基于sampling的beam search decoding方案。</p>
<p>经典的beam search在decoding部分，是基于MAP（最大后验概率）进行贪婪解码的，这种方案生成的responses具有简短、无信息量以及高频的特点，通俗地讲会生成很多的类似“呵呵”的话，没有太多营养和价值。(Jiwei Li,2015)在解决这个问题时，在decoding部分通过MMI（互信息）对N-best结果进行重排序，这种方法对于生成短文本效果显著，但对于生成长文本效果不佳。因为，基于MAP的beam search天然存在这样的问题，N-best和重排序都解决不了根本性的问题。针对这一问题，本文提出了一种基于sampling的beam search解码方案，sampling即在每一步解码时都sample出D个token作为候选，搜索完毕或达到预设的长度之后，生成B个候选responses，然后进行重排序。</p>
<p>本文的另外一大亮点是用了大量的对话数据，用了很大规模参数的模型进行了实验。实验评价标准，在自动评价这部分，设计了一个N选1的实验，给定一个输入，将正确输出和错误输出混在一起，模型需要从中选择正确的输出，用选择准确率来作为自动评价指标。本文没有用到经典的BLEU指标，因为这个指标确实不适合评价对话的生成质量。为了更有说服力，本文用人工对结果进行评价。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>本文用到的对话数据：<br>1、<a href="https://redd.it/3bxlg7" target="_blank" rel="external">Reddit Data</a><br>2、<a href="http://opus.lingfil.uu.se/OpenSubtitles.php" target="_blank" rel="external">2009 Open Subtitles data</a><br>3、<a href="https://data.stackexchange.com/" target="_blank" rel="external">Stack Exchange data</a><br>4、本文作者从Web抽取的对话数据（待公开）</p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>用seq2seq方法研究生成对话的质量（包括长度、多样性）的工作并不多，具有代表性的有下面两个工作：<br>1、Wu,2016 提出了用length-normalization的方案来生成更长的对话<br>2、Jiwei Li,2015 提出了在解码阶段用MMI（互信息）对N-best结果进行重排序，旨在获得信息量更大的对话。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文模型部分并没有太多的创新，因为是工业部门的paper，所以更多的是考虑实用性，即能否在大规模数据集上应用该模型，集中体现在glimpse模型上。为了生成更加长、更加多样性的对话，在原有beam search + 重排序的基础上，引入了sampling机制，给生成过程增加了更多的可能性，也是工程上的trick。对话效果的评价是一件很难的事情，人类希望bot可以生成类人的对话，回复的长度可以定量描述，但多样性、生动性、拟人化等等都难以定量描述，所以在探索生成对话的这个方向上还有很长的路要走。</p>
<h1 id="Hierarchical-Memory-Networks"><a href="#Hierarchical-Memory-Networks" class="headerlink" title="Hierarchical Memory Networks"></a><a href="https://arxiv.org/pdf/1605.07427v1.pdf" target="_blank" rel="external">Hierarchical Memory Networks</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>1、Université de Montréal, Canada.<br>2、Twitter Cortex, USA.<br>3、IBM Watson Research Center, USA.<br>4、CIFAR, Canada.  </p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Hierarchical Memory Networks，Maximum Inner Product Search (MIPS)</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>记忆网络主要包括hard attention和soft attenion两种，然而hard不能用于反向传播算法进行端到端训练，所以只能使用强化学习的方法进行训练；soft所涉及的计算参数又很大，只适合于少量Memory。本文提出Hierarchical Memory Networks(HMN)模型，算是soft和hard的一个混合模型，计算量减少且训练更加容易，<br>实验结果也很好。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>soft attention是对所有的memory都要进行attention的计算，对全集计算使计算量很大。HMN利用层次化结构使得attention的集合缩小，利用MaximumInner Product Search(MIPS)的方法从全集中获得一个最优子集，在子集上面去做attention就大大降低计算量。这样的方式又和hard attention预测关注点的方法有些类似，将注意力放在最相关的那部分，这个的做法也更接近于人的注意力思维。  文章的核心部分在于如何获取与query最相近的子集。</p>
<p>主实验主要包括两个:<br>1、Exact K-MIPS：计算复杂度依然和soft attention差不多。<br>2、Approximate K-MIPS：利用Maximum Cosine Similarity Search(MCSS)的方法代替MIPS的方法，牺牲一些精确度，降低复杂度和加快训练速度。  </p>
<p>MIPS有三种方法，分别是基于hash,基于tree,基于clustering，基于上述三种方法文中又做了几组组对比实验，最后实验结果显示基于clustering的效果是最好的。</p>
<p>文章得到的实验结果如下：<br><img src="media/HMN_Result.png" alt="HMN_Result"></p>
<h2 id="资源-（可选）"><a href="#资源-（可选）" class="headerlink" title="资源 （可选）"></a>资源 （可选）</h2><p>1、<a href="https://www.dropbox.com/s/tohrsllcfy7rch4/SimpleQuestions_v2.tgz" target="_blank" rel="external">The SimpleQuestions dataset</a>(使用的是Large-scale simple question answering with memory networks文章中的数据集)<br>2、<a href="https://research.facebook.com/research/babi/" target="_blank" rel="external">babi</a></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、arXiv 2014, soft attention,《Neural turing machines》<br>2、CoRR 2015, hard attention,《Reinforcement learning neural turing machine》<br>3、ICLR 2015, memory network,《Memory networks》<br>4、arXiv 2015,《End-to-end memory networks》,引入半监督记忆网络可以自学所需要的facts。<br>5、CoRR 2016, DMN, 《Dynamic memory networks for visual and textual question<br>answering》,增加了一个episodic memory 使得可以动态更新memory里面的内容。</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>文章的创新主要在于修改了两个模块：Memory和Reader。<br>1、将memory的结构从a flat of array变成了hierarchical memory structure。将memory分成若干groups,这些groups又可以在进行更高级别的组合。<br>2、reader是从MIPS选出的子集中使用soft attention。MIPS从memory中选出一<br>个group子集作为最相关的子集。</p>
<h1 id="Mode-Regularized-Generative-Adversarial-Networks"><a href="#Mode-Regularized-Generative-Adversarial-Networks" class="headerlink" title="Mode Regularized Generative Adversarial Networks "></a><a href="http://openreview.net/pdf?id=HJKkY35le" target="_blank" rel="external">Mode Regularized Generative Adversarial Networks </a></h1><h2 id="作者-4"><a href="#作者-4" class="headerlink" title="作者"></a>作者</h2><p>Tong Che; Yanran Li</p>
<h2 id="单位-4"><a href="#单位-4" class="headerlink" title="单位"></a>单位</h2><p>Montreal Institute for Learning Algorithms;<br>Department of Computing, The Hong Kong Polytechnic University</p>
<h2 id="关键词-4"><a href="#关键词-4" class="headerlink" title="关键词"></a>关键词</h2><p>GAN, Regularizers</p>
<h2 id="文章来源-4"><a href="#文章来源-4" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h2><p>本文针对的问题是：1、GAN 的训练过程很不稳定 2、GAN 生成的样本局限于训练样本中的大 model 上，不能平衡数据的分布（missing model problem）。<br>两个问题互相影响，导致训练结果不好。</p>
<h2 id="模型-4"><a href="#模型-4" class="headerlink" title="模型"></a>模型</h2><p>针对上面的问题，作者提出了两种 regularizers 去控制 GAN 的训练过程。<br>第一个 regularizer 也被作者称为 Regularized-GAN。作者认为可以从 generator 入手，给 generator 增加 regularizer，使得其具有更好的 gradient ，这样 G 和 D 都能稳定训练。<br>具体的方法是增加一个 encoder E(x) : X → Z.即把原先的 noise vector z 改为 z = encoder(X) ，即然后再 G(encoder(X))。如下图：<br><img src="media/16-5.png" alt="16-5"></p>
<p>这样做有两个好处。第一，原始的模型很容易出现梯度消失的情况，因为 discriminator D 特别容易区分真实数据和生成数据导致 generator 就得不到 D 的梯度。作者的模型多了一个 reconstruction 的部分，这样生成出来数据不再那样容易被 D 识别出来。所以 D 和 G 就都能一直有 gradient 去训练，从而提高稳定性。第二，对于 x ，G(E(x)) 会尽量去生成 x 原本所属的类，从而一定程度解决了 missing model problem。<br>第二个 regularizer 基于第一个 regularizer 旨在改进训练的方法，也被作者称为 manifold-diffusion GAN。分为两步，第一步 manifold step 训练 discriminator D1 ，目的是减少 G(Enc(X)) 和 X 的的差别；第二步 diffusion 就是训练 D2 让 G(Enc(X)) 和 G(z) 分布的距离接近。如下图：</p>
<p><img src="media/16-6.png" alt="16-6"></p>
<p>最后，作者把 GAN 的网络训练坍塌的情况考虑进去，提出了新的 evaluation metric。</p>
<h2 id="相关工作-4"><a href="#相关工作-4" class="headerlink" title="相关工作"></a>相关工作</h2><p>本篇文章的作者李嫣然写过一篇非常棒的<a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;mid=2650325352&amp;idx=1&amp;sn=90fb15cee44fa7175a804418259d352e&amp;mpshare=1&amp;scene=1&amp;srcid=0829ixEhnGChNKAl5kgz6b9V#rd" target="_blank" rel="external">综述</a> ,在这里就不累赘阐述了。</p>
<h2 id="简评-4"><a href="#简评-4" class="headerlink" title="简评"></a>简评</h2><p>当下 GAN 的研究非常火爆，出现了许许多多对 GAN 的改进，本篇文章的提出的两种 regularizers 非常有效的提高了 GAN 的稳定性（其中 regularizer 的思想也受到了监督学习的启发），值得对 GAN 感兴趣的同学研读。</p>
<h2 id="完成人信息"><a href="#完成人信息" class="headerlink" title="完成人信息"></a>完成人信息</h2><p>professorshui@gmail.com</p>
<h1 id="Learning-to-compose-words-into-sentences-with-reinforcement-learning"><a href="#Learning-to-compose-words-into-sentences-with-reinforcement-learning" class="headerlink" title="Learning to compose words into sentences with reinforcement learning"></a><a href="https://openreview.net/forum?id=Skvgqgqxe" target="_blank" rel="external">Learning to compose words into sentences with reinforcement learning</a></h1><h2 id="作者-5"><a href="#作者-5" class="headerlink" title="作者"></a>作者</h2><p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling</p>
<h2 id="单位-5"><a href="#单位-5" class="headerlink" title="单位"></a>单位</h2><p>Google</p>
<h2 id="关键词-5"><a href="#关键词-5" class="headerlink" title="关键词"></a>关键词</h2><p>Tree-LSTM, Reinforcement Learning</p>
<h2 id="文章来源-5"><a href="#文章来源-5" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017</p>
<h2 id="问题-5"><a href="#问题-5" class="headerlink" title="问题"></a>问题</h2><p>使用强化学习来构建树结构的神经网络Tree-LSTM，学习自然语言的句子表示</p>
<h2 id="模型-5"><a href="#模型-5" class="headerlink" title="模型"></a>模型</h2><p>模型分为两部分：Tree-LSTM和强化学习模型<br>应用Tree-LSTM(可以通过LSTM的忘记门机制，跳过整棵对结果影响不大的子树)，并结合{SHIFT，REDUCE}操作，SHIFT操作对应将一个节点压入栈，REDUCE对应将两个元素组合，从而建立树结构</p>
<p>强化学习用来寻找最佳的节点组合情况，RL模型中的状态s即当前构建的树结构，a为{SHIFT，REDUCE}操作，reward对应不同downstream<br> task(例：若是用该句子表示进行分类任务，则r对应从策略网络中采样得到句子表示的分类准确性的概率)</p>
<h2 id="资源-3"><a href="#资源-3" class="headerlink" title="资源"></a>资源</h2><p>作者将该工作进行了四组实验，情感分类，语义相关性判断，自然语言推理，句子生成<br>分别应用Stanford Sentiment Treebank，Sentences Involving Compositional Knowledge corpus，Stanford Natural Language Inference corpus，IMDB movie review corpus</p>
<h2 id="相关工作-5"><a href="#相关工作-5" class="headerlink" title="相关工作"></a>相关工作</h2><p>与Socher等人之前提出的Recursive NN,MV-RNN,RNTN，Tree-LSTM等工作一脉相承，本文又加入了RL方式构建树形结构</p>
<h2 id="简评-5"><a href="#简评-5" class="headerlink" title="简评"></a>简评</h2><p>将强化学习引入句子表示学习之中，学习构建树的不同方式，从左向右，从右向左，双向，有监督、半监督、预先无结构等方式去构建树结构，但是训练时间较长，在几个任务上效果提升不是特别明显。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>GAN是当下的研究热点之一，在图像领域中研究较多，本期的两篇paper探讨了GAN在NLP中的应用，值得关注和期待。最后感谢@destinwang、@gcyydxf、@chunhualiu、@tonya、@suhui和@zhangjun六位童鞋的辛勤工作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;本期PaperWeekly将带着大家来看一下ICLR 2017的六篇paper，其中包括当下非常火热的GAN在NLP中的应用，开放域聊天机器
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>中文分词工具测评</title>
    <link href="http://rsarxiv.github.io/2016/11/29/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E6%B5%8B%E8%AF%84/"/>
    <id>http://rsarxiv.github.io/2016/11/29/中文分词工具测评/</id>
    <published>2016-11-29T17:49:50.000Z</published>
    <updated>2016-11-29T19:07:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>分词对于研究和应用中文自然语言处理的童鞋来说，都是一个非常非常基础的部件，分词的质量直接影响到后续词性标注、命名实体识别、句法分析等部件的准确性。作为一个基础部件，学术界对分词的研究已经非常久了，市面上流行的几大开源分词工具也被工业界的各大公司应用很多年了。最近，中文分词随着一篇博文的发表被推到了风口浪尖，引发众多大牛在微博、微信群里的激烈讨论。本文并不想对这篇博文进行过多评论，只是想用公开的数据集对各大分词工具进行一个客观地测评，以供大家在选择工具时有所依据。</p>
<h1 id="中文分词工具"><a href="#中文分词工具" class="headerlink" title="中文分词工具"></a>中文分词工具</h1><p>本文选择了4个常见的分词工具，分别是：哈工大LTP、中科院计算所NLPIR、清华大学THULAC和jieba，为了对比分词速度，选择了这四个工具的c++版本进行评测。</p>
<p>1、LTP <a href="https://github.com/HIT-SCIR/ltp" target="_blank" rel="external">https://github.com/HIT-SCIR/ltp</a><br>2、NLPIR <a href="https://github.com/NLPIR-team/NLPIR" target="_blank" rel="external">https://github.com/NLPIR-team/NLPIR</a><br>3、THULAC <a href="https://github.com/thunlp/THULAC" target="_blank" rel="external">https://github.com/thunlp/THULAC</a><br>4、jieba <a href="https://github.com/yanyiwu/cppjieba" target="_blank" rel="external">https://github.com/yanyiwu/cppjieba</a></p>
<h1 id="测试数据集"><a href="#测试数据集" class="headerlink" title="测试数据集"></a>测试数据集</h1><p>1、SIGHAN Bakeoff 2005 MSR, 560KB  <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/</a><br>2、SIGHAN Bakeoff 2005 PKU, 510KB  <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/</a><br>3、人民日报 2014, 65MB  <a href="https://pan.baidu.com/s/1hq3KKXe" target="_blank" rel="external">https://pan.baidu.com/s/1hq3KKXe</a></p>
<p>前两个数据集是SIGHAN于2005年组织的中文分词比赛所用的数据集，也是学术界测试分词工具的标准数据集，本文用于测试各大分词工具的准确性，而最后一个数据集规模较大，用于测试分词速度。</p>
<h1 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h1><p>用SIGHAN Bakeoff 2005比赛中所自带的score脚本、test gold数据和training words数据对4个工具进行准确性测试，具体使用方法可参考：<a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip</a> 中的readme文件。</p>
<h1 id="测试硬件"><a href="#测试硬件" class="headerlink" title="测试硬件"></a>测试硬件</h1><p>Intel Core i7-6700 CPU@3.40GHz*8</p>
<h1 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h1><p>1、MSR测试结果<br><img src="media/1.png" alt="1"></p>
<p>2、PKU测试结果<br><img src="media/2.png" alt="2"></p>
<p>3、人民日报测试结果<br><img src="media/3.png" alt="3"></p>
<h1 id="测试结论"><a href="#测试结论" class="headerlink" title="测试结论"></a>测试结论</h1><p>1、一个好的分词工具不应该只能在一个数据集上得到不错的指标，而应该在各个数据集都有很不错的表现。从这一点来看，thulac和ltp都表现非常不错。</p>
<p>2、因为分词是个基础部件，分词速度对于一个分词工具来说也至关重要。从这一点来看，thulac和jieba表现的不错。</p>
<p>3、大家都知道，基本的分词依赖模型，但真正想用分词工具来解决应用层面上的问题，都需要借助于词库，本文测试的4个工具均支持用户自定义词库。</p>
<p>4、特别需要强调的一点是，哈工大的ltp支持分词模型的在线训练，即在系统自带模型的基础上可以不断地增加训练数据，来得到更加丰富、更加个性化的分词模型。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>争论是一个好的事情，尤其是不同背景的人站在不同的角度对同一个事情进行争论，常常会碰撞出知识的火花，对于这个领域的发展有更好地推动作用。希望类似的争论可以多一些，让刚刚入门的或者准备入门的童鞋可以更加客观地看到一个领域的发展现状，而不是盲目地被一些热门的词蒙蔽双眼，失去判断。对于分词来说，最近几年大热的深度学习模型，并不会比之前传统的crf模型有多大性能上的突破，所以大家应该理性地看待深度学习以及人工智能，捧得越高可能摔得越惨。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1、Zhongguo Li, Maosong Sun. Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics, vol. 35, no. 4, pp. 505-512, 2009.<br>2、Meishan Zhang, Yue Zhang, Guohong Fu. Transition-Based Neural Word Segmentation<br><a href="http://www.aclweb.org/anthology/P/P16/P16-1040.pdf" target="_blank" rel="external">http://www.aclweb.org/anthology/P/P16/P16-1040.pdf</a><br>3、Meishan Zhang, Zhilong Deng，Wanxiang Che, and Ting Liu. Combining Statistical Model and Dictionary for Domain Adaption of Chinese Word Segmentation. Journal of Chinese Information Processing. 2012, 26 (2) : 8-12 (in Chinese)<br>4、Wanxiang Che, Zhenghua Li, and Ting Liu. LTP: A Chinese Language Technology Platform. In Proceedings of the Coling 2010:Demonstrations. 2010.08, pp13-16, Beijing, China.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;分词对于研究和应用中文自然语言处理的童鞋来说，都是一个非常非常基础的部件，分词的质量直接影响到后续词性标注、命名实体识别、句法分析等部件的准
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>本周值得读(2016.11.21-2016.11.25)</title>
    <link href="http://rsarxiv.github.io/2016/11/26/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-11-21-2016-11-25/"/>
    <id>http://rsarxiv.github.io/2016/11/26/本周值得读-2016-11-21-2016-11-25/</id>
    <published>2016-11-27T06:12:34.000Z</published>
    <updated>2016-11-27T06:31:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Generative-Deep-Neural-Networks-for-Dialogue-A-Short-Review"><a href="#Generative-Deep-Neural-Networks-for-Dialogue-A-Short-Review" class="headerlink" title="Generative Deep Neural Networks for Dialogue: A Short Review"></a><a href="http://t.cn/RfX2bms" target="_blank" rel="external">Generative Deep Neural Networks for Dialogue: A Short Review</a></h2><p>【对话系统】本文对seq2seq方法在对话系统中的应用做了一个简短的对比和综述，主要是针对几位作者提出的三种深度学习模型：HRED、VHRED和MrRNN，实验数据用了Ubuntu Dialogue Corpus和Twitter Corpus。不管是用seq2seq生成也好，还是套用模板也罢，对话系统的难点仍是上下文的理解和如何输出一些高质量的对话，有些应用场景对response的要求没那么高，只要可以达到一定实际效果即可，而有的则需要生成更加接近人类的对话。本文适合研究深度seq2seq的童鞋以及想看看各种seq2seq效果如何的童鞋来读。本文总结的三个模型原文链接：</p>
<p>(a) MrRNN: <a href="https://arxiv.org/pdf/1606.00776.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1606.00776.pdf</a><br>(b) VHRED: <a href="https://arxiv.org/pdf/1605.06069v3.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1605.06069v3.pdf</a><br>(c) HRED: <a href="https://arxiv.org/pdf/1507.04808v3.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1507.04808v3.pdf</a></p>
<h2 id="Coherent-Dialogue-with-Attention-based-Language-Models"><a href="#Coherent-Dialogue-with-Attention-based-Language-Models" class="headerlink" title="Coherent Dialogue with Attention-based Language Models"></a><a href="http://t.cn/Rfag1Jx" target="_blank" rel="external">Coherent Dialogue with Attention-based Language Models</a></h2><p>【对话系统】考虑并理解上下文是Chatbot的一大难点，也是目前绝大多数chatbot不智能的主要原因之一。本文提出了一种动态的attention模型，在理解用户请求的时候，动态地考虑历史信息。本文用到了两个开放数据集，分别是MovieTriples和Ubuntu Troubleshoot dataset。建议对chatbot感兴趣的同学可以精读此文。</p>
<h2 id="Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks"><a href="#Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks" class="headerlink" title="Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks"></a><a href="http://t.cn/RfXLHIP" target="_blank" rel="external">Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks</a></h2><p>【课程学习】Curriculum Learning是一类模拟小孩子学习过程的学习算法，简单地说是指在训练模型是从简单的样本开始，逐渐增加学习样本的难度。本文以情感分析为研究对象，对Curriculum Learning如何提升LSTM模型在情感分析任务上的效果进行了实验研究，并给出了可视化的结果。本文适合研究Curriculum Learning的童鞋以及在训练模型中想尝试下Curriculum Learning思路的童鞋研读。</p>
<h2 id="Variable-Computation-in-Recurrent-Neural-Networks"><a href="#Variable-Computation-in-Recurrent-Neural-Networks" class="headerlink" title="Variable Computation in Recurrent Neural Networks"></a><a href="http://t.cn/RfXUmyN" target="_blank" rel="external">Variable Computation in Recurrent Neural Networks</a></h2><p>【RNN研究】RNN在解决序列建模问题有着天然的优势，但有些序列数据存在周期性的变化，或者短时间内变化并不明显，比如视频数据，因此固定不变的RNN训练方案会浪费计算资源，本文针对这一问题，提出了一种RNN变计算训练方案，即在计算下一个time step的hidden state时，不需要上一个time step所有的维度，只取一部分来计算，其他的维度复制过来即可。这篇工作的相关的前人研究包括：2014年的A Clockwork RNN，链接如下：<a href="https://arxiv.org/pdf/1402.3511.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1402.3511.pdf</a></p>
<h2 id="Learning-to-Distill-The-Essence-Vector-Modeling-Framework"><a href="#Learning-to-Distill-The-Essence-Vector-Modeling-Framework" class="headerlink" title="Learning to Distill: The Essence Vector Modeling Framework"></a><a href="http://t.cn/RfoWk0K" target="_blank" rel="external">Learning to Distill: The Essence Vector Modeling Framework</a></h2><p>【表示学习】本文研究的内容包括两个点，一个是无监督学习，一个是文档表示。词表示、句子表示都有比较多的解决方案，但实际应用中文档级别的表示非常重要，比如情感分析、文本摘要等任务。本文提出了一种无监督的方法对文档以及背后所蕴藏的背景知识进行低维表示。自然语言随着元素级别地提升（从字、词、短语、句子到文档），研究的难度随之增加，实用程度随之减少。建议想从无监督学习方法有所突破以及想试试文档表示的童鞋可以来读本文。</p>
<h2 id="Unsupervised-Learning-of-Sentence-Representations-using-Convolutional-Neural-Networks"><a href="#Unsupervised-Learning-of-Sentence-Representations-using-Convolutional-Neural-Networks" class="headerlink" title="Unsupervised Learning of Sentence Representations using Convolutional Neural Networks"></a><a href="http://t.cn/Rf9VNdk" target="_blank" rel="external">Unsupervised Learning of Sentence Representations using Convolutional Neural Networks</a></h2><p>【句子表示】本文的贡献在于提出了一种新的CNN-LSTM auto-encoder，作为一种无监督的句子学习模型。</p>
<h2 id="Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers"><a href="#Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers" class="headerlink" title="Emergent Logical Structure in Vector Representations of Neural Readers"></a><a href="http://t.cn/Rf9Vwi7" target="_blank" rel="external">Emergent Logical Structure in Vector Representations of Neural Readers</a></h2><p>【问答系统】针对最近提出的各种各样的attention based reader models,本文作者做了一个比较全面的总结和分析，并且通过数学分析和实验展示了模型之间的相关性。PaperWeekly第十四期的文章有相关的paper note可以参考<a href="http://rsarxiv.github.io/2016/11/19/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%9C%9F/">地址</a></p>
<h1 id="公益广告"><a href="#公益广告" class="headerlink" title="公益广告"></a>公益广告</h1><p>美国国立卫生研究院招博士后，研究领域包括：NLP、text mining和machine learning，感兴趣的童鞋可以看过来，详情请戳<a href="https://www.stat.washington.edu/jobs/archive/2013/may/05.20.13_NIH_E_B_NLP_Post_Doc_Ad_PDF_May_20_2013.pdf" target="_blank" rel="external">这里</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Generative-Deep-Neural-Networks-for-Dialogue-A-Short-
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十五期</title>
    <link href="http://rsarxiv.github.io/2016/11/26/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%BA%94%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/26/PaperWeekly-第十五期/</id>
    <published>2016-11-26T18:37:08.000Z</published>
    <updated>2016-11-26T18:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>NMT是热门研究领域之一，尤其是Google和百度都推出了自己的NMT翻译系统，在工业界、学术界和翻译界都引起了轩然大波，一时间对NMT技术的研究和讨论达到了顶峰。Attention模型在NLP中最早的使用正是在NMT领域出现的，包括横扫很多领域的seq2seq+attention解决方案，都是在NMT模型的基础上进行相应的一些小改动而成的。所以，本期PaperWeekly带大家看一看最近两年Attention模型在NMT领域中的研究进展，本文包括以下paper：</p>
<p>1、Neural Machine Translation by Jointly Learning to Align and Translate, 2015<br>2、Effective approaches to attention-based neural machine translation, 2015<br>3、Modeling Coverage for Neural Machine Translation,  2016<br>4、Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation, 2016<br>5、Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation, 2016</p>
<h1 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate" class="headerlink" title="Neural Machine Translation by Jointly Learning to Align and Translate"></a><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Jacobs University Bremen, Germany<br><br>Universite ́ de Montre ́al</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>NMT, attention</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2015</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这篇论文首次提出在NMT中使用attention的机制，可以使模型自动确定源句子中和目标词语最相关的部分，相比于基本的encoder-decoder方法提高了翻译效果。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>该论文使用的基本模型是一个双向RNN的encoder-decoder的结构。在这篇论文之前，encoder部分都是直接把输入句子encode成一个固定长度的上下文向量c，然后decoder再根据该向量来产生翻译。但是由于句子长度不定，这种做法对长句子的效果不理想。<br><img src="media/model.png" alt="mode"></p>
<p>上图是这篇论文提出的模型结构，作者首次提出了在decoder中加入一种attention的机制。直观上理解，就是decoder可以决定更多地注意原句子中的某些部分，从而不必把原句子中的所有信息都encode成一个固定的向量。具体来讲，上下文向量ci由下式计算得出：<br><img src="media/ci.png" alt="ci"></p>
<p>其中，<br><img src="media/aij.png" alt="aij"></p>
<p>其中，<br><img src="media/eij.png" alt="eij"></p>
<p>上式中的a便是alignment model，可以用来估计位置j附近的输入和位置i的输出之间的匹配程度。本论文中的alignment model是一个前馈神经网络，它和模型中的其它部分一起进行训练。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>1、英法翻译数据集 <a href="http://www.statmt.org/wmt14/translation-task.html" target="_blank" rel="external">ACL WMT ’14</a></p>
<p>2、一个基本的RNN encoder-decoder模型的实现 <a href="https://github.com/lisa-groundhog/GroundHog." target="_blank" rel="external">GroundHog</a></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、2013年，一个类似的aligning的方法被提出用于手写体生成。论文：Graves(2013) Generating sequences with recurrent neural networks<br>2、2014年，seq2seq的神经网络模型用于机器翻译。论文：Sutskever(2014) Sequence to sequence learning with neural networks</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>本论文创新性地在NMT中提出了attention的机制，可以使模型在每一步注意到源句子中不同的部分，从而提高了NMT的效果，该效果的提升对于长句子的翻译尤其明显。</p>
<h1 id="Effective-approaches-to-attention-based-neural-machine-translation"><a href="#Effective-approaches-to-attention-based-neural-machine-translation" class="headerlink" title="Effective approaches to attention-based neural machine translation"></a><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="external">Effective approaches to attention-based neural machine translation</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Minh-Thang Luong, Hieu Pham, Christopher D. Manning</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>Computer Science Department, Stanford University</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>NMT;Global Attention;Local Attention</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>EMNLP 2015</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>Attention机制引入极大提升了NMT的翻译质量，但对于Attention实现架构的讨论还很少，尤其是全局Attention的计算效率问题。本文就是讨论各种优化策略，包括Global Attention, Local Attention，Input-feeding方法等。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>Global Attenion，生成上下文向量c_t时，考虑原文编码过程中的所有隐状态。<br>  <img src="media/1GlobalAttention.png" alt="1GlobalAttention"></p>
<p>Local Attention，对于每个正在生成的译词，预测一个原文对齐的位置，只考虑该位置前后一个窗口范围内的原文编码隐状态。  </p>
<p><img src="media/2LocalAttention.png" alt="2LocalAttention"></p>
<p>Input-feeding，用一个额外的向量，来记住哪些词是已经翻译过的，即考虑了coverage的问题。  </p>
<p><img src="media/3Input-Feeding.png" alt="3Input-Feeding"></p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、训练数据：WMT14 (4.5M句对，116M 英文词，110M德文词)<br>2、开发集：newstest2013 (3000句)<br>3、测试集：newstest2014(2737句)和newstest2015(2169句)<br>4、代码和模型共享在：<a href="http://nlp.stanford.edu/projects/nmt/" target="_blank" rel="external">http://nlp.stanford.edu/projects/nmt/</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>主要是follow了(Bahdanau et al., 2015; Jean et al., 2015)的工作，对Attention的机制进行了探讨和改进。  </p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>English-German的实验结果，较不用attention的方法提升了5个多点BLEU，充分证明了attention的有效性。<br>实验结果的表格详细列出了各种改进方法带来的收益，跟进者不妨仔细看看（以及第5节的分析），可以很快了解各种折腾的方向。</p>
<p><img src="media/4ExperimentResult.png" alt="4ExperimentResult"></p>
<h2 id="完成人信息"><a href="#完成人信息" class="headerlink" title="完成人信息"></a>完成人信息</h2><p>微博 @MyGod9，语智云帆创始人，机器翻译老兵，NMT追随者，weiyongpeng@lingosail.com </p>
<h1 id="Modeling-Coverage-for-Neural-Machine-Translation"><a href="#Modeling-Coverage-for-Neural-Machine-Translation" class="headerlink" title="Modeling Coverage for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1601.04811v6.pdf" target="_blank" rel="external">Modeling Coverage for Neural Machine Translation</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>诺亚方舟实验室，清华大学</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>NMT</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL2016</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>解决经典神经机器翻译模型中存在的over-translation（过度翻译）和under-translation(翻译不足）的问题。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>在传统NMT模型中，加入统计机器翻译策略中的coverage方法，来追踪、判断原始句子是否被翻译，如下图、公式所示。<br><img src="media/pic1.png" alt="pi"><br><img src="media/pic2.png" alt="pi"><br><img src="media/pic3.png" alt="pi"><br>其中，C为新引入的coverage向量。</p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>前序文章：Neural Machine Translation by Jointly Learning to Align and Translate</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>该文是基于Neural Machine Translation by Jointly Learning to Align and Translate之上的工作，引入了统计机器翻译中的Coverage方法来尝试避免NMT中的一些问题。根据文章的试验结果，这种方法能够提升翻译效果。由于写作此文时笔者未作实验，因此实际效果有待进一步衡量。</p>
<h1 id="Agreement-based-Joint-Training-for-Bidirectional-Attention-based-Neural-Machine-Translation"><a href="#Agreement-based-Joint-Training-for-Bidirectional-Attention-based-Neural-Machine-Translation" class="headerlink" title="Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation"></a><a href="https://arxiv.org/abs/1512.04650" target="_blank" rel="external">Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>Tsinghua University</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Bidirectional NMT; Attention</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>IJCAI 2016</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>由于自然语言错综复杂的结构，单向的注意力模型只能引入注意力机制的部分regulization。文章提出了联合训练双向的注意力模型，尽可能使注意力在两个方向上保持一致。</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p>模型的中心思想就是对于相同的training data，使source-to-target和target-to-source两个模型在alignment matrices上保持一致。这样能够去掉一些注意力噪声，使注意力更加集中、准确。更确切地说，作者引入了一个新的目标函数：</p>
<p><img src="media/1.png" alt="1"></p>
<p>其中<br><img src="media/2.png" alt="2">表示source-to-target基于注意力的翻译模型，而<img src="media/3.png" alt="3">表示target-to-source的模型。<img src="media/4.png" alt="4">表示对于句子s source-to-target的alignment matrix，而<img src="media/5.png" alt="5">表示target-to-source的。<img src="media/6.png" alt="6">是损失函数，可以衡量两个alignment matrix之间的disagree程度。</p>
<p>对于<img src="media/6.png" alt="6">,有几种不同的定义方法：<br>1、Square of addition(SOA)<br><img src="media/7.png" alt="7"></p>
<p>2、Square of subtraction(SOS)<br><img src="media/9.png" alt="9"></p>
<p>3、Multiplication(MUL)<br><img src="media/10.png" alt="10"></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p>作者文中说的是bidirectional translation的alignment matrices要一致；还有另外一篇文章“Agreement on Target-bidirectional Neural Machine Translation”是说decoding的时候可以正向或者反向产生目标句子，把这二者进行联合训练。另外，最近也有很多关于bidirectional training或者类似思想的文章，比如“Dual Learning for Machine Translation. Computation and Language”将reinforcement的概念引入了bidirectional training当中，“Neural Machine Translation with Reconstruction” 希望能从target hidden state恢复出source sentence</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>这篇文章胜在idea,很巧妙地想到了让正反向的注意力一致来改进attention。</p>
<h1 id="Improving-Attention-Modeling-with-Implicit-Distortion-and-Fertility-for-Machine-Translation"><a href="#Improving-Attention-Modeling-with-Implicit-Distortion-and-Fertility-for-Machine-Translation" class="headerlink" title="Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation"></a><a href="https://arxiv.org/abs/1601.03317" target="_blank" rel="external">Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation</a></h1><h2 id="作者-4"><a href="#作者-4" class="headerlink" title="作者"></a>作者</h2><p>Shi Feng, Shujie Liu, Nan Yang, Mu Li, Ming Zhou, Kenny Q.Zhu</p>
<h2 id="单位-4"><a href="#单位-4" class="headerlink" title="单位"></a>单位</h2><p>Shanghai Jiao Tong University, Microsoft Research</p>
<h2 id="关键词-4"><a href="#关键词-4" class="headerlink" title="关键词"></a>关键词</h2><p>NMT, Attention, Fertility, Distortion</p>
<h2 id="文章来源-4"><a href="#文章来源-4" class="headerlink" title="文章来源"></a>文章来源</h2><p>COLING 2016</p>
<h2 id="问题-4"><a href="#问题-4" class="headerlink" title="问题"></a>问题</h2><p>使用attention机制解决NMT中调序和繁衍率的问题。</p>
<h2 id="模型-4"><a href="#模型-4" class="headerlink" title="模型"></a>模型</h2><p>模型非常简单，即在attention机制中将前一时刻的context vector c作为输入传入当前时刻attention中（命名为RecAtt）。如图：</p>
<p><img src="media/coling.jpg" alt="coling"></p>
<p>通过这样的RecAtt机制，attention部分的网络相当于记忆了之前时刻的context。</p>
<h2 id="相关工作-4"><a href="#相关工作-4" class="headerlink" title="相关工作"></a>相关工作</h2><p>ACL 2016李航老师组的工作 Modeling Coverage for Neural Machine Translation利用了attention机制来解决了NMT中“欠翻译”和“过翻译”的问题。</p>
<h2 id="简评-4"><a href="#简评-4" class="headerlink" title="简评"></a>简评</h2><p>该文章的创新之处在于提出将attention计算得到的context vector c作为attention的输入，这样就是的attention机制带有一种recurrent的意味。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本期PaperWeekly精选了5篇Attention模型在NMT任务上的研究工作，Attention模型的发展不仅仅推动着NMT的进步，同时也可以借鉴于其他的任务中，比如QA，比如chatbot。感谢@MyGod9 @雨神 @susie-nmt @李争 @magic282 五位童鞋的辛勤付出。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;NMT是热门研究领域之一，尤其是Google和百度都推出了自己的NMT翻译系统，在工业界、学术界和翻译界都引起了轩然大波，一时间对NMT技术
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>悉尼科技大学博士后招聘信息</title>
    <link href="http://rsarxiv.github.io/2016/11/19/%E6%82%89%E5%B0%BC%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E5%90%8E%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/"/>
    <id>http://rsarxiv.github.io/2016/11/19/悉尼科技大学博士后招聘信息/</id>
    <published>2016-11-20T04:47:55.000Z</published>
    <updated>2016-11-20T04:49:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, I am recruiting a Postdoc in Machine Learning for two (2) years. In brief, the candidate should:</p>
<p>1 Hold a PhD in machine learning and have a good research track record.</p>
<p>2 Have a genuine interest in mathematical modeling.</p>
<p>3 Good communication skills and is willing to help PhD students resolving their mathematical issues.</p>
<p>4 Excellent in programming and experimentation.</p>
<p>The candidate will work with Dr Richard Xu (Yida.Xu@uts.edu.au), where his team’s recent research themes include: Bayesian Non-Parametric (BNP), Monte-Carlo inference, Matrix (and Tensor) factorization and Deep Learning. The application areas include both computer vision and document. There is no strict requirement that the candidate must align his/her research exactly to Richard’s existing work, i.e., the candidate can continue to work in his/her established field as long as the group benefit from his/her presence.</p>
<p>Please contact Richard to obtain further information, and remember to check out his website:</p>
<p><a href="http://www-staff.it.uts.edu.au/~ydxu/" target="_blank" rel="external">http://www-staff.it.uts.edu.au/~ydxu/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi, I am recruiting a Postdoc in Machine Learning for two (2) years. In brief, the candidate should:&lt;/p&gt;
&lt;p&gt;1 Hold a PhD in machine learn
    
    </summary>
    
    
      <category term="招聘" scheme="http://rsarxiv.github.io/tags/%E6%8B%9B%E8%81%98/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.11.14-2016.11.18</title>
    <link href="http://rsarxiv.github.io/2016/11/19/cs-CL-weekly-2016-11-14-2016-11-18/"/>
    <id>http://rsarxiv.github.io/2016/11/19/cs-CL-weekly-2016-11-14-2016-11-18/</id>
    <published>2016-11-20T04:30:55.000Z</published>
    <updated>2016-11-20T04:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="UTCNN-a-Deep-Learning-Model-of-Stance-Classificationon-on-Social-Media-Text"><a href="#UTCNN-a-Deep-Learning-Model-of-Stance-Classificationon-on-Social-Media-Text" class="headerlink" title="UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text"></a><a href="http://t.cn/RfGR4GE" target="_blank" rel="external">UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text</a></h2><p>【文本分类】【社交网络】社交网络中蕴藏着大量的非结构化的文本，对社交网络的挖掘也是一块重要的研究内容。本文研究内容为立场分类，创新之处在于做分类时不仅仅考虑该文本信息本身，而且考虑了与该文本相关的评论、反馈、用户信息、话题等各种文本信息。模型部分没有太多的新意，是经典的CNN。本文适合做社交网络文本挖掘的童鞋来读。</p>
<h2 id="A-Way-out-of-the-Odyssey-Analyzing-and-Combining-Recent-Insights-for-LSTMs"><a href="#A-Way-out-of-the-Odyssey-Analyzing-and-Combining-Recent-Insights-for-LSTMs" class="headerlink" title="A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs"></a><a href="http://t.cn/RfVSmk7" target="_blank" rel="external">A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs</a></h2><p>【文本分类】RNN及其扩展LSTM在文本分类中得到了广泛的应用，本文探究了多种LSTM的小变种对分类结果的影响，一些小改变对结果还是有一定影响的。本文适合工程上用LSTM解决文本分类问题的童鞋精读。</p>
<h2 id="Linguistically-Regularized-LSTMs-for-Sentiment-Classification"><a href="#Linguistically-Regularized-LSTMs-for-Sentiment-Classification" class="headerlink" title="Linguistically Regularized LSTMs for Sentiment Classification"></a><a href="http://t.cn/Rf56bpv" target="_blank" rel="external">Linguistically Regularized LSTMs for Sentiment Classification</a></h2><p>【情感分析】本文最大的亮点在于将语言学资源，比如情感词典，否定词，表示程度的词等等以约束条件的形式融入到了现有的句子级别的LSTM分类模型中，取得了不错的效果。深度学习火起来之后，大家都推崇数据驱动的模型，希望找到一种简单粗糙的解决方案，而忽视了经典的自然语言资源和语言学的知识。经典的这些资源都是非常宝贵的东西，如何将这些知识融入到现有的深度学习模型中，是个很难但却非常有意义的事情，本文在句子级别的情感分类任务中做了相关的探索。推荐研究情感分析的童鞋精读。</p>
<h2 id="Google’s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation"><a href="#Google’s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation" class="headerlink" title="Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"></a><a href="http://t.cn/Rf5I4nw" target="_blank" rel="external">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a></h2><p>【机器翻译】Google NMT系统支持Zero-Shot翻译，从训练集充足的A-&gt;B，B-&gt;C两个翻译模型中可以推出一个质量不错的A-&gt;C模型，A和C并不需要很充足的训练集。</p>
<h2 id="Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot"><a href="#Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot" class="headerlink" title="Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot"></a><a href="http://t.cn/Rf5IB9P" target="_blank" rel="external">Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot</a></h2><p>【多模态】【机器翻译】现在的人工智能还达不到很高的智能水平，但是处理或者理解一些稍微初级的东西可能还行，比如3岁孩子的故事之类的。之前萌生一个想法，能不能针对小孩学习，比如学习英语，传统的方法可能是给一句中文，教一句英语，感觉用到的信息量还是少，能不能一边看图，一边学习英语，相当于用到了图像这个信息，孩子在学习的过程中会多一些信息维度，学习效果可能会更好一些。本文正是做了这么一件事情，借助图像作为机器翻译的桥梁。</p>
<h2 id="Neural-Machine-Translation-with-Pivot-Languages"><a href="#Neural-Machine-Translation-with-Pivot-Languages" class="headerlink" title="Neural Machine Translation with Pivot Languages"></a><a href="http://t.cn/RftFqja" target="_blank" rel="external">Neural Machine Translation with Pivot Languages</a></h2><p>【机器翻译】很多语言的机器翻译都面临着一个语言对数据集匮乏的问题，一个比较直观的思路是，用一种常见的语言作为“桥梁”，连接起两种语言对数据集匮乏的语言。昨天Google的zero-shot也正是这么一种思路，本文也在这方面进行了研究工作，即想做A-&gt;C的翻译，需要拿一个热门语言B作为桥梁，构造一个A-&gt;B,B-&gt;C的联合训练模型，本文用英语作为B，用德语、法语、西班牙语分别作为A和C进行了两组实验，验证了模型的有效性。这种“串行”seq2seq的思路，其实可以尝试一些其他的任务，做一些seq2seq2seq…的模型出来。</p>
<h2 id="Joint-Representation-Learning-of-Text-and-Knowledge-for-Knowledge-Graph-Completion"><a href="#Joint-Representation-Learning-of-Text-and-Knowledge-for-Knowledge-Graph-Completion" class="headerlink" title="Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion"></a><a href="http://t.cn/Rf5J12U" target="_blank" rel="external">Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion</a></h2><p>【知识表示】“联合学习”是个热门词，“联合学习”可以避免一些语言分析过程（比如：句法依存分析）带来的误差。本文在学习词、实体和关系表示时同时用到了text和知识图谱信息，得到了不错的效果。</p>
<h2 id="Multi-lingual-Knowledge-Graph-Embeddings-for-Cross-lingual-Knowledge-Alignment"><a href="#Multi-lingual-Knowledge-Graph-Embeddings-for-Cross-lingual-Knowledge-Alignment" class="headerlink" title="Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment"></a><a href="http://t.cn/Rf5XVuD" target="_blank" rel="external">Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment</a></h2><p>【知识图谱】一个新模型来填坑，Trans系列的新成员——MTransE </p>
<h2 id="End-to-End-Neural-Sentence-Ordering-Using-Pointer-Network"><a href="#End-to-End-Neural-Sentence-Ordering-Using-Pointer-Network" class="headerlink" title="End-to-End Neural Sentence Ordering Using Pointer Network"></a><a href="http://t.cn/RftkYoF" target="_blank" rel="external">End-to-End Neural Sentence Ordering Using Pointer Network</a></h2><p>【句子排序】【文本摘要】句子排序是一项重要的基本工作，尤其是在做单文档和多文档文本抽取式摘要时显得特别重要。经典的排序方法几乎都是考虑单个句子所包含的信息进行排序，忽略了句子的上下文信息。本文工作借鉴了Pointer Network的思路，提出了一种端到端排序模型，在排序时考虑句子的上下文。通过两组实验验证了本文算法的有效性。机器翻译中的seq2seq+attention已经成功的应用在了很多任务上，但针对具体任务不同的特点进行针对性地修正会带来比较理想的结果。建议研究文本摘要的童鞋读本文。</p>
<h2 id="The-Amazing-Mysteries-of-the-Gutter-Drawing-Inferences-Between-Panels-in-Comic-Book-Narratives"><a href="#The-Amazing-Mysteries-of-the-Gutter-Drawing-Inferences-Between-Panels-in-Comic-Book-Narratives" class="headerlink" title="The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives"></a><a href="http://t.cn/RfVoHOF" target="_blank" rel="external">The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives</a></h2><p>【问答系统】基于上下文的问答已经有很多数据集了，基于图像的问答也有一些数据集了。漫画是一类大家小时候都喜欢的读物，包含了丰富的图像和文本数据（对话）。本文给出了一个大型数据集，包括了丰富的图像和文本，规模在120万（120GB）左右。数据给出了几个任务，基于图像的问答任务，基于对话文本的问答任务和文本排序任务。对问答感兴趣，想找一些新数据来刷一刷榜的童鞋可以看过来。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="Highlights-of-EMNLP-2016-Dialogue-deep-learning-and-more"><a href="#Highlights-of-EMNLP-2016-Dialogue-deep-learning-and-more" class="headerlink" title="Highlights of EMNLP 2016: Dialogue, deep learning, and more"></a><a href="http://blog.aylien.com/highlights-emnlp-2016-dialogue-deeplearning-and-more/" target="_blank" rel="external">Highlights of EMNLP 2016: Dialogue, deep learning, and more</a></h2><p>NLP技术服务公司Aylien写的EMNLP 2016总结</p>
<h1 id="一句话公益广告"><a href="#一句话公益广告" class="headerlink" title="一句话公益广告"></a>一句话公益广告</h1><p>悉尼科技大学Dr Richard Xu招机器学习博士后，感兴趣的童鞋看过来。具体信息请点阅读原文<a href="http://rsarxiv.github.io/2016/11/19/%E6%82%89%E5%B0%BC%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E5%90%8E%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/">查看链接</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;UTCNN-a-Deep-Learning-Model-of-Stance-Classificationo
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十四期</title>
    <link href="http://rsarxiv.github.io/2016/11/19/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/19/PaperWeekly-第十四期/</id>
    <published>2016-11-19T17:58:02.000Z</published>
    <updated>2016-11-19T18:34:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>PaperWeekly已经介绍过不少Question Answering的相关工作。主要有DeepMind Attentive Reader，FAIR Memory Networks，Danqi’s Stanford Reader, Attention Sum Reader, Gated Attention Sum Reader, Attention Over Attention Reader, etc. 这些模型关联性很大，或多或少存在相似之处。本文给大家介绍一下Toyota Technological Institute at Chicago (TTIC)在Question Answering方面的相关工作，共有3篇paper：</p>
<p>1、Who did What: A Large-Scale Person-Centered Cloze Dataset, 2016<br>2、Broad Context Language Modeling as Reading Comprehension, 2016<br>3、Emergent Logical Structure in Vector Representations of Neural Readers, 2016</p>
<h1 id="Who-did-What-A-Large-Scale-Person-Centered-Cloze-Dataset"><a href="#Who-did-What-A-Large-Scale-Person-Centered-Cloze-Dataset" class="headerlink" title="Who did What: A Large-Scale Person-Centered Cloze Dataset"></a><a href="https://tticnlp.github.io/who_did_what/" target="_blank" rel="external">Who did What: A Large-Scale Person-Centered Cloze Dataset</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David McAllester</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>EMNLP 2016</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>文章构建了一个新的Question Answering dataset，”Who did What”。</p>
<p>sample instance如下图所示。<br><img src="media/example.png" alt="example"></p>
<p>问题的句子总是挖掉了一些named entities，然后给出在文中出现过的别的named entities作为选项。这一个dataset的难度要高于之前的CNN/DM dataset，可以作为创建新模型的参考数据集。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>构建此数据集的方法与CNN/DM不同，问题并不是context passge的一个summary。问题与context均来自Gigaword Corpus，他们是两篇非常相关的文章。</p>
<p>具体来说，我们先找到一篇文章，作为question文章。然后提取出文中第一句话的named entities，删除其中的一个named entity作为将要被预测的答案。然后利用这一句question sentence，我们可以利用一些Information Retrieval系统从Gigaword Corpus找到一篇相关的文章作为passage。这篇文章与question文章不同，但是包含着与question sentence非常类似的信息。</p>
<p>有了passage之后，我们再从passage中找出named entities作为candidate answers。</p>
<p>为了使任务难度更大，我们用一些简单的baseline (First person in passage, etc) 将一些很容易做出的问题删掉，只留下比较困难的instances。这样构建的数据比CNN/DM会困难不少。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>相信作者创建的新数据集会给Machine comprehension带来一些新的问题与挑战，是很有价值的资源。文章采用的baseline suppresion方法可以用比较小的代价加大问题的难度，值得参考。</p>
<h1 id="Broad-Context-Language-Modeling-as-Reading-Comprehension"><a href="#Broad-Context-Language-Modeling-as-Reading-Comprehension" class="headerlink" title="Broad Context Language Modeling as Reading Comprehension"></a><a href="https://arxiv.org/abs/1610.08431" target="_blank" rel="external">Broad Context Language Modeling as Reading Comprehension</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>不久前发布的<a href="https://arxiv.org/abs/1606.06031" target="_blank" rel="external">LAMBADA dataset</a>中，作者尝试的各种baseline models都给出了比较差的结果。</p>
<p>每一个LAMBADA instance如下图所示。</p>
<p><img src="media/LAMBADA.png" alt="LAMBADA"></p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>在观察了LAMBADA dataset之后，我们认为可以利用Reading comprehension models来提升准确率，而不必使用传统的language model。</p>
<p>由于state of the art reading comprehension models需要给出candidate answers，然后从中选出一个作为预测的答案，我们就将所有在context中出现过的单词都作为一个candidate answer。</p>
<p>LAMBADA给出的训练集是一些小说的文本。为了使训练集与测试集的数据类型保持一致，我们构建了一个biased training set。具体的做法是，我们将training set划分成4-5句话的context，然后保证target word在context passage中出现，只保留这样的训练数据。我们在新构建的training set上训练各种attention based models,得到了比原作者好得多的测试结果。</p>
<p><img src="media/zewei-results.png" alt="zewei-results"></p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>这篇文章中，作者利用了简单的方法和模型将LAMBADA dataset的准确率从7.3%提高到45.4%，非常简单有效。</p>
<h1 id="Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers"><a href="#Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers" class="headerlink" title="Emergent Logical Structure in Vector Representations of Neural Readers"></a><a href="http://openreview.net/pdf?id=ryWKREqxx" target="_blank" rel="external">Emergent Logical Structure in Vector Representations of Neural Readers</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ICLR 2017 Submission</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>最近提出的各种各样的attention based reader models,本文作者做了一个比较全面的总结和分析，并且通过数学分析和实验展示了模型之间的相关性。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>本文作者认为，当前的attention based models可以分为两类，aggregation readers(包括attentive readers和stanford readers)以及explicit reference readers(包括attention sum reader和gated attention sum reader)。</p>
<p>这两种reader可以用如下的公式联系在一起。</p>
<p><img src="media/formula1.png" alt="formula1"></p>
<p>要满足上述等式，只需要满足下面的公式。</p>
<p><img src="media/formula2.png" alt="formula2"></p>
<p>也就是说，只有正确答案所在的hidden vector和question vector得到的inner product才能给出不为零的常数。以下实验结论支持了这一假设。</p>
<p><img src="media/experiment.png" alt="experiment"></p>
<p>由于CNN/DM在训练和测试中经过了anonymization，作者认为此inner product其实可以分为两部分，一部分与anonymized token ID有关，另一部分与ID无关。与ID相关的那一部分在inner product应该直接给出0的答案。如下述公式所示。</p>
<p><img src="media/formula3.png" alt="formula3"></p>
<p>本文的另一部分工作是在attention readers上加入一些linguistic features提升各个数据集的准确读，这里不仔细描述。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文是对于各个attetion based neural reader models很好的总结，它很好地连接了各个不同的model，说明了为何看似不同的model能够给出非常类似的结果。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>问答系统是一类大的问题，也是目前NLP应用的研究热点之一。本文作者介绍了TTIC在QA研究中的一些成果，其中第二篇是本文作者近期的paper。感谢来自芝加哥大学的@ZeweiChu童鞋辛勤的劳动。</p>
<h1 id="公益广告"><a href="#公益广告" class="headerlink" title="公益广告"></a>公益广告</h1><p>清华大学计算机系自然语言处理实验室招聘博士后</p>
<h2 id="将从事的研究方向"><a href="#将从事的研究方向" class="headerlink" title="将从事的研究方向"></a>将从事的研究方向</h2><p>围绕自然语言处理、语义分析、统计机器翻译或社会计算开展深入的研究工作。实验室具体信息见：<a href="http://nlp.csai.tsinghua.edu.cn" target="_blank" rel="external">http://nlp.csai.tsinghua.edu.cn</a></p>
<h2 id="应聘条件"><a href="#应聘条件" class="headerlink" title="应聘条件"></a>应聘条件</h2><p>1、具有计算机科学技术或相关学科博士学位（博士毕业两年内）；<br>2、熟悉自然语言处理或机器学习的基本理论、模型与算法，曾在国内外重要学术刊物或重要国际会议（CCF A类）上发表（含已录用）高水平学术论文；<br>3、在句法分析、语义分析方面有较好研究基础者优先；<br>4、具有较强的编程能力及项目研发能力；<br>5、责任心强，具有较好的团队合作精神和创新意识，英语阅读及写作能力较强；<br>6、符合清华大学博士后招收条件。</p>
<h2 id="工资待遇"><a href="#工资待遇" class="headerlink" title="工资待遇"></a>工资待遇</h2><p>享受清华大学博士后待遇及课题组津贴。全力支持申请国家自然科学基金、全国博士后管委会、北京市、清华大学的相关研究计划。</p>
<h2 id="申请材料"><a href="#申请材料" class="headerlink" title="申请材料"></a>申请材料</h2><p>1、个人简历、学位证书及成绩单复印件；<br>2、最具代表性的论文2篇；<br>3、博士后期间研究设想（简明扼要）；<br>4、其它任何支持材料。</p>
<h2 id="导师及联系方式"><a href="#导师及联系方式" class="headerlink" title="导师及联系方式"></a>导师及联系方式</h2><p>合作导师：孙茂松教授、刘知远助理教授<br>联系人：刘知远<br>电子邮件： liuzy@tsinghua.edu.cn</p>
<p>有意者请将申请材料发至电子邮箱，请在邮件主题中注明姓名和“申请博士后”。材料通过初选者进行面谈（面谈时间另行通知），然后走清华大学博士后申请程序。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;PaperWeekly已经介绍过不少Question Answering的相关工作。主要有DeepMind Attentive Reader
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.11.07-2016.11.11</title>
    <link href="http://rsarxiv.github.io/2016/11/13/cs-CL-weekly-2016-11-07-2016-11-11/"/>
    <id>http://rsarxiv.github.io/2016/11/13/cs-CL-weekly-2016-11-07-2016-11-11/</id>
    <published>2016-11-13T19:10:19.000Z</published>
    <updated>2016-11-13T19:23:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Learning-Recurrent-Span-Representations-for-Extractive-Question-Answering"><a href="#Learning-Recurrent-Span-Representations-for-Extractive-Question-Answering" class="headerlink" title="Learning Recurrent Span Representations for Extractive Question Answering"></a><a href="https://arxiv.org/pdf/1611.01436v1.pdf" target="_blank" rel="external">Learning Recurrent Span Representations for Extractive Question Answering</a></h2><p>【机器阅读】不同的阅读理解数据集产生答案的方式不同，有的是给定N个候选答案，有的是规定从原文中的entity中进行选择，有的是从原文中的任意token进行选择等等。本文所用的数据集是SQuAD，候选答案是原文中的任意字符串，难度较大，答案可能是一个词或者几个词都有可能。本文在前人研究的基础上提出了一种显式表示answer span的模型，取得了不错的效果。</p>
<h2 id="Answering-Complicated-Question-Intents-Expressed-in-Decomposed-Question-Sequences"><a href="#Answering-Complicated-Question-Intents-Expressed-in-Decomposed-Question-Sequences" class="headerlink" title="Answering Complicated Question Intents Expressed in Decomposed Question Sequences"></a><a href="https://arxiv.org/pdf/1611.01242v1.pdf" target="_blank" rel="external">Answering Complicated Question Intents Expressed in Decomposed Question Sequences</a></h2><p>【复杂问答】基于语义分析的问答系统最近流行于解决长、难问题，本文研究的内容是如何处理多个相互关联的简单问题？（即将复杂问题分解成多个相关简答问题）并给出了一个任务数据集。这个问题的一大难点在于相互关联的问题需要共指消解的工作。本文将单轮问答对话分解成多轮问题过程，上下文的处理非常重要。建议研究聊天机器人的童鞋来精读此文。</p>
<h2 id="Unsupervised-Pretraining-for-Sequence-to-Sequence-Learning"><a href="#Unsupervised-Pretraining-for-Sequence-to-Sequence-Learning" class="headerlink" title="Unsupervised Pretraining for Sequence to Sequence Learning"></a><a href="https://arxiv.org/pdf/1611.02683v1.pdf" target="_blank" rel="external">Unsupervised Pretraining for Sequence to Sequence Learning</a></h2><p>【seq2seq】【文本摘要】seq2seq是一种效果非常不错的框架，尤其是输入-输出数据非常充分的时候。但很多语言翻译问题并不能拿到非常多的训练数据，效果就会打折扣。本文针对这一问题，在原有seq2seq框架上提出了一种小改动。 encoder和decoder的初始值用训练好的语言模型来赋值，用可以获得的少量训练数据对模型进行训练和调优，本文方法的效果在机器翻译任务和abstractive式摘要任务中得到了验证。从本文中也可以看出，一个好的初值不仅可以使得训练更快，而且可以得到更好的结果。本文适合用seq2seq解决工程问题的童鞋读。</p>
<h2 id="Sentence-Ordering-using-Recurrent-Neural-Networks"><a href="#Sentence-Ordering-using-Recurrent-Neural-Networks" class="headerlink" title="Sentence Ordering using Recurrent Neural Networks"></a><a href="https://arxiv.org/pdf/1611.02654v1.pdf" target="_blank" rel="external">Sentence Ordering using Recurrent Neural Networks</a></h2><p>【句子排序】【文本摘要】句子排序任务对于研究文档的连贯性非常有意义，而连贯性对于很多任务非常重要，比如文本摘要。本文在这个任务上用了流行的seq2seq方法，并且给出了一种可视化的句子表示效果。建议研究摘要的童鞋读。</p>
<h2 id="Modeling-Coverage-for-Neural-Machine-Translation"><a href="#Modeling-Coverage-for-Neural-Machine-Translation" class="headerlink" title="Modeling Coverage for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1601.04811v6.pdf" target="_blank" rel="external">Modeling Coverage for Neural Machine Translation</a></h2><p>【机器翻译】针对神经网络机器翻译（NMT）译文中经常出现的遗漏翻译（under-translation）和过度翻译（over-translation）问题，华为诺亚方舟实验室首次提出对覆盖率（coverage）进行建模。该方法的主要思想是为每个源端词维护一个coverage vector以表示该词被翻译（或覆盖）的程度。在解码过程中该覆盖率信息会传入attention model，以使它更关注于未被翻译的源端词，实验表示该方法能显著减少遗漏翻译和过度翻译错误数量，该工作发表在ACL 2016上。</p>
<h2 id="Efficient-Summarization-with-Read-Again-and-Copy-Mechanism"><a href="#Efficient-Summarization-with-Read-Again-and-Copy-Mechanism" class="headerlink" title="Efficient Summarization with Read-Again and Copy Mechanism"></a><a href="https://arxiv.org/pdf/1611.03382.pdf" target="_blank" rel="external">Efficient Summarization with Read-Again and Copy Mechanism</a></h2><p>【文本摘要】本文适合研究文本摘要，尤其是用seq2seq来解决句子级摘要的童鞋进行研读。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="刘知远老师在将门的talk"><a href="#刘知远老师在将门的talk" class="headerlink" title="刘知远老师在将门的talk"></a><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/index_cn.html" target="_blank" rel="external">刘知远老师在将门的talk</a></h2><p>对“表示学习和知识获取”感兴趣的童鞋可以看过来。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Learning-Recurrent-Span-Representations-for-Extractiv
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十三期</title>
    <link href="http://rsarxiv.github.io/2016/11/10/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B8%89%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/10/PaperWeekly-第十三期/</id>
    <published>2016-11-11T02:50:42.000Z</published>
    <updated>2016-11-11T04:21:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本期的PaperWeekly一共分享四篇最近arXiv上放出的高质量paper，包括：机器翻译、表示学习、推荐系统和聊天机器人。人工智能及其相关研究日新月异，本文将带着大家了解一下以上四个研究方向都有哪些最新进展。四篇paper分别是：</p>
<p>1、A General Framework for Content-enhanced Network Representation Learning, 2016.10</p>
<p>2、Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks, 2016.11</p>
<p>3、Dual Learning for Machine Translation, 2016.11</p>
<p>4、Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems, 2016.10</p>
<h1 id="A-General-Framework-for-Content-enhanced-Network-Representation-Learning"><a href="#A-General-Framework-for-Content-enhanced-Network-Representation-Learning" class="headerlink" title="A General Framework for Content-enhanced Network Representation Learning"></a><a href="https://arxiv.org/abs/1610.02906" target="_blank" rel="external">A General Framework for Content-enhanced Network Representation Learning</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Xiaofei Sun, Jiang Guo, Xiao Ding and Ting Liu</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>network representation, content-enhanced</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>同时利用网络结构特征和文本特征来学习网络中节点的embedding</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>总的来说这篇paper的思路比较清晰，学习的方法上很大程度上参考了word2vec的方法。对于一个节点v，将与v相连的节点当做正例，不想连的节点当做负例。那么如何融入内容呢？在网络中设置虚拟的内容节点c，将描述v节点的文本内容c_v当做正例，其他的当做负例c_v’。在优化时同时考虑网络相似性和文本相似性，让v的向量靠近正例远离负例。</p>
<p><img src="media/network-illustration.png" alt="network-illustration"></p>
<p>总的优化函数如下所示，由两个部分L_nn(节点与节点连接)和L_nc(节点与内容连接)线性组合而成，alpha越大则考虑网络结构越多文本内容越少。</p>
<p><img src="media/joint-learning.png" alt="joint-learning"></p>
<p>L_nn和L_nc大体思想如上面所言，两者损失函数一致，尽量接近正例远离反例。但是两者在描述节点概率（相似度）上会有所不同。</p>
<p><img src="media/node-node-link.png" alt="node-node-link"></p>
<p>对于节点与节点之间的概率，由于网络结构要考虑有向性，因此将节点的embedding切分成in和out两半，用sigmoid算两个节点的相似度。</p>
<p><img src="media/node-node-probability.png" alt="node-node-probability"></p>
<p>节点与内容的概率也是类似，不过内容节点的embedding是固定的，通过额外的文本模型训练出来的。这里尝试的文本model包括word2vec，RNN和BiRNN。</p>
<p><img src="media/node-content-probability.png" alt="node-content-probability"></p>
<p>最后在节点分类任务上进行了评测，同时结合网络结构特征和文本特征确实带来了明显的提高。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>用到的数据集是DBLP（cn.aminer.org/citation）和自己采集的知乎用户网络。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>这两年network representation的工作如雨后春笋，在DeepWalk之后有十余篇论文出现。这篇文章在相关工作里有相对全面的覆盖，对这方面工作有兴趣的同学值得参考。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>尽管相关模型层出迭见，但略感遗憾的是感觉目前并没有在network embedding之上的较为成功的应用，大多benchmark都是节点分类和链接预测，应用价值有限。十分期待一些更为新颖的benchmark的出现。</p>
<h1 id="Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks"><a href="#Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks" class="headerlink" title="Collaborative Recurrent Autoencoder Recommend while Learning to Fill in the Blanks"></a><a href="https://arxiv.org/pdf/1611.00454v1.pdf" target="_blank" rel="external">Collaborative Recurrent Autoencoder Recommend while Learning to Fill in the Blanks</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Hao Wang, Xingjian Shi, Dit-Yan Yeung</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>HKUST</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>Recommendation, Collaborative Filtering, RNN</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>Arxiv, to appear at NIPS’16</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>本文的主要贡献是提出collaborative recurrent autoencoder (CRAE)，将CF (collaborative filtering)跟RNN结合在一起，提高推荐的准确率，并且可以用于sequence generation task。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>传统的LSTM模型没有考虑进噪声，对不足的训练数据稳定性不好，文章提出RRN (robust recurrent networks)，为RNN的加噪版本，RRN中的噪声直接在网络中向前或者向后传播，不需要分开的网络来估计latent variables的分布，更容易实现且效率高。CARE的模型如下图所示，序列处理的信息保存在cell state s_t和输出状态h_t中，两个RRN可以组合形成编码译码结构。</p>
<p><img src="media/collaborative1.png" alt="collaborative1"></p>
<p>Wildcard denoising的目的是缓解overfitting，做法是随机选择一些词，替换成<wildcard>，而不是直接扔掉词，实验验证准确率会提成20%左右。Beta-pooling的目的是将向量序列pool成固定长度为2K_W的单向量，帮助rating matrix的矩阵分解；因为不同序列可能需要不同大小的权重，所以需要变长的beta向量来帮助pooling，文章采用beta分布。</wildcard></p>
<p>Learning的过程采用MAP，类似于CDL和DTR。学到矩阵U和V之后，我们可以预计评分矩阵R。</p>
<h2 id="资源-1"><a href="#资源-1" class="headerlink" title="资源"></a>资源</h2><p>1、<a href="http://www.citeulike.org/faq/data.adp" target="_blank" rel="external">CiteULike</a><br>2、<a href="http://www.wanghao.in/" target="_blank" rel="external">Netflix</a></p>
<h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><p>选取当中两个比较有意思的work。<br>1、CTR (collaborative topic reguression)<br>将topic model和probabilistic matrix factorization (PMF)，但是CTR采用bag-of-words的表示形式，忽略了词序和每个词的局部语境，而这些对文章表示和word embeddings能提供有价值的信息。<br>2、CDL (collaborative deep learning)<br>将CF和probabilistic stacked denoising autoencoder (SDAE)结合起来，是一个以bag-of-words为输入的feedforward模型，并不能解决sequence generation的问题。</p>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>这篇文章将RNN用于recommendation，并且与rating matrix结合起来，比较有意思，而且考虑了数据稀疏的情况，pooling的方法也值得借鉴。</p>
<h1 id="Dual-Learning-for-Machine-Translation"><a href="#Dual-Learning-for-Machine-Translation" class="headerlink" title="Dual Learning for Machine Translation"></a><a href="https://arxiv.org/pdf/1611.00179.pdf" target="_blank" rel="external">Dual Learning for Machine Translation</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Yingce Xia1, Di He, Tao Qin, Liwei Wang, Nenghai Yu1, Tie-Yan Liu, Wei-Ying Ma</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>1.University of Science and Technology of China<br>2.Key Laboratory of Machine Perception (MOE), School of EECS, Peking University<br>3.Microsoft Research</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Dual Learning, Machine Translation, Deep Reinforcement Learning</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv, 1 Nov 2016 </p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>文章针对机器翻译时需要的人工标注的双语平行语料获取代价高的问题，提出了Dual Learning Model使用单语语料来进行训练，取得了比使用双语平行语料训练的模型更好的结果。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>模型的核心思想见下图：<br><img src="media/Dual_Learning.png" alt="Dual_Learning"></p>
<p>(注:上图来自CCL2016马维英老师PPT)<br>对上图的详细解释：<br>模型中有两个Agent，Agengt_A和Agent_B,Agent_A只能够理解A语言，Agent_B只能理解B语言，model f是将A语言翻译成B语言的翻译模型，model f是将B语言翻译成A语言的翻译模<br>型。上图的执行过程可以按照下面的解释进行：<br>1、Agent_A 发送一句A语言的自然语言的话X1<br>2、model f将X转换成为B语言的自然语言Y<br>3、Agent_B收到Y，并将Y 传送给model g<br>4、model g将Y转换成源语言A的自然语言X2<br>5、比较X1和X2的差异性，并给出反馈.并进行1到4的反复训练</p>
<p>模型的算法过程：<br><img src="media/Dual_Learning_Algorithm.png" alt="Dual_Learning_Algorith"></p>
<p>在step8的时候对翻译模型翻译的结果使用语言模型做了一个判定，判定一个句子在多大程度上是自然语言。step9是给communication一个reward，step10将step8和step9加权共同作为样例的reward.然后使用policy gradient进行优化。<br>需要说明的model f和model g是已有的模型或者说在刚开始的时候使用少量的双语语料进行训练得到吗，然后逐渐加大单语语料的比例。</p>
<h2 id="资源-2"><a href="#资源-2" class="headerlink" title="资源"></a>资源</h2><p>NMT code:<a href="https://github.com/nyu-dl" target="_blank" rel="external">https://github.com/nyu-dl</a><br>compute BLEU score by the multi-bleu.perl:<a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl" target="_blank" rel="external">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl</a></p>
<h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>1、the standard NMT, Neural machine translation by jointly learning to align<br>and translate. ICLR, 2015.<br>2、pseudo-NMT, Improving neural machine translation models with monolingual data. In ACL, 2016.</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>本文的思想很创新，利用了机器翻译中的dual mechinism，仅仅利用少部分双语语料和大部分单语语料就可以达到之前NMT的效果，甚至还高了2到3个百分点。<br>dual的思想不仅可以用于机器翻译中，还可以用于图片、语音、文字等多种语言的共同学习，这样的相互作用共同学习更接近于人类对周围世界认识的方式，接受来自各个方面的信心，综合进行学习。</p>
<h1 id="Two-are-Better-than-One-An-Ensemble-of-Retrieval-and-Generation-Based-Dialog"><a href="#Two-are-Better-than-One-An-Ensemble-of-Retrieval-and-Generation-Based-Dialog" class="headerlink" title="Two are Better than One: An Ensemble of Retrieval and Generation-Based Dialog"></a><a href="https://arxiv.org/pdf/1610.07149v1.pdf" target="_blank" rel="external">Two are Better than One: An Ensemble of Retrieval and Generation-Based Dialog</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, Ming Zhang</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p>北京大学</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>对话系统、open domain、chatbot</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>arXiv</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>对话系统中可将问题和检索的结果同时作为输入Encoder之后进行解码Decoder，再将生成的结果和原检索结果重排序</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p><img src="media/14788352072241.jpg" alt=""><br><img src="media/14788352189655.jpg" alt=""></p>
<h2 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h2><p><img src="media/14788352485111.jpg" alt=""><br><img src="media/14788352559281.jpg" alt=""></p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>作者的思路非常简单，原来的回复生成模型容易发生回复内容短或者回复信息无意义的问题，在此作者将候选结果和原来的问句同时作为RNN生成器的输入，生成结果后再将本次生成的结果加入原检索候选集中，进行重新排序，实验结果证明此种方法比单独使用检索或单独使用生成效果有大幅提升。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>新的研究成果不见得可以直接应用于工程中，但新的paper，尤其是高质量paper中，一定会有很多的创新点，每一个创新点都可能会为后续的研究、工程实现等带来启发，甚至是一些技术上的突破。从本期开始，PaperWeekly会不定期地分享类似的内容，以方便大家了解最新的研究成果。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;本期的PaperWeekly一共分享四篇最近arXiv上放出的高质量paper，包括：机器翻译、表示学习、推荐系统和聊天机器人。人工智能及其
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.10.31-2016.11.04</title>
    <link href="http://rsarxiv.github.io/2016/11/05/cs-CL-weekly-2016-10-31-2016-11-04/"/>
    <id>http://rsarxiv.github.io/2016/11/05/cs-CL-weekly-2016-10-31-2016-11-04/</id>
    <published>2016-11-05T16:40:05.000Z</published>
    <updated>2016-11-05T16:52:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Neural-Machine-Translation-in-Linear-Time"><a href="#Neural-Machine-Translation-in-Linear-Time" class="headerlink" title="Neural Machine Translation in Linear Time"></a><a href="https://arxiv.org/pdf/1610.10099v1.pdf" target="_blank" rel="external">Neural Machine Translation in Linear Time</a></h2><p>【机器翻译】本文提出了一种新的encoder-decoder模型ByteNet。它是由两个扩张（dilated）卷积神经网络堆叠起来的。ByteNet的优势在于时间复杂度是线性的。工作来自deepmind。建议研究机器翻译以及使用MT模型做其他任务的童鞋精读。</p>
<h2 id="Dual-Learning-for-Machine-Translation"><a href="#Dual-Learning-for-Machine-Translation" class="headerlink" title="Dual Learning for Machine Translation"></a><a href="https://arxiv.org/pdf/1611.00179v1.pdf" target="_blank" rel="external">Dual Learning for Machine Translation</a></h2><p>【机器翻译】【增强学习】本文解决的问题是机器翻译中双语训练语料需求过多的问题，旨在通过一种手段来减少数据标注工作。作者采用的方法是现在重新流行的增强学习方法，翻译通常是一个对偶过程，比如：英翻法和法翻英。整个学习过程可以简单的描述如下：对偶的两个翻译任务可以当做是两个agent A和B，通过少量的双语标注数据可以学习出一个初级的翻译模型，同时通过大量的单语数据（无需标注）来学习出相应的语言模型；A将单语数据翻译成B，B通过自身的语言模型对A的翻译结果进行误差反馈，A进行学习；同理，B也可以向A学习，直到收敛。整个学习过程中，训练了A-&gt;B和B-&gt;A两个翻译模型，但是用到的双语标注数据就会比较少。</p>
<h2 id="End-to-End-Reading-Comprehension-with-Dynamic-Answer-Chunk-Ranking"><a href="#End-to-End-Reading-Comprehension-with-Dynamic-Answer-Chunk-Ranking" class="headerlink" title="End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking"></a><a href="https://arxiv.org/pdf/1610.09996v2.pdf" target="_blank" rel="external">End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking</a></h2><p>【机器阅读】本文研究的问题是最近一年非常流行的机器阅读理解问题，给定一段文本和一个问题，输出一个答案（选择、生成）。本文提出了一种新的模型，相比之前模型来说，改进的地方是可以给出变长度的答案。在之前模型的基础上，添加了一个entity表示模块，并且对候选的entity进行排序，得到正确答案。本文在SQuAD上进行了测试，拿到了最好的结果。建议研究QA和机器阅读的童鞋来精读这篇文章，并且开始新一轮SQuAD刷榜。</p>
<h2 id="Knowledge-Questions-from-Knowledge-Graphs"><a href="#Knowledge-Questions-from-Knowledge-Graphs" class="headerlink" title="Knowledge Questions from Knowledge Graphs"></a><a href="https://arxiv.org/pdf/1610.09935v2.pdf" target="_blank" rel="external">Knowledge Questions from Knowledge Graphs</a></h2><p>【问题生成】【知识图谱】本文研究的内容是从知识图谱中自动生成一些具有一定难度且答案唯一的问题，用于教育或评估。问题的第一个难点在于如何确保从图谱中选择的答案具有唯一性，第二个难点是如何评价所生成问题的难度。这个任务非常有趣，也是知识图谱在实际中的一个应用场景。任务本身比文章的模型和方法更值得思考。还是说回chatbot，QA chatbot，都说缺少数据，已构建好的知识图谱本身就是一个很大的数据源，如何利用它，如何将其更好地用于生成有用的训练数据，本文的任务也许会带来一些启发。这个任务也不算首创，之前KBQA的工作都有通过知识库出发来生成问题，且通过平行语料扩展，包括Percy liang等不少大牛的工作都考虑了这点，这里相当于延续，量化了难度确保了答案唯一性等</p>
<h2 id="LightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Networks"><a href="#LightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Networks" class="headerlink" title="LightRNN: Memory and Computation-Efficient Recurrent Neural Networks"></a><a href="https://arxiv.org/pdf/1610.09893v1.pdf" target="_blank" rel="external">LightRNN: Memory and Computation-Efficient Recurrent Neural Networks</a></h2><p>【新RNN】本文提出了一种新的思路来提高RNN的效果，包括时间和空间上的。最核心的点在于构建了一种全新的word embedding表示方式，传统的方法是词表中的每个词都用一个向量表示。将每个词都放入到一张二维表中，表中的每个词都有其所在的行向量和列向量共同表示，如图1所示。从而将词表示的规模从|V|个向量降到了2*sqrt(V)。本文还针对这种表示方法，构建了一种新的RNN模型LightRNN，并在大型数据集上进行了语言模型任务的评测，验证了本文方法在时间和空间上的性能提升。 </p>
<h2 id="Chinese-Poetry-Generation-with-Planning-based-Neural-Network"><a href="#Chinese-Poetry-Generation-with-Planning-based-Neural-Network" class="headerlink" title="Chinese Poetry Generation with Planning based Neural Network"></a><a href="https://arxiv.org/pdf/1610.09889v1.pdf" target="_blank" rel="external">Chinese Poetry Generation with Planning based Neural Network</a></h2><p>【诗词生成】本文研究的任务非常有趣，通过神经网络模型来生成唐诗，类似地可以开展宋词等任务。端到端地训练、学习具有很强的应用性，只要能够给定输入序列和输出序列，打开脑洞，做任何好玩的任务都有可能。</p>
<h2 id="MusicMood-Predicting-the-mood-of-music-from-song-lyrics-using-machine-learning"><a href="#MusicMood-Predicting-the-mood-of-music-from-song-lyrics-using-machine-learning" class="headerlink" title="MusicMood: Predicting the mood of music from song lyrics using machine learning"></a><a href="https://arxiv.org/pdf/1611.00138v1.pdf" target="_blank" rel="external">MusicMood: Predicting the mood of music from song lyrics using machine learning</a></h2><p>【音乐推荐系统】本文研究内容为通过机器学习方法从歌词中来预测音乐的情绪，算是自然语言处理在音乐中的应用。这种简单的应用，可以为音乐推荐系统提供一些特征，现有的音乐推荐系统可以做参考。</p>
<h2 id="Detecting-Context-Dependent-Messages-in-a-Conversational-Environment"><a href="#Detecting-Context-Dependent-Messages-in-a-Conversational-Environment" class="headerlink" title="Detecting Context Dependent Messages in a Conversational Environment"></a><a href="https://arxiv.org/pdf/1611.00483v2.pdf" target="_blank" rel="external">Detecting Context Dependent Messages in a Conversational Environment</a></h2><p>【chatbot】【上下文】chatbot的难点之一在于如何准确理解“人话”，“人话”有个显著的特点是简短而且非正式，常见的NLP分析方法，词性标注、句法分析等都不好用。理解“人话”需要结合上下文。那么，第一个问题来了，理解某句话应该取哪几句history作为上下文，第二个问题是如何理解上下文？本文旨在解决第一个问题，这个问题研究空间比较大，本文做了初步尝试。对chatbot感兴趣的童鞋，不管是学术界还是工业界的童鞋都可以读一下本文，或许会带来一些启发和思考。</p>
<h2 id="Natural-Parameter-Networks-A-Class-of-Probabilistic-Neural-Networks"><a href="#Natural-Parameter-Networks-A-Class-of-Probabilistic-Neural-Networks" class="headerlink" title="Natural-Parameter Networks: A Class of Probabilistic Neural Networks"></a><a href="https://arxiv.org/pdf/1611.00448v1.pdf" target="_blank" rel="external">Natural-Parameter Networks: A Class of Probabilistic Neural Networks</a></h2><p>【NIPS2016】本文提出了一类概率神经网络（贝叶斯），旨在解决现有神经网络在数据规模小的时候容易过拟合的问题。</p>
<h2 id="Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks"><a href="#Collaborative-Recurrent-Autoencoder-Recommend-while-Learning-to-Fill-in-the-Blanks" class="headerlink" title="Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks"></a><a href="https://arxiv.org/pdf/1611.00454v1.pdf" target="_blank" rel="external">Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks</a></h2><p>【推荐系统】本文的亮点在于将RNN和协同过滤无缝结合起来。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="聊天机器人资料汇总"><a href="#聊天机器人资料汇总" class="headerlink" title="聊天机器人资料汇总"></a><a href="https://www.52ml.net/20510.html" target="_blank" rel="external">聊天机器人资料汇总</a></h2><p>来自52ml汇总的聊天机器人资料</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一周值得读&quot;&gt;&lt;a href=&quot;#一周值得读&quot; class=&quot;headerlink&quot; title=&quot;一周值得读&quot;&gt;&lt;/a&gt;一周值得读&lt;/h1&gt;&lt;h2 id=&quot;Neural-Machine-Translation-in-Linear-Time&quot;&gt;&lt;a href=&quot;#
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly 第十二期</title>
    <link href="http://rsarxiv.github.io/2016/11/03/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%BA%8C%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/03/PaperWeekly-第十二期/</id>
    <published>2016-11-04T02:47:13.000Z</published>
    <updated>2016-11-04T03:19:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>文本摘要是自然语言处理的一大经典任务，研究的历史比较长。随着目前互联网生产出的文本数据越来越多，文本信息过载问题越来越严重，对各类文本进行一个“降维”处理显得非常必要，文本摘要便是其中一个重要的手段。传统的文本摘要方法，不管是句子级别、单文档还是多文档摘要，都严重依赖特征工程，随着深度学习的流行以及seq2seq+attention模型在机器翻译领域中的突破，文本摘要任务也迎来了一种全新的思路。本期PaperWeekly将会分享4篇在这方面做得非常出色的paper：</p>
<p>1、A Neural Attention Model for Abstractive Sentence Summarization, 2015<br>2、Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond, 2016<br>3、Neural Summarization by Extracting Sentences and Words, 2016<br>4、AttSum: Joint Learning of Focusing and Summarization with Neural Attention, 2016</p>
<h1 id="A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization"><a href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization" class="headerlink" title="A Neural Attention Model for Abstractive Sentence Summarization"></a><a href="https://aclweb.org/anthology/D/D15/D15-1044.pdf" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a></h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>Rush, A. M., Chopra, S., &amp; Weston, J.</p>
<h2 id="单位"><a href="#单位" class="headerlink" title="单位"></a>单位</h2><p>Facebook AI Research / Harvard SEAS</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>Neural Attention, Abstractive Sentence Summarization</p>
<h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>EMNLP 2015</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这篇来自Facebook的paper的主题是基于attention based NN的生成式句子摘要/压缩。</p>
<p><img src="media/emnlp.JPG" alt="1"></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>该工作使用提出了一种encoder-decoder框架下的句子摘要模型。</p>
<p><img src="media/emnlp2.JPG" alt="encoder"></p>
<p>作者在文章中介绍了三种不同的encoding方法，分别为：</p>
<ol>
<li>Bag-of-Words Encoder。词袋模型即将输入句子中词的词向量进行平均。</li>
<li>CNN encoder</li>
<li>Attention-Based Encoder。该encoder使用CNN对已生成的最近c（c为窗口大小）个词进行编码,再用编码出来的context向量对输入句子做attention，从而实现对输入的加权平均。</li>
</ol>
<p>模型中的decoder为修改过的NNLM，具体地：</p>
<p><img src="media/emnlp3.JPG" alt="1"></p>
<p>式中$$y_c$$为已生成的词中大小为c的窗口，与encoder中的Attention-Based Encoder同义。</p>
<p>与目前主流的基于seq2seq的模型不同，该模型中encoder并未采用流行的RNN。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>该文章使用了English Gigaword作为语料，选择新闻中的首句作为输入，新闻标题作为输出，以此构建平行语料。<br>具体的数据构建方法参见文章。</p>
<p>此外，该文章还使用了DUC2004作为测试集。</p>
<h2 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h2><p>在调研范围内，该文章是使用attention机制进行摘要的第一篇。且作者提出了利用Gigaword构建大量平行句对的方法，使得利用神经网络训练成为可能，之后多篇工作都使用了该方法构建训练数据。</p>
<h1 id="Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond"><a href="#Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond" class="headerlink" title="Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"></a><a href="https://aclweb.org/anthology/K/K16/K16-1028.pdf" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></h1><h2 id="作者-1"><a href="#作者-1" class="headerlink" title="作者"></a>作者</h2><p>Nallapati, Ramesh, et al.</p>
<h2 id="单位-1"><a href="#单位-1" class="headerlink" title="单位"></a>单位</h2><p>IBM Watson</p>
<h2 id="关键词-1"><a href="#关键词-1" class="headerlink" title="关键词"></a>关键词</h2><p>seq2seq, Summarization</p>
<h2 id="文章来源-1"><a href="#文章来源-1" class="headerlink" title="文章来源"></a>文章来源</h2><p>In CoNLL 2016</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>该工作主要研究了基于seq2seq模型的生成式文本摘要。<br>该文章不仅包括了句子压缩方面的工作，还给出了一个新的文档到多句子的数据集。</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p><img src="media/conll.JPG" alt=""></p>
<p>该文章使用了常用的seq2seq作为基本模型，并在其基础上添加了很多feature：</p>
<ol>
<li>Large Vocabulary Trick。<br>参见Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2014. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007.</li>
<li><p>添加feature。例如POS tag， TF、IDF， NER tag等。这些feature会被embed之后与输入句子的词向量拼接起来作为encoder的输入。</p>
</li>
<li><p>pointing / copy 机制。使用一个gate来判断是否要从输入句子中拷贝词或者使用decoder生成词。参见ACL 2016的两篇相关paper。</p>
</li>
<li><p>Hierarchical Attention。这是用于文章摘要中多句子的attention，思路借鉴了Jiwei Li的一篇auto encoder的工作。大致思路为使用句子级别的weight对句子中的词进行re-scale。</p>
</li>
</ol>
<h2 id="数据-1"><a href="#数据-1" class="headerlink" title="数据"></a>数据</h2><ol>
<li>English Gigaword</li>
<li>DUC 2004</li>
<li>提出了CNN/Daily Mail Corpus</li>
</ol>
<h2 id="简评-1"><a href="#简评-1" class="headerlink" title="简评"></a>简评</h2><p>该工作为在第一篇文章基础上的改进工作，做了大量的实验，非常扎实。文章提出的feature-rich encoder对其他工作也有参考意义，即将传统方法中的特征显示地作为神经网络的输入，提高了效果。</p>
<h1 id="Neural-Summarization-by-Extracting-Sentences-and-Words"><a href="#Neural-Summarization-by-Extracting-Sentences-and-Words" class="headerlink" title="Neural Summarization by Extracting Sentences and Words"></a><a href="https://aclweb.org/anthology/P/P16/P16-1046.pdf" target="_blank" rel="external">Neural Summarization by Extracting Sentences and Words</a></h1><h2 id="作者-2"><a href="#作者-2" class="headerlink" title="作者"></a>作者</h2><p>Cheng, Jianpeng, and Mirella Lapata.</p>
<h2 id="单位-2"><a href="#单位-2" class="headerlink" title="单位"></a>单位</h2><p>University of Edinburgh</p>
<h2 id="关键词-2"><a href="#关键词-2" class="headerlink" title="关键词"></a>关键词</h2><p>Extractive Summarization, Neural Attention</p>
<h2 id="文章来源-2"><a href="#文章来源-2" class="headerlink" title="文章来源"></a>文章来源</h2><p>ACL 2016</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h2><p>使用神经网络进行抽取式摘要，分别为句子抽取和单词抽取。</p>
<h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p><img src="media/extractModel1.JPG" alt=""></p>
<h3 id="句子抽取"><a href="#句子抽取" class="headerlink" title="句子抽取"></a>句子抽取</h3><p>由于该工作为文档的摘要，故其使用了两层encoder，分别为：</p>
<ol>
<li>词级别的encoder，基于CNN。即对句子做卷积再做max pooling从而获得句子的表示。</li>
<li>句子级别的encoder，基于RNN。将句子的表示作为输入，即获得文档的表示。</li>
</ol>
<p>由于是抽取式摘要，其使用了一个RNN decoder，但其作用并非生成，而是用作sequence labeling，对输入的句子判断是否进行抽取，类似于pointer network。</p>
<h3 id="词的抽取"><a href="#词的抽取" class="headerlink" title="词的抽取"></a>词的抽取</h3><p>对于词的抽取，该模型同样适用了hierarchical attention。与句子抽取不同，词的抽取更类似于生成，只是将输入文档的单词作为decoder的词表。</p>
<h2 id="数据-2"><a href="#数据-2" class="headerlink" title="数据"></a>数据</h2><p>从DailyMail news中根据其highlight构建抽取式摘要数据集。</p>
<h2 id="简评-2"><a href="#简评-2" class="headerlink" title="简评"></a>简评</h2><p>该工作的特别之处在于对attention机制的使用。该paper之前的许多工作中的attention机制都与Bahdanau的工作相同，即用attention对某些向量求weighted sum。该工作则直接使用attention的分数进行对文档中句子进行选择，实际上与pointer networks意思相近。</p>
<h1 id="AttSum-Joint-Learning-of-Focusing-and-Summarization-with-Neural-Attention"><a href="#AttSum-Joint-Learning-of-Focusing-and-Summarization-with-Neural-Attention" class="headerlink" title="AttSum: Joint Learning of Focusing and Summarization with Neural Attention"></a><a href="http://www4.comp.polyu.edu.hk/~cszqcao/data/attsum.pdf" target="_blank" rel="external">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></h1><h2 id="作者-3"><a href="#作者-3" class="headerlink" title="作者"></a>作者</h2><p>Cao, Ziqiang, et al.</p>
<h2 id="单位-3"><a href="#单位-3" class="headerlink" title="单位"></a>单位</h2><p> The Hong Kong Polytechnic University, Peking University, Microsoft Research</p>
<h2 id="关键词-3"><a href="#关键词-3" class="headerlink" title="关键词"></a>关键词</h2><p>Query-focused Summarization</p>
<h2 id="文章来源-3"><a href="#文章来源-3" class="headerlink" title="文章来源"></a>文章来源</h2><p>COLING 2016</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h2><p>Query-focused多文档抽取式摘要</p>
<h2 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h2><p><img src="media/coling.JPG" alt=""></p>
<p>由于该任务为针对某个query抽取出可以回答该query的摘要，模型使用了attention机制对句子进行加权，加权的依据为文档句子对query的相关性（基于attention），从而对句子ranking，进而抽取出摘要。具体地：</p>
<ol>
<li>使用CNN对句子进行encoding</li>
<li>利用query，对句子表示进行weighted sum pooling。</li>
<li>使用cosine similarity对句子排序。</li>
</ol>
<h2 id="数据-3"><a href="#数据-3" class="headerlink" title="数据"></a>数据</h2><p>DUC 2005 ∼ 2007 query-focused summarization benchmark datasets</p>
<h2 id="简评-3"><a href="#简评-3" class="headerlink" title="简评"></a>简评</h2><p>该文章的亮点之处在于使用attention机制对文档中句子进行weighted-sum pooling，以此完成query-focused的句子表示和ranking。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本次主要介绍了四篇文本摘要的工作，前两篇为生成式（abstractive）摘要，后两篇为抽取式（extractive）摘要。对于生成式摘要，目前主要是基于encoder-decoder模式的生成，但这种方法受限于语料的获得，而Rush等提出了利用English Gigaword（即新闻数据）构建平行句对语料库的方法。IBM在Facebook工作启发下，直接使用了seq2seq with attention模型进行摘要的生成，获得了更好的效果。对于抽取式摘要，神经网络模型的作用多用来学习句子表示进而用于后续的句子ranking。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引&quot;&gt;&lt;a href=&quot;#引&quot; class=&quot;headerlink&quot; title=&quot;引&quot;&gt;&lt;/a&gt;引&lt;/h1&gt;&lt;p&gt;文本摘要是自然语言处理的一大经典任务，研究的历史比较长。随着目前互联网生产出的文本数据越来越多，文本信息过载问题越来越严重，对各类文本进行一个“降维
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
</feed>
