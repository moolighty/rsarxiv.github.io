<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  
  <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#</title>
  
   <meta name="description" content="(欢迎大家订阅本博客，订阅地址是RSS)
本篇将分享的文章相比于昨天那篇Sequence to Sequence Learning with Neural Networks更早地使用了seq2seq的框架来解决机器翻译的问题，可能上一篇来自于Google，工程性更强一些，学术性有一些不足。本文来自于">
  

  <meta property="og:title" content="Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="PaperWeekly"/>
 <meta property="og:image" content="undefined"/>
  
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="main">
    <div class="behind">
      <div class="back">
        <a href="/" class="black-color"><i class="fa fa-times" aria-hidden="true"></i></a>
      </div>
      <div class="description">
        &nbsp;
      </div>
    </div>
    <div class="container">
      

  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#
    </h1>
  


    </div>
    <div class="meta center">
      
<time datetime="2016-06-01T15:15:17.000Z">
  <i class="fa fa-calendar"></i>&nbsp;
  2016-06-01
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/PaperWeekly/">PaperWeekly</a>·<a href="/tags/seq2seq/">seq2seq</a>


    </div>
    <hr>
    <div class="picture-container">
      
    </div>
    <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将分享的文章相比于昨天那篇<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>更早地使用了seq2seq的框架来解决机器翻译的问题，可能上一篇来自于Google，工程性更强一些，学术性有一些不足。本文来自于学术机构，学术范更浓一些。本文的题目是<a href="http://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder–Decoder for Statistical<br>  Machine Translation</a>，作者是来自蒙特利尔大学的<a href="http://www.kyunghyuncho.me/" target="_blank" rel="external">Kyunghyun Cho</a>博士（现在在纽约大学任教），2014年6月登在arxiv上。</p>
<p>本文最大的两个贡献是：</p>
<p>1、提出了一种类似于LSTM的GRU结构作为RNN的hidden unit，并且具有比LSTM更少的参数，更不容易过拟合。</p>
<p>2、较早地（据说2013年就有人提出用seq2seq思路来解决问题）将seq2seq应用在了机器翻译领域，并且取得了不错的效果。</p>
<p>自然语言生成（NLG）领域中有很多任务，比如机器翻译，<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>，自动问答，对话生成等，都是根据一个上下文来生成一个文本序列，这里分类两个过程，encoder部分将输入序列表示成一个context，decoder部分在context的条件下生成一个输出序列，联合训练两个部分得到最优的模型。这里的context就像是一个memory，试着保存了encoder部分的所有信息（但往往用较低的维度表示整个输入序列一定会造成信息损失）。本文的思路就是如此，具体可参看下图：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig1.png" width="600" height="600">
<p>本文模型将encoder部分的最后一个hidden state作为context输入给decoder，decoder中的每一个时间t的hidden state s(t)都与context,s(t-1),y(t-1)有关系，而每一个时间t的输出y(t)都与context,s(t),y(t-1)有关。当然，这种模型是非常灵活的，你的context可以有很多种选择，比如可以选encoder中所有hidden state组成的矩阵来作为context，可以用BiRNN计算出两个last hidden state进行拼接作为context；而s(t)和y(t)根据RNN结构不同，也可以将context作为s(0)依次向后传递，而不是每次都依赖于context。</p>
<p>说完了模型部分，来说说本文最大的贡献是提出了GRU，一种更轻量级的hidden unit，效果还不输LSTM，函数结构如下图：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig2.png" width="400" height="400">
<p>GRU有两个门函数，reset gate和update gate，公式如下：</p>
<p>reset gate：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig3.png" width="300" height="300">
<p>update gate：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig4.png" width="300" height="300">
<p>reset gate接近于0的时候，当前hidden state会忽略前面的hidden state，在当前输入处reset。reset gate控制了哪些信息可以通过，而update gate控制着多少信息可以通过，与LSTM中的cell扮演着相似的角色。计算出每一步的reset和update gate，即可计算出当前的hidden state，如下：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig5.png" width="300" height="300">
<p>这里，</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig6.png" width="300" height="300">
<p>实验部分，作者利用本文模型得到了满意的结果，不再赘述。</p>
<p>另外，本文在附录部分给出了一个比较详尽的encoder-decoder公式推导过程，大家可以参看原文。</p>
<p>从context预测output，是一件很神奇的事情。而context又是千变万化的，当下正流行的模型attention model正是在context上做了文章，得到了更好的结果。相信，对context的变化和应用会带来更多好玩的模型。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">


  </article>
  </script>
    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>
  <div class="busuanzi center">
    页阅读量:&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;・&nbsp;
    站访问量:&nbsp;<span id="busuanzi_value_site_pv"></span>&nbsp;・&nbsp;
    站访客数:&nbsp;<span id="busuanzi_value_site_uv"></span>
  </div>




    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot container">
    <div class="firstrow">
        <a href="#top" >
        <i class="fa fa-arrow-right"></i>
        </a>
        © XXX 2015-2016
    </div>
    <div class="secondrow">
        <a href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.js"></script>
<script type="text/javascript">

// comments below to disable loading animation
function revealOnScroll() {
  var scrolled = $(window).scrollTop();
  $(".excerpt, .index-title, .index-meta, p").each(function() {
    var current = $(this),
      height = $(window).outerHeight(),
      offsetTop = current.offset().top;
    (scrolled + height + 50 > offsetTop) ? current.addClass("animation"):'';
  });
}
$(window).on("scroll", revealOnScroll);
$(document).ready(revealOnScroll)

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

// back to top scripts
$("a[href='#top']").click(function() {
  $("html, body").animate({ scrollTop: 0 }, 500);
  return false;
});


var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
