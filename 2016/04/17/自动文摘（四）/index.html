<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  
  <title>自动文摘（四）</title>
  
   <meta name="description" content="引这篇博客是自动文摘系列的第四篇，重点介绍近期abstractive summarization的一些研究情况。abstractive是学术界研究的热点，尤其是Machine Translation中的encoder-decoder框架和attention mechanism十分火热，大家都试着将a">
  

  <meta property="og:title" content="自动文摘（四）"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="PaperWeekly"/>
 <meta property="og:image" content="undefined"/>
  
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="main">
    <div class="behind">
      <div class="back">
        <a href="/" class="black-color"><i class="fa fa-times" aria-hidden="true"></i></a>
      </div>
      <div class="description">
        &nbsp;
      </div>
    </div>
    <div class="container">
      

  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        自动文摘（四）
    </h1>
  


    </div>
    <div class="meta center">
      
<time datetime="2016-04-18T02:06:18.000Z">
  <i class="fa fa-calendar"></i>&nbsp;
  2016-04-17
</time>






    
    &nbsp;
    <i class="fa fa-tag"></i>&nbsp;
    <a href="/tags/nlp/">nlp</a>·<a href="/tags/自动文摘/">自动文摘</a>


    </div>
    <hr>
    <div class="picture-container">
      
    </div>
    <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>这篇博客是自动文摘系列的第四篇，重点介绍近期abstractive summarization的一些研究情况。abstractive是学术界研究的热点，尤其是Machine Translation中的encoder-decoder框架和attention mechanism十分火热，大家都试着将abstractive问题转换为sequence-2-sequence问题，套用上面两种技术，得到state-of-the-art结果，2015年来已经有许多篇paper都是这种套路，于是就有了下面的吐槽：</p>
<img src="http://ww4.sinaimg.cn/mw690/62caff97jw1f2qam97d3tj20sg0nd775.jpg" width="400" height="650">
<h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>Encoder-Decoder不是一种模型，而是一种框架，一种处理问题的思路，最早应用于机器翻译领域，输入一个序列，输出另外一个序列。机器翻译问题就是将一种语言序列转换成另外一种语言序列，将该技术扩展到其他领域，比如输入序列可以是文字，语音，图像，视频，输出序列可以是文字，图像，可以解决很多别的类型的问题。这一大类问题就是上图中的sequence-to-sequence问题。这里以输入为文本，输出也为文本作为例子进行介绍：</p>
<img src="/2016/04/17/自动文摘（四）/seq2seq.png" width="400" height="650">
<p>encoder部分是将输入序列表示成一个带有语义的向量，使用最广泛的表示技术是Recurrent Neural Network，RNN是一个基本模型，在训练的时候会遇到gradient explode或者gradient vanishing的问题，导致无法训练，所以在实际中经常使用的是经过改良的LSTM RNN或者GRU RNN对输入序列进行表示，更加复杂一点可以用BiRNN、BiRNN with LSTM、BiRNN with GRU、多层RNN等模型来表示，输入序列最终表示为最后一个word的hidden state vector。</p>
<p>decoder部分是以encoder生成的hidden state vector作为输入“解码”出目标文本序列，本质上是一个语言模型，最常见的是用Recurrent Neural Network Language Model(RNNLM)，只要涉及到RNN就会有训练的问题，也就需要用LSTM、GRU和一些高级的model来代替。目标序列的生成和LM做句子生成的过程类似，只是说计算条件概率时需要考虑encoder向量。</p>
<p>这里，每一种模型几乎都可以出一篇paper，尤其是在这个技术刚刚开始应用在各个领域中的时候，大家通过尝试不同的模型组合，得到state-of-the-art结果。</p>
<p>该框架最早被应用在Google Translation中，paper详情可以见[1]，2014年12月发在arxiv上。</p>
<h1 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h1><p>注意力机制在NLP中的使用也就是2015年的事情，也是从机器翻译领域开始。我们仔细看decoder中生成目标文本序列这部分，第一个word的生成完全依赖于encoder的last hidden state vector，而这个vector更多的是表示输入序列的最后一个word的意思，也就是说rnn一般来说都是一个有偏的模型。</p>
<p>打个比方，rnn可以理解为一个人看完了一段话，他可能只记得最后几个词说明的意思，但是如果你问他前面的信息，他就不能准确地回答，attention可以理解为，提问的信息只与之前看完的那段话中一部分关系密切，而其他部分关系不大，这个人就会将自己的注意力锁定在这部分信息中。这个就是所谓attention mechanism的原理，每个hidden state vector对于decoder生成每个单词都有影响，但影响分布并不相同，请看下图：</p>
<img src="/2016/04/17/自动文摘（四）/attention.png" width="400" height="650">
<p>图中行文本代表输出，列文本代表输入，颜色越深表示两个词相关性越强，即生成该词时需要多注意对应的输入词。不同的paper在使用attention上会有不同的技巧，这里不一一赘述了。</p>
<h1 id="Neural-Summarization"><a href="#Neural-Summarization" class="headerlink" title="Neural Summarization"></a>Neural Summarization</h1><p>使用deep learning技术来做abstractive summarization的paper屈指可数，大体的思路也类似，大概如下：</p>
<p>0、首先将自动文摘的问题构造成一个seq2seq问题，通常的做法是将某段文本的first sentence作为输入，headlines作为输出，本质上变成了一个headlines generative问题。</p>
<p>1、选择一个big corpus作为训练、测试集。自动文摘的技术没有太成熟的一个重要原因在于没有一个成熟的大规模语料。一般来说都选择Gigawords作为训练、测试集，然后用DUC的数据集进行验证和对比。</p>
<p>2、选择一个合适的encoder，这里可以选simple rnn，lstm rnn，gru rnn，simple birnn，lstm birnn，gru birnn，deep rnn，cnn，以及各种各样的cnn。不同model之间的组合都是一种创新，只不过创新意义不太大。用encoder将输入文本表示成一个向量。</p>
<p>3、选择一个合适的decoder，decoder的作用是一个language model，用来生成summary words。</p>
<p>4、设计一个合适的attention model。不仅仅基于encoder last hidden state vector和上文来预测输出文本序列，更要基于输入中“注意力”更高的词来预测相应的词。</p>
<p>5、设计一个copy net。只要是语言模型都会存在相同的问题，比如out-of-vocabulary词的处理，尤其是做新闻类摘要的生成时，很多词都是人名、机构名等专有名词，所以这里需要用copy net 将输入中的词copy过来生成输出。在生成中文摘要问题上，将words降维到characters可以避免oov的问题，并且取得不错的结果。</p>
<p>接下来想做的事情是将neural summarization相关的paper精读之后写成blog。</p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></p>
<p>[2] <a href="http://cn.arxiv.org/pdf/1509.00685" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a></p>
<p>[3] <a href="http://cn.arxiv.org/pdf/1506.05865" target="_blank" rel="external">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<p>[4] <a href="http://cn.arxiv.org/pdf/1603.06393" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">


  </article>
  </script>
    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>
  <div class="busuanzi center">
    页阅读量:&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;・&nbsp;
    站访问量:&nbsp;<span id="busuanzi_value_site_pv"></span>&nbsp;・&nbsp;
    站访客数:&nbsp;<span id="busuanzi_value_site_uv"></span>
  </div>




    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot container">
    <div class="firstrow">
        <a href="#top" >
        <i class="fa fa-arrow-right"></i>
        </a>
        © XXX 2015-2016
    </div>
    <div class="secondrow">
        <a href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.js"></script>
<script type="text/javascript">

// comments below to disable loading animation
function revealOnScroll() {
  var scrolled = $(window).scrollTop();
  $(".excerpt, .index-title, .index-meta, p").each(function() {
    var current = $(this),
      height = $(window).outerHeight(),
      offsetTop = current.offset().top;
    (scrolled + height + 50 > offsetTop) ? current.addClass("animation"):'';
  });
}
$(window).on("scroll", revealOnScroll);
$(document).ready(revealOnScroll)

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

// back to top scripts
$("a[href='#top']").click(function() {
  $("html, body").animate({ scrollTop: 0 }, 500);
  return false;
});


var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
